{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " nasnet_mobile /6分类.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhgtcIYzXSphyKLO95qzvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxs1652/zcq/blob/master/nasnet_mobile_6%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOQVf58mfjrf",
        "outputId": "c8235690-5082-4407-da9f-96796c805284"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 21 04:05:19 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qLK5JwGfp4W",
        "outputId": "18d3dfb2-bcd9-4003-dff0-99209390351a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugp6PDHYfskM",
        "outputId": "e6fa1fd0-ee56-4c45-b007-b291a9e4b390"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/class/dataall"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/class/dataall\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XticT8i4f9Px"
      },
      "source": [
        "import pandas as pd\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf98xesngozd"
      },
      "source": [
        "df=pd.read_csv('images.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Z-KXSsZJgxIW",
        "outputId": "e85e8993-3808-40be-91e6-eb8898429d74"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png  ...  5\n",
              "0  E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  4\n",
              "1  E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "2  E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  5\n",
              "3  E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  5\n",
              "4  E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  5\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfwJXAoOg5Ov"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivZNIUXSiH77",
        "outputId": "d84460ff-0bce-43b3-fd11-bcfc7e25863a"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png',\n",
              "       'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5',\n",
              "       'Unnamed: 6', '5'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZCDsEmjoiu4H",
        "outputId": "b78826b3-7a0a-4b9b-a38f-759c0ad5c12e"
      },
      "source": [
        "df['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png'][1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'E:/smallcut/data\\\\smallImages\\\\images\\\\DSC_5856_12\\\\DSC_5856_12_2.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "UUbXtjSejhDH",
        "outputId": "1f9b4638-5f0f-4412-a874-dd6c8c220550"
      },
      "source": [
        "df[df['5']==0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_1...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6413</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_2...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6423</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_2...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6432</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_2...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6433</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_2...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6437</th>\n",
              "      <td>E:/smallcut/data\\smallImages\\images\\DSC_5856_2...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>976 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png  ...  5\n",
              "1     E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "10    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "40    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "56    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "64    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...                 ...  0\n",
              "...                                                 ...                 ... ..\n",
              "6413  E:/smallcut/data\\smallImages\\images\\DSC_5856_2...                 ...  0\n",
              "6423  E:/smallcut/data\\smallImages\\images\\DSC_5856_2...                 ...  0\n",
              "6432  E:/smallcut/data\\smallImages\\images\\DSC_5856_2...                 ...  0\n",
              "6433  E:/smallcut/data\\smallImages\\images\\DSC_5856_2...                 ...  0\n",
              "6437  E:/smallcut/data\\smallImages\\images\\DSC_5856_2...                 ...  0\n",
              "\n",
              "[976 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXWn_3s4jvUb"
      },
      "source": [
        "data0=df[df['5']==0]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNuHRetSktRl",
        "outputId": "cfeb0e9c-885f-4574-d148-4d377116c56c"
      },
      "source": [
        "data0.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     E:/smallcut/data\\smallImages\\images\\DSC_5856_1...\n",
              "10    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...\n",
              "40    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...\n",
              "56    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...\n",
              "64    E:/smallcut/data\\smallImages\\images\\DSC_5856_1...\n",
              "Name: E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qTPyRrj7kzCB",
        "outputId": "476d7f30-263d-46ae-c926-948cd4c42f68"
      },
      "source": [
        "data0[40][48:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DSC_5856_12_41.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-_aise0lHk6",
        "outputId": "27a51c0b-1ee3-4e3e-de8a-95437905c542"
      },
      "source": [
        "type(data0.values)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_P5FLwNTmdLA",
        "outputId": "212decaa-7984-4dfe-d3d6-99dc1508c087"
      },
      "source": [
        "data0[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'E:/smallcut/data\\\\smallImages\\\\images\\\\DSC_5856_12\\\\DSC_5856_12_2.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV6PfzNboPIl",
        "outputId": "4c4fe1b1-85c9-4629-98b1-325e8854006b"
      },
      "source": [
        "a=data0.values\r\n",
        "type(a)\r\n",
        "len(a)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "976"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFRAiJ5Io-Li"
      },
      "source": [
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data1 = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "#strr[strr.rindex( '\\\\' ) + 1 : len(strr)] "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKpk7eSppVf"
      },
      "source": [
        "#data1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQFVPEODpqTR",
        "outputId": "5b509e2a-6189-4514-ed9a-1ecae19441fd"
      },
      "source": [
        "import shutil\r\n",
        "for ln in data1:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/0/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_2.png\n",
            "DSC_5856_12_11.png\n",
            "DSC_5856_12_41.png\n",
            "DSC_5856_12_57.png\n",
            "DSC_5856_12_65.png\n",
            "DSC_5856_12_66.png\n",
            "DSC_5856_12_68.png\n",
            "DSC_5856_12_90.png\n",
            "DSC_5856_12_103.png\n",
            "DSC_5856_12_108.png\n",
            "DSC_5856_12_121.png\n",
            "DSC_5856_12_123.png\n",
            "DSC_5856_12_133.png\n",
            "DSC_5856_12_136.png\n",
            "DSC_5856_12_145.png\n",
            "DSC_5856_12_157.png\n",
            "DSC_5856_12_172.png\n",
            "DSC_5856_12_231.png\n",
            "DSC_5856_12_260.png\n",
            "DSC_5856_12_269.png\n",
            "DSC_5856_12_285.png\n",
            "DSC_5856_12_322.png\n",
            "DSC_5856_12_334.png\n",
            "DSC_5856_12_364.png\n",
            "DSC_5856_12_368.png\n",
            "DSC_5856_12_372.png\n",
            "DSC_5856_12_375.png\n",
            "DSC_5856_12_385.png\n",
            "DSC_5856_12_393.png\n",
            "DSC_5856_12_397.png\n",
            "DSC_5856_12_404.png\n",
            "DSC_5856_12_456.png\n",
            "DSC_5856_12_464.png\n",
            "DSC_5856_12_476.png\n",
            "DSC_5856_12_498.png\n",
            "DSC_5856_13_10.png\n",
            "DSC_5856_13_52.png\n",
            "DSC_5856_13_60.png\n",
            "DSC_5856_13_113.png\n",
            "DSC_5856_13_137.png\n",
            "DSC_5856_13_140.png\n",
            "DSC_5856_13_182.png\n",
            "DSC_5856_13_195.png\n",
            "DSC_5856_13_204.png\n",
            "DSC_5856_13_245.png\n",
            "DSC_5856_13_259.png\n",
            "DSC_5856_13_297.png\n",
            "DSC_5856_13_321.png\n",
            "DSC_5856_13_331.png\n",
            "DSC_5856_13_335.png\n",
            "DSC_5856_13_398.png\n",
            "DSC_5856_13_469.png\n",
            "DSC_5856_21_0.png\n",
            "DSC_5856_21_3.png\n",
            "DSC_5856_21_7.png\n",
            "DSC_5856_21_14.png\n",
            "DSC_5856_21_19.png\n",
            "DSC_5856_21_20.png\n",
            "DSC_5856_21_24.png\n",
            "DSC_5856_21_34.png\n",
            "DSC_5856_21_39.png\n",
            "DSC_5856_21_43.png\n",
            "DSC_5856_21_48.png\n",
            "DSC_5856_21_80.png\n",
            "DSC_5856_21_92.png\n",
            "DSC_5856_21_96.png\n",
            "DSC_5856_21_106.png\n",
            "DSC_5856_21_110.png\n",
            "DSC_5856_21_111.png\n",
            "DSC_5856_21_122.png\n",
            "DSC_5856_21_124.png\n",
            "DSC_5856_21_138.png\n",
            "DSC_5856_21_139.png\n",
            "DSC_5856_21_152.png\n",
            "DSC_5856_21_153.png\n",
            "DSC_5856_21_177.png\n",
            "DSC_5856_21_183.png\n",
            "DSC_5856_21_184.png\n",
            "DSC_5856_21_188.png\n",
            "DSC_5856_21_194.png\n",
            "DSC_5856_21_198.png\n",
            "DSC_5856_21_201.png\n",
            "DSC_5856_21_208.png\n",
            "DSC_5856_21_212.png\n",
            "DSC_5856_21_213.png\n",
            "DSC_5856_21_216.png\n",
            "DSC_5856_21_217.png\n",
            "DSC_5856_21_227.png\n",
            "DSC_5856_21_236.png\n",
            "DSC_5856_21_249.png\n",
            "DSC_5856_21_257.png\n",
            "DSC_5856_21_263.png\n",
            "DSC_5856_21_267.png\n",
            "DSC_5856_21_268.png\n",
            "DSC_5856_21_272.png\n",
            "DSC_5856_21_276.png\n",
            "DSC_5856_21_278.png\n",
            "DSC_5856_21_287.png\n",
            "DSC_5856_21_288.png\n",
            "DSC_5856_21_312.png\n",
            "DSC_5856_21_313.png\n",
            "DSC_5856_21_319.png\n",
            "DSC_5856_21_324.png\n",
            "DSC_5856_21_332.png\n",
            "DSC_5856_21_333.png\n",
            "DSC_5856_21_338.png\n",
            "DSC_5856_21_339.png\n",
            "DSC_5856_21_341.png\n",
            "DSC_5856_21_342.png\n",
            "DSC_5856_21_376.png\n",
            "DSC_5856_21_402.png\n",
            "DSC_5856_21_403.png\n",
            "DSC_5856_21_409.png\n",
            "DSC_5856_21_412.png\n",
            "DSC_5856_21_415.png\n",
            "DSC_5856_21_428.png\n",
            "DSC_5856_21_430.png\n",
            "DSC_5856_21_435.png\n",
            "DSC_5856_21_439.png\n",
            "DSC_5856_21_440.png\n",
            "DSC_5856_21_448.png\n",
            "DSC_5856_21_454.png\n",
            "DSC_5856_21_461.png\n",
            "DSC_5856_21_468.png\n",
            "DSC_5856_21_478.png\n",
            "DSC_5856_21_479.png\n",
            "DSC_5856_21_483.png\n",
            "DSC_5856_21_484.png\n",
            "DSC_5856_21_489.png\n",
            "DSC_5856_21_490.png\n",
            "DSC_5856_21_497.png\n",
            "DSC_5856_25_1.png\n",
            "DSC_5856_25_4.png\n",
            "DSC_5856_25_6.png\n",
            "DSC_5856_25_18.png\n",
            "DSC_5856_25_20.png\n",
            "DSC_5856_25_32.png\n",
            "DSC_5856_25_37.png\n",
            "DSC_5856_25_39.png\n",
            "DSC_5856_25_56.png\n",
            "DSC_5856_25_59.png\n",
            "DSC_5856_25_67.png\n",
            "DSC_5856_25_70.png\n",
            "DSC_5856_25_73.png\n",
            "DSC_5856_25_77.png\n",
            "DSC_5856_25_79.png\n",
            "DSC_5856_25_85.png\n",
            "DSC_5856_25_88.png\n",
            "DSC_5856_25_92.png\n",
            "DSC_5856_25_96.png\n",
            "DSC_5856_25_105.png\n",
            "DSC_5856_25_109.png\n",
            "DSC_5856_25_112.png\n",
            "DSC_5856_25_113.png\n",
            "DSC_5856_25_116.png\n",
            "DSC_5856_25_125.png\n",
            "DSC_5856_25_132.png\n",
            "DSC_5856_25_134.png\n",
            "DSC_5856_25_137.png\n",
            "DSC_5856_25_147.png\n",
            "DSC_5856_25_157.png\n",
            "DSC_5856_25_163.png\n",
            "DSC_5856_25_165.png\n",
            "DSC_5856_25_167.png\n",
            "DSC_5856_25_168.png\n",
            "DSC_5856_25_174.png\n",
            "DSC_5856_25_175.png\n",
            "DSC_5856_25_192.png\n",
            "DSC_5856_25_197.png\n",
            "DSC_5856_25_200.png\n",
            "DSC_5856_25_209.png\n",
            "DSC_5856_25_221.png\n",
            "DSC_5856_25_223.png\n",
            "DSC_5856_25_224.png\n",
            "DSC_5856_25_226.png\n",
            "DSC_5856_25_228.png\n",
            "DSC_5856_25_231.png\n",
            "DSC_5856_25_235.png\n",
            "DSC_5856_25_237.png\n",
            "DSC_5856_25_242.png\n",
            "DSC_5856_25_248.png\n",
            "DSC_5856_25_249.png\n",
            "DSC_5856_25_256.png\n",
            "DSC_5856_25_257.png\n",
            "DSC_5856_25_259.png\n",
            "DSC_5856_25_275.png\n",
            "DSC_5856_25_276.png\n",
            "DSC_5856_25_278.png\n",
            "DSC_5856_25_279.png\n",
            "DSC_5856_25_289.png\n",
            "DSC_5856_25_293.png\n",
            "DSC_5856_25_294.png\n",
            "DSC_5856_25_297.png\n",
            "DSC_5856_25_314.png\n",
            "DSC_5856_25_316.png\n",
            "DSC_5856_25_329.png\n",
            "DSC_5856_25_331.png\n",
            "DSC_5856_25_332.png\n",
            "DSC_5856_25_340.png\n",
            "DSC_5856_25_341.png\n",
            "DSC_5856_25_343.png\n",
            "DSC_5856_25_345.png\n",
            "DSC_5856_25_346.png\n",
            "DSC_5856_25_349.png\n",
            "DSC_5856_25_350.png\n",
            "DSC_5856_25_352.png\n",
            "DSC_5856_25_354.png\n",
            "DSC_5856_25_355.png\n",
            "DSC_5856_25_362.png\n",
            "DSC_5856_25_365.png\n",
            "DSC_5856_25_370.png\n",
            "DSC_5856_25_376.png\n",
            "DSC_5856_25_379.png\n",
            "DSC_5856_25_381.png\n",
            "DSC_5856_25_382.png\n",
            "DSC_5856_25_384.png\n",
            "DSC_5856_25_386.png\n",
            "DSC_5856_25_390.png\n",
            "DSC_5856_25_394.png\n",
            "DSC_5856_25_402.png\n",
            "DSC_5856_25_414.png\n",
            "DSC_5856_25_416.png\n",
            "DSC_5856_25_417.png\n",
            "DSC_5856_25_435.png\n",
            "DSC_5856_25_437.png\n",
            "DSC_5856_25_439.png\n",
            "DSC_5856_25_441.png\n",
            "DSC_5856_25_444.png\n",
            "DSC_5856_25_446.png\n",
            "DSC_5856_25_448.png\n",
            "DSC_5856_25_455.png\n",
            "DSC_5856_25_458.png\n",
            "DSC_5856_25_460.png\n",
            "DSC_5856_25_462.png\n",
            "DSC_5856_25_463.png\n",
            "DSC_5856_25_473.png\n",
            "DSC_5856_25_478.png\n",
            "DSC_5856_25_484.png\n",
            "DSC_5856_25_485.png\n",
            "DSC_5856_25_486.png\n",
            "DSC_5856_25_488.png\n",
            "DSC_5856_25_489.png\n",
            "DSC_5856_25_492.png\n",
            "DSC_5856_25_493.png\n",
            "DSC_5856_25_496.png\n",
            "DSC_5856_25_497.png\n",
            "DSC_5856_25_499.png\n",
            "DSC_5856_11_1.png\n",
            "DSC_5856_11_5.png\n",
            "DSC_5856_11_16.png\n",
            "DSC_5856_11_41.png\n",
            "DSC_5856_11_44.png\n",
            "DSC_5856_11_54.png\n",
            "DSC_5856_11_63.png\n",
            "DSC_5856_11_72.png\n",
            "DSC_5856_11_74.png\n",
            "DSC_5856_11_79.png\n",
            "DSC_5856_11_85.png\n",
            "DSC_5856_11_98.png\n",
            "DSC_5856_11_99.png\n",
            "DSC_5856_11_101.png\n",
            "DSC_5856_11_103.png\n",
            "DSC_5856_11_115.png\n",
            "DSC_5856_11_116.png\n",
            "DSC_5856_11_122.png\n",
            "DSC_5856_11_133.png\n",
            "DSC_5856_11_134.png\n",
            "DSC_5856_11_143.png\n",
            "DSC_5856_11_153.png\n",
            "DSC_5856_11_168.png\n",
            "DSC_5856_11_173.png\n",
            "DSC_5856_11_186.png\n",
            "DSC_5856_11_189.png\n",
            "DSC_5856_11_192.png\n",
            "DSC_5856_11_193.png\n",
            "DSC_5856_11_207.png\n",
            "DSC_5856_11_220.png\n",
            "DSC_5856_11_227.png\n",
            "DSC_5856_11_241.png\n",
            "DSC_5856_11_243.png\n",
            "DSC_5856_11_248.png\n",
            "DSC_5856_11_251.png\n",
            "DSC_5856_11_261.png\n",
            "DSC_5856_11_262.png\n",
            "DSC_5856_11_264.png\n",
            "DSC_5856_11_267.png\n",
            "DSC_5856_11_268.png\n",
            "DSC_5856_11_275.png\n",
            "DSC_5856_11_279.png\n",
            "DSC_5856_11_282.png\n",
            "DSC_5856_11_284.png\n",
            "DSC_5856_11_287.png\n",
            "DSC_5856_11_289.png\n",
            "DSC_5856_11_291.png\n",
            "DSC_5856_11_294.png\n",
            "DSC_5856_11_296.png\n",
            "DSC_5856_11_303.png\n",
            "DSC_5856_11_313.png\n",
            "DSC_5856_11_317.png\n",
            "DSC_5856_11_322.png\n",
            "DSC_5856_11_334.png\n",
            "DSC_5856_11_339.png\n",
            "DSC_5856_11_354.png\n",
            "DSC_5856_11_358.png\n",
            "DSC_5856_11_361.png\n",
            "DSC_5856_11_363.png\n",
            "DSC_5856_11_370.png\n",
            "DSC_5856_11_376.png\n",
            "DSC_5856_11_378.png\n",
            "DSC_5856_11_386.png\n",
            "DSC_5856_11_387.png\n",
            "DSC_5856_11_394.png\n",
            "DSC_5856_11_399.png\n",
            "DSC_5856_11_403.png\n",
            "DSC_5856_11_416.png\n",
            "DSC_5856_11_424.png\n",
            "DSC_5856_11_429.png\n",
            "DSC_5856_11_430.png\n",
            "DSC_5856_11_434.png\n",
            "DSC_5856_11_436.png\n",
            "DSC_5856_11_437.png\n",
            "DSC_5856_11_442.png\n",
            "DSC_5856_11_445.png\n",
            "DSC_5856_11_446.png\n",
            "DSC_5856_11_449.png\n",
            "DSC_5856_11_455.png\n",
            "DSC_5856_11_477.png\n",
            "DSC_5856_11_480.png\n",
            "DSC_5856_11_490.png\n",
            "DSC_5856_2_18.png\n",
            "DSC_5856_2_35.png\n",
            "DSC_5856_2_41.png\n",
            "DSC_5856_2_49.png\n",
            "DSC_5856_2_50.png\n",
            "DSC_5856_2_54.png\n",
            "DSC_5856_2_56.png\n",
            "DSC_5856_2_57.png\n",
            "DSC_5856_2_60.png\n",
            "DSC_5856_2_62.png\n",
            "DSC_5856_2_63.png\n",
            "DSC_5856_2_65.png\n",
            "DSC_5856_2_70.png\n",
            "DSC_5856_2_71.png\n",
            "DSC_5856_2_73.png\n",
            "DSC_5856_2_81.png\n",
            "DSC_5856_2_85.png\n",
            "DSC_5856_2_86.png\n",
            "DSC_5856_2_95.png\n",
            "DSC_5856_2_97.png\n",
            "DSC_5856_2_100.png\n",
            "DSC_5856_2_104.png\n",
            "DSC_5856_2_108.png\n",
            "DSC_5856_2_109.png\n",
            "DSC_5856_2_116.png\n",
            "DSC_5856_2_117.png\n",
            "DSC_5856_2_123.png\n",
            "DSC_5856_2_128.png\n",
            "DSC_5856_2_130.png\n",
            "DSC_5856_2_132.png\n",
            "DSC_5856_2_135.png\n",
            "DSC_5856_2_143.png\n",
            "DSC_5856_2_149.png\n",
            "DSC_5856_2_150.png\n",
            "DSC_5856_2_161.png\n",
            "DSC_5856_2_164.png\n",
            "DSC_5856_2_168.png\n",
            "DSC_5856_2_173.png\n",
            "DSC_5856_2_175.png\n",
            "DSC_5856_2_176.png\n",
            "DSC_5856_2_179.png\n",
            "DSC_5856_2_183.png\n",
            "DSC_5856_2_189.png\n",
            "DSC_5856_2_191.png\n",
            "DSC_5856_2_192.png\n",
            "DSC_5856_2_197.png\n",
            "DSC_5856_2_200.png\n",
            "DSC_5856_2_202.png\n",
            "DSC_5856_2_204.png\n",
            "DSC_5856_2_206.png\n",
            "DSC_5856_2_207.png\n",
            "DSC_5856_2_210.png\n",
            "DSC_5856_2_218.png\n",
            "DSC_5856_2_220.png\n",
            "DSC_5856_2_223.png\n",
            "DSC_5856_2_232.png\n",
            "DSC_5856_2_237.png\n",
            "DSC_5856_2_239.png\n",
            "DSC_5856_2_241.png\n",
            "DSC_5856_2_244.png\n",
            "DSC_5856_2_246.png\n",
            "DSC_5856_2_250.png\n",
            "DSC_5856_2_252.png\n",
            "DSC_5856_2_255.png\n",
            "DSC_5856_2_260.png\n",
            "DSC_5856_2_261.png\n",
            "DSC_5856_2_264.png\n",
            "DSC_5856_2_266.png\n",
            "DSC_5856_2_269.png\n",
            "DSC_5856_2_271.png\n",
            "DSC_5856_2_273.png\n",
            "DSC_5856_2_277.png\n",
            "DSC_5856_2_280.png\n",
            "DSC_5856_2_295.png\n",
            "DSC_5856_2_300.png\n",
            "DSC_5856_2_303.png\n",
            "DSC_5856_2_306.png\n",
            "DSC_5856_2_311.png\n",
            "DSC_5856_2_313.png\n",
            "DSC_5856_2_316.png\n",
            "DSC_5856_2_318.png\n",
            "DSC_5856_2_320.png\n",
            "DSC_5856_2_335.png\n",
            "DSC_5856_2_340.png\n",
            "DSC_5856_2_347.png\n",
            "DSC_5856_2_348.png\n",
            "DSC_5856_2_353.png\n",
            "DSC_5856_2_356.png\n",
            "DSC_5856_2_357.png\n",
            "DSC_5856_2_362.png\n",
            "DSC_5856_2_372.png\n",
            "DSC_5856_2_375.png\n",
            "DSC_5856_2_389.png\n",
            "DSC_5856_2_393.png\n",
            "DSC_5856_2_405.png\n",
            "DSC_5856_2_413.png\n",
            "DSC_5856_2_416.png\n",
            "DSC_5856_2_418.png\n",
            "DSC_5856_2_422.png\n",
            "DSC_5856_2_424.png\n",
            "DSC_5856_2_425.png\n",
            "DSC_5856_2_428.png\n",
            "DSC_5856_2_429.png\n",
            "DSC_5856_2_432.png\n",
            "DSC_5856_2_434.png\n",
            "DSC_5856_2_442.png\n",
            "DSC_5856_2_444.png\n",
            "DSC_5856_2_446.png\n",
            "DSC_5856_2_460.png\n",
            "DSC_5856_2_471.png\n",
            "DSC_5856_2_482.png\n",
            "DSC_5856_2_487.png\n",
            "DSC_5856_2_498.png\n",
            "DSC_5856_31_1.png\n",
            "DSC_5856_31_2.png\n",
            "DSC_5856_31_12.png\n",
            "DSC_5856_31_14.png\n",
            "DSC_5856_31_16.png\n",
            "DSC_5856_31_18.png\n",
            "DSC_5856_31_19.png\n",
            "DSC_5856_31_27.png\n",
            "DSC_5856_31_28.png\n",
            "DSC_5856_31_31.png\n",
            "DSC_5856_31_32.png\n",
            "DSC_5856_31_39.png\n",
            "DSC_5856_31_41.png\n",
            "DSC_5856_31_43.png\n",
            "DSC_5856_31_54.png\n",
            "DSC_5856_31_55.png\n",
            "DSC_5856_31_58.png\n",
            "DSC_5856_31_70.png\n",
            "DSC_5856_31_81.png\n",
            "DSC_5856_31_85.png\n",
            "DSC_5856_31_88.png\n",
            "DSC_5856_31_90.png\n",
            "DSC_5856_31_94.png\n",
            "DSC_5856_31_104.png\n",
            "DSC_5856_31_120.png\n",
            "DSC_5856_31_122.png\n",
            "DSC_5856_31_124.png\n",
            "DSC_5856_31_125.png\n",
            "DSC_5856_31_127.png\n",
            "DSC_5856_31_130.png\n",
            "DSC_5856_31_132.png\n",
            "DSC_5856_31_134.png\n",
            "DSC_5856_31_136.png\n",
            "DSC_5856_31_138.png\n",
            "DSC_5856_31_147.png\n",
            "DSC_5856_31_149.png\n",
            "DSC_5856_31_152.png\n",
            "DSC_5856_31_153.png\n",
            "DSC_5856_31_157.png\n",
            "DSC_5856_31_160.png\n",
            "DSC_5856_31_168.png\n",
            "DSC_5856_31_177.png\n",
            "DSC_5856_31_178.png\n",
            "DSC_5856_31_181.png\n",
            "DSC_5856_31_184.png\n",
            "DSC_5856_31_185.png\n",
            "DSC_5856_31_201.png\n",
            "DSC_5856_31_202.png\n",
            "DSC_5856_31_205.png\n",
            "DSC_5856_31_206.png\n",
            "DSC_5856_31_208.png\n",
            "DSC_5856_31_209.png\n",
            "DSC_5856_31_220.png\n",
            "DSC_5856_31_221.png\n",
            "DSC_5856_31_224.png\n",
            "DSC_5856_31_226.png\n",
            "DSC_5856_31_227.png\n",
            "DSC_5856_31_229.png\n",
            "DSC_5856_31_236.png\n",
            "DSC_5856_31_241.png\n",
            "DSC_5856_31_246.png\n",
            "DSC_5856_31_252.png\n",
            "DSC_5856_31_261.png\n",
            "DSC_5856_31_262.png\n",
            "DSC_5856_31_264.png\n",
            "DSC_5856_31_267.png\n",
            "DSC_5856_31_268.png\n",
            "DSC_5856_31_270.png\n",
            "DSC_5856_31_272.png\n",
            "DSC_5856_31_293.png\n",
            "DSC_5856_31_294.png\n",
            "DSC_5856_31_298.png\n",
            "DSC_5856_31_299.png\n",
            "DSC_5856_31_302.png\n",
            "DSC_5856_31_303.png\n",
            "DSC_5856_31_310.png\n",
            "DSC_5856_31_311.png\n",
            "DSC_5856_31_313.png\n",
            "DSC_5856_31_320.png\n",
            "DSC_5856_31_324.png\n",
            "DSC_5856_31_326.png\n",
            "DSC_5856_31_328.png\n",
            "DSC_5856_31_330.png\n",
            "DSC_5856_31_332.png\n",
            "DSC_5856_31_334.png\n",
            "DSC_5856_31_336.png\n",
            "DSC_5856_31_338.png\n",
            "DSC_5856_31_349.png\n",
            "DSC_5856_31_353.png\n",
            "DSC_5856_31_354.png\n",
            "DSC_5856_31_365.png\n",
            "DSC_5856_31_368.png\n",
            "DSC_5856_31_370.png\n",
            "DSC_5856_31_371.png\n",
            "DSC_5856_31_374.png\n",
            "DSC_5856_31_384.png\n",
            "DSC_5856_31_386.png\n",
            "DSC_5856_31_403.png\n",
            "DSC_5856_31_405.png\n",
            "DSC_5856_31_414.png\n",
            "DSC_5856_31_415.png\n",
            "DSC_5856_31_417.png\n",
            "DSC_5856_31_420.png\n",
            "DSC_5856_31_421.png\n",
            "DSC_5856_31_429.png\n",
            "DSC_5856_31_430.png\n",
            "DSC_5856_31_443.png\n",
            "DSC_5856_31_455.png\n",
            "DSC_5856_31_457.png\n",
            "DSC_5856_31_458.png\n",
            "DSC_5856_31_460.png\n",
            "DSC_5856_31_462.png\n",
            "DSC_5856_31_472.png\n",
            "DSC_5856_31_473.png\n",
            "DSC_5856_31_475.png\n",
            "DSC_5856_31_486.png\n",
            "DSC_5856_31_487.png\n",
            "DSC_5856_31_495.png\n",
            "DSC_5856_31_497.png\n",
            "DSC_5856_31_499.png\n",
            "DSC_5856_52_5.png\n",
            "DSC_5856_52_6.png\n",
            "DSC_5856_52_7.png\n",
            "DSC_5856_52_13.png\n",
            "DSC_5856_52_14.png\n",
            "DSC_5856_52_16.png\n",
            "DSC_5856_52_18.png\n",
            "DSC_5856_52_19.png\n",
            "DSC_5856_52_21.png\n",
            "DSC_5856_52_22.png\n",
            "DSC_5856_52_27.png\n",
            "DSC_5856_52_28.png\n",
            "DSC_5856_52_29.png\n",
            "DSC_5856_52_31.png\n",
            "DSC_5856_52_33.png\n",
            "DSC_5856_52_34.png\n",
            "DSC_5856_52_35.png\n",
            "DSC_5856_52_36.png\n",
            "DSC_5856_52_38.png\n",
            "DSC_5856_52_42.png\n",
            "DSC_5856_52_43.png\n",
            "DSC_5856_52_46.png\n",
            "DSC_5856_52_49.png\n",
            "DSC_5856_52_54.png\n",
            "DSC_5856_52_56.png\n",
            "DSC_5856_52_60.png\n",
            "DSC_5856_52_63.png\n",
            "DSC_5856_52_69.png\n",
            "DSC_5856_52_71.png\n",
            "DSC_5856_52_72.png\n",
            "DSC_5856_52_73.png\n",
            "DSC_5856_52_76.png\n",
            "DSC_5856_52_78.png\n",
            "DSC_5856_52_79.png\n",
            "DSC_5856_52_81.png\n",
            "DSC_5856_52_84.png\n",
            "DSC_5856_52_85.png\n",
            "DSC_5856_52_87.png\n",
            "DSC_5856_52_88.png\n",
            "DSC_5856_52_94.png\n",
            "DSC_5856_52_96.png\n",
            "DSC_5856_52_98.png\n",
            "DSC_5856_52_99.png\n",
            "DSC_5856_52_103.png\n",
            "DSC_5856_52_105.png\n",
            "DSC_5856_52_109.png\n",
            "DSC_5856_52_110.png\n",
            "DSC_5856_52_114.png\n",
            "DSC_5856_52_116.png\n",
            "DSC_5856_52_117.png\n",
            "DSC_5856_52_120.png\n",
            "DSC_5856_52_123.png\n",
            "DSC_5856_52_124.png\n",
            "DSC_5856_52_131.png\n",
            "DSC_5856_52_133.png\n",
            "DSC_5856_52_139.png\n",
            "DSC_5856_52_143.png\n",
            "DSC_5856_52_153.png\n",
            "DSC_5856_52_155.png\n",
            "DSC_5856_52_157.png\n",
            "DSC_5856_52_159.png\n",
            "DSC_5856_52_161.png\n",
            "DSC_5856_52_163.png\n",
            "DSC_5856_52_164.png\n",
            "DSC_5856_52_166.png\n",
            "DSC_5856_52_168.png\n",
            "DSC_5856_52_170.png\n",
            "DSC_5856_52_173.png\n",
            "DSC_5856_52_174.png\n",
            "DSC_5856_52_175.png\n",
            "DSC_5856_52_176.png\n",
            "DSC_5856_52_177.png\n",
            "DSC_5856_52_184.png\n",
            "DSC_5856_52_185.png\n",
            "DSC_5856_52_188.png\n",
            "DSC_5856_52_189.png\n",
            "DSC_5856_52_190.png\n",
            "DSC_5856_52_192.png\n",
            "DSC_5856_52_198.png\n",
            "DSC_5856_52_199.png\n",
            "DSC_5856_52_201.png\n",
            "DSC_5856_52_204.png\n",
            "DSC_5856_52_207.png\n",
            "DSC_5856_52_208.png\n",
            "DSC_5856_52_212.png\n",
            "DSC_5856_52_213.png\n",
            "DSC_5856_52_214.png\n",
            "DSC_5856_52_215.png\n",
            "DSC_5856_52_216.png\n",
            "DSC_5856_52_217.png\n",
            "DSC_5856_52_218.png\n",
            "DSC_5856_52_219.png\n",
            "DSC_5856_52_221.png\n",
            "DSC_5856_52_224.png\n",
            "DSC_5856_52_226.png\n",
            "DSC_5856_52_227.png\n",
            "DSC_5856_52_229.png\n",
            "DSC_5856_52_231.png\n",
            "DSC_5856_52_233.png\n",
            "DSC_5856_52_234.png\n",
            "DSC_5856_52_238.png\n",
            "DSC_5856_52_239.png\n",
            "DSC_5856_52_247.png\n",
            "DSC_5856_52_248.png\n",
            "DSC_5856_52_249.png\n",
            "DSC_5856_52_255.png\n",
            "DSC_5856_52_256.png\n",
            "DSC_5856_52_258.png\n",
            "DSC_5856_52_261.png\n",
            "DSC_5856_52_269.png\n",
            "DSC_5856_52_272.png\n",
            "DSC_5856_52_275.png\n",
            "DSC_5856_52_277.png\n",
            "DSC_5856_52_278.png\n",
            "DSC_5856_52_279.png\n",
            "DSC_5856_52_281.png\n",
            "DSC_5856_52_289.png\n",
            "DSC_5856_52_291.png\n",
            "DSC_5856_52_300.png\n",
            "DSC_5856_52_303.png\n",
            "DSC_5856_52_304.png\n",
            "DSC_5856_52_305.png\n",
            "DSC_5856_52_306.png\n",
            "DSC_5856_52_308.png\n",
            "DSC_5856_52_309.png\n",
            "DSC_5856_52_311.png\n",
            "DSC_5856_52_312.png\n",
            "DSC_5856_52_313.png\n",
            "DSC_5856_52_320.png\n",
            "DSC_5856_52_322.png\n",
            "DSC_5856_52_329.png\n",
            "DSC_5856_52_330.png\n",
            "DSC_5856_52_336.png\n",
            "DSC_5856_52_338.png\n",
            "DSC_5856_52_340.png\n",
            "DSC_5856_52_341.png\n",
            "DSC_5856_52_343.png\n",
            "DSC_5856_52_349.png\n",
            "DSC_5856_52_351.png\n",
            "DSC_5856_52_353.png\n",
            "DSC_5856_52_354.png\n",
            "DSC_5856_52_355.png\n",
            "DSC_5856_52_357.png\n",
            "DSC_5856_52_358.png\n",
            "DSC_5856_52_361.png\n",
            "DSC_5856_52_363.png\n",
            "DSC_5856_52_364.png\n",
            "DSC_5856_52_366.png\n",
            "DSC_5856_52_370.png\n",
            "DSC_5856_52_374.png\n",
            "DSC_5856_52_376.png\n",
            "DSC_5856_52_377.png\n",
            "DSC_5856_52_381.png\n",
            "DSC_5856_52_382.png\n",
            "DSC_5856_52_383.png\n",
            "DSC_5856_52_384.png\n",
            "DSC_5856_52_386.png\n",
            "DSC_5856_52_389.png\n",
            "DSC_5856_52_391.png\n",
            "DSC_5856_52_392.png\n",
            "DSC_5856_52_395.png\n",
            "DSC_5856_52_401.png\n",
            "DSC_5856_52_403.png\n",
            "DSC_5856_52_405.png\n",
            "DSC_5856_52_408.png\n",
            "DSC_5856_52_409.png\n",
            "DSC_5856_52_413.png\n",
            "DSC_5856_52_414.png\n",
            "DSC_5856_52_416.png\n",
            "DSC_5856_52_417.png\n",
            "DSC_5856_52_418.png\n",
            "DSC_5856_52_419.png\n",
            "DSC_5856_52_425.png\n",
            "DSC_5856_52_429.png\n",
            "DSC_5856_52_433.png\n",
            "DSC_5856_52_434.png\n",
            "DSC_5856_52_437.png\n",
            "DSC_5856_52_438.png\n",
            "DSC_5856_52_440.png\n",
            "DSC_5856_52_441.png\n",
            "DSC_5856_52_442.png\n",
            "DSC_5856_52_444.png\n",
            "DSC_5856_52_449.png\n",
            "DSC_5856_52_450.png\n",
            "DSC_5856_52_455.png\n",
            "DSC_5856_52_456.png\n",
            "DSC_5856_52_461.png\n",
            "DSC_5856_52_464.png\n",
            "DSC_5856_52_466.png\n",
            "DSC_5856_52_468.png\n",
            "DSC_5856_52_470.png\n",
            "DSC_5856_52_474.png\n",
            "DSC_5856_52_476.png\n",
            "DSC_5856_52_477.png\n",
            "DSC_5856_52_481.png\n",
            "DSC_5856_52_487.png\n",
            "DSC_5856_52_491.png\n",
            "DSC_5856_52_495.png\n",
            "DSC_5856_52_496.png\n",
            "DSC_5856_12_512.png\n",
            "DSC_5856_12_528.png\n",
            "DSC_5856_12_531.png\n",
            "DSC_5856_12_544.png\n",
            "DSC_5856_12_565.png\n",
            "DSC_5856_12_599.png\n",
            "DSC_5856_12_608.png\n",
            "DSC_5856_12_618.png\n",
            "DSC_5856_12_647.png\n",
            "DSC_5856_12_660.png\n",
            "DSC_5856_12_673.png\n",
            "DSC_5856_12_696.png\n",
            "DSC_5856_12_744.png\n",
            "DSC_5856_12_783.png\n",
            "DSC_5856_12_797.png\n",
            "DSC_5856_12_816.png\n",
            "DSC_5856_12_845.png\n",
            "DSC_5856_12_857.png\n",
            "DSC_5856_12_872.png\n",
            "DSC_5856_12_902.png\n",
            "DSC_5856_12_907.png\n",
            "DSC_5856_12_937.png\n",
            "DSC_5856_12_940.png\n",
            "DSC_5856_12_948.png\n",
            "DSC_5856_12_961.png\n",
            "DSC_5856_12_980.png\n",
            "DSC_5856_12_985.png\n",
            "DSC_5856_12_993.png\n",
            "DSC_5856_12_1061.png\n",
            "DSC_5856_12_1070.png\n",
            "DSC_5856_12_1112.png\n",
            "DSC_5856_12_1114.png\n",
            "DSC_5856_12_1135.png\n",
            "DSC_5856_12_1136.png\n",
            "DSC_5856_12_1138.png\n",
            "DSC_5856_12_1141.png\n",
            "DSC_5856_12_1147.png\n",
            "DSC_5856_12_1186.png\n",
            "DSC_5856_12_1250.png\n",
            "DSC_5856_12_1272.png\n",
            "DSC_5856_12_1321.png\n",
            "DSC_5856_12_1333.png\n",
            "DSC_5856_12_1335.png\n",
            "DSC_5856_12_1337.png\n",
            "DSC_5856_12_1339.png\n",
            "DSC_5856_12_1343.png\n",
            "DSC_5856_12_1351.png\n",
            "DSC_5856_12_1368.png\n",
            "DSC_5856_12_1369.png\n",
            "DSC_5856_12_1380.png\n",
            "DSC_5856_12_1391.png\n",
            "DSC_5856_12_1399.png\n",
            "DSC_5856_12_1408.png\n",
            "DSC_5856_12_1414.png\n",
            "DSC_5856_12_1427.png\n",
            "DSC_5856_12_1428.png\n",
            "DSC_5856_12_1441.png\n",
            "DSC_5856_12_1463.png\n",
            "DSC_5856_12_1465.png\n",
            "DSC_5856_12_1475.png\n",
            "DSC_5856_12_1483.png\n",
            "DSC_5856_12_1487.png\n",
            "DSC_5856_12_1493.png\n",
            "DSC_5856_13_500.png\n",
            "DSC_5856_13_506.png\n",
            "DSC_5856_13_555.png\n",
            "DSC_5856_13_582.png\n",
            "DSC_5856_13_609.png\n",
            "DSC_5856_13_615.png\n",
            "DSC_5856_13_703.png\n",
            "DSC_5856_13_733.png\n",
            "DSC_5856_13_739.png\n",
            "DSC_5856_13_745.png\n",
            "DSC_5856_13_850.png\n",
            "DSC_5856_13_938.png\n",
            "DSC_5856_13_1132.png\n",
            "DSC_5856_13_1137.png\n",
            "DSC_5856_13_1149.png\n",
            "DSC_5856_13_1169.png\n",
            "DSC_5856_13_1195.png\n",
            "DSC_5856_13_1197.png\n",
            "DSC_5856_13_1249.png\n",
            "DSC_5856_13_1271.png\n",
            "DSC_5856_13_1282.png\n",
            "DSC_5856_13_1285.png\n",
            "DSC_5856_13_1313.png\n",
            "DSC_5856_13_1336.png\n",
            "DSC_5856_13_1346.png\n",
            "DSC_5856_13_1366.png\n",
            "DSC_5856_13_1367.png\n",
            "DSC_5856_13_1454.png\n",
            "DSC_5856_13_1474.png\n",
            "DSC_5856_21_502.png\n",
            "DSC_5856_21_522.png\n",
            "DSC_5856_21_526.png\n",
            "DSC_5856_21_557.png\n",
            "DSC_5856_21_561.png\n",
            "DSC_5856_21_573.png\n",
            "DSC_5856_21_586.png\n",
            "DSC_5856_21_593.png\n",
            "DSC_5856_21_594.png\n",
            "DSC_5856_21_619.png\n",
            "DSC_5856_21_627.png\n",
            "DSC_5856_21_629.png\n",
            "DSC_5856_21_630.png\n",
            "DSC_5856_21_641.png\n",
            "DSC_5856_21_653.png\n",
            "DSC_5856_21_659.png\n",
            "DSC_5856_21_661.png\n",
            "DSC_5856_21_681.png\n",
            "DSC_5856_21_685.png\n",
            "DSC_5856_21_690.png\n",
            "DSC_5856_21_701.png\n",
            "DSC_5856_21_707.png\n",
            "DSC_5856_21_713.png\n",
            "DSC_5856_21_717.png\n",
            "DSC_5856_21_729.png\n",
            "DSC_5856_21_741.png\n",
            "DSC_5856_21_757.png\n",
            "DSC_5856_21_770.png\n",
            "DSC_5856_21_773.png\n",
            "DSC_5856_21_778.png\n",
            "DSC_5856_21_779.png\n",
            "DSC_5856_21_785.png\n",
            "DSC_5856_21_813.png\n",
            "DSC_5856_21_814.png\n",
            "DSC_5856_21_825.png\n",
            "DSC_5856_21_832.png\n",
            "DSC_5856_21_849.png\n",
            "DSC_5856_21_852.png\n",
            "DSC_5856_21_854.png\n",
            "DSC_5856_21_865.png\n",
            "DSC_5856_21_866.png\n",
            "DSC_5856_21_878.png\n",
            "DSC_5856_21_885.png\n",
            "DSC_5856_21_889.png\n",
            "DSC_5856_21_894.png\n",
            "DSC_5856_21_904.png\n",
            "DSC_5856_21_906.png\n",
            "DSC_5856_21_910.png\n",
            "DSC_5856_21_927.png\n",
            "DSC_5856_21_930.png\n",
            "DSC_5856_21_947.png\n",
            "DSC_5856_21_956.png\n",
            "DSC_5856_21_966.png\n",
            "DSC_5856_21_967.png\n",
            "DSC_5856_21_971.png\n",
            "DSC_5856_21_972.png\n",
            "DSC_5856_21_978.png\n",
            "DSC_5856_21_983.png\n",
            "DSC_5856_21_984.png\n",
            "DSC_5856_21_992.png\n",
            "DSC_5856_21_995.png\n",
            "DSC_5856_21_1007.png\n",
            "DSC_5856_21_1009.png\n",
            "DSC_5856_21_1012.png\n",
            "DSC_5856_21_1027.png\n",
            "DSC_5856_21_1037.png\n",
            "DSC_5856_21_1076.png\n",
            "DSC_5856_21_1084.png\n",
            "DSC_5856_21_1087.png\n",
            "DSC_5856_21_1092.png\n",
            "DSC_5856_21_1097.png\n",
            "DSC_5856_21_1101.png\n",
            "DSC_5856_21_1103.png\n",
            "DSC_5856_21_1110.png\n",
            "DSC_5856_21_1113.png\n",
            "DSC_5856_21_1117.png\n",
            "DSC_5856_21_1145.png\n",
            "DSC_5856_21_1146.png\n",
            "DSC_5856_21_1148.png\n",
            "DSC_5856_21_1149.png\n",
            "DSC_5856_21_1157.png\n",
            "DSC_5856_21_1158.png\n",
            "DSC_5856_21_1159.png\n",
            "DSC_5856_21_1162.png\n",
            "DSC_5856_21_1167.png\n",
            "DSC_5856_21_1174.png\n",
            "DSC_5856_21_1178.png\n",
            "DSC_5856_21_1188.png\n",
            "DSC_5856_21_1208.png\n",
            "DSC_5856_21_1221.png\n",
            "DSC_5856_21_1228.png\n",
            "DSC_5856_21_1234.png\n",
            "DSC_5856_21_1235.png\n",
            "DSC_5856_21_1239.png\n",
            "DSC_5856_21_1247.png\n",
            "DSC_5856_21_1261.png\n",
            "DSC_5856_21_1266.png\n",
            "DSC_5856_21_1271.png\n",
            "DSC_5856_21_1285.png\n",
            "DSC_5856_21_1286.png\n",
            "DSC_5856_21_1318.png\n",
            "DSC_5856_21_1334.png\n",
            "DSC_5856_21_1338.png\n",
            "DSC_5856_21_1340.png\n",
            "DSC_5856_21_1348.png\n",
            "DSC_5856_21_1353.png\n",
            "DSC_5856_21_1356.png\n",
            "DSC_5856_21_1358.png\n",
            "DSC_5856_21_1373.png\n",
            "DSC_5856_21_1381.png\n",
            "DSC_5856_21_1392.png\n",
            "DSC_5856_21_1393.png\n",
            "DSC_5856_21_1395.png\n",
            "DSC_5856_21_1401.png\n",
            "DSC_5856_21_1413.png\n",
            "DSC_5856_21_1422.png\n",
            "DSC_5856_21_1432.png\n",
            "DSC_5856_21_1434.png\n",
            "DSC_5856_21_1438.png\n",
            "DSC_5856_21_1448.png\n",
            "DSC_5856_21_1461.png\n",
            "DSC_5856_21_1464.png\n",
            "DSC_5856_21_1470.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIP8qYqLtN5u"
      },
      "source": [
        "data1=df[df['5']==1]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIYc4XEsvAk5"
      },
      "source": [
        "a=data1.values\r\n",
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17NAiwApvm5x",
        "outputId": "c3ff04ad-2257-4d04-ff3f-a4a9f81cd80e"
      },
      "source": [
        "for ln in data:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/1/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_6.png\n",
            "DSC_5856_12_10.png\n",
            "DSC_5856_12_40.png\n",
            "DSC_5856_12_46.png\n",
            "DSC_5856_12_49.png\n",
            "DSC_5856_12_52.png\n",
            "DSC_5856_12_95.png\n",
            "DSC_5856_12_104.png\n",
            "DSC_5856_12_126.png\n",
            "DSC_5856_12_135.png\n",
            "DSC_5856_12_143.png\n",
            "DSC_5856_12_158.png\n",
            "DSC_5856_12_165.png\n",
            "DSC_5856_12_210.png\n",
            "DSC_5856_12_244.png\n",
            "DSC_5856_12_266.png\n",
            "DSC_5856_12_280.png\n",
            "DSC_5856_12_281.png\n",
            "DSC_5856_12_286.png\n",
            "DSC_5856_12_310.png\n",
            "DSC_5856_12_367.png\n",
            "DSC_5856_12_401.png\n",
            "DSC_5856_12_405.png\n",
            "DSC_5856_12_411.png\n",
            "DSC_5856_12_450.png\n",
            "DSC_5856_12_487.png\n",
            "DSC_5856_13_43.png\n",
            "DSC_5856_13_51.png\n",
            "DSC_5856_13_59.png\n",
            "DSC_5856_13_74.png\n",
            "DSC_5856_13_157.png\n",
            "DSC_5856_13_161.png\n",
            "DSC_5856_13_167.png\n",
            "DSC_5856_13_207.png\n",
            "DSC_5856_13_226.png\n",
            "DSC_5856_13_227.png\n",
            "DSC_5856_13_242.png\n",
            "DSC_5856_13_243.png\n",
            "DSC_5856_13_255.png\n",
            "DSC_5856_13_256.png\n",
            "DSC_5856_13_266.png\n",
            "DSC_5856_13_267.png\n",
            "DSC_5856_13_294.png\n",
            "DSC_5856_13_300.png\n",
            "DSC_5856_13_305.png\n",
            "DSC_5856_13_323.png\n",
            "DSC_5856_13_338.png\n",
            "DSC_5856_13_353.png\n",
            "DSC_5856_13_380.png\n",
            "DSC_5856_13_390.png\n",
            "DSC_5856_13_413.png\n",
            "DSC_5856_13_426.png\n",
            "DSC_5856_13_452.png\n",
            "DSC_5856_13_490.png\n",
            "DSC_5856_21_5.png\n",
            "DSC_5856_21_8.png\n",
            "DSC_5856_21_12.png\n",
            "DSC_5856_21_23.png\n",
            "DSC_5856_21_27.png\n",
            "DSC_5856_21_31.png\n",
            "DSC_5856_21_42.png\n",
            "DSC_5856_21_58.png\n",
            "DSC_5856_21_62.png\n",
            "DSC_5856_21_67.png\n",
            "DSC_5856_21_70.png\n",
            "DSC_5856_21_71.png\n",
            "DSC_5856_21_72.png\n",
            "DSC_5856_21_78.png\n",
            "DSC_5856_21_83.png\n",
            "DSC_5856_21_87.png\n",
            "DSC_5856_21_97.png\n",
            "DSC_5856_21_107.png\n",
            "DSC_5856_21_114.png\n",
            "DSC_5856_21_115.png\n",
            "DSC_5856_21_131.png\n",
            "DSC_5856_21_149.png\n",
            "DSC_5856_21_155.png\n",
            "DSC_5856_21_173.png\n",
            "DSC_5856_21_174.png\n",
            "DSC_5856_21_185.png\n",
            "DSC_5856_21_197.png\n",
            "DSC_5856_21_199.png\n",
            "DSC_5856_21_206.png\n",
            "DSC_5856_21_215.png\n",
            "DSC_5856_21_221.png\n",
            "DSC_5856_21_229.png\n",
            "DSC_5856_21_233.png\n",
            "DSC_5856_21_251.png\n",
            "DSC_5856_21_253.png\n",
            "DSC_5856_21_256.png\n",
            "DSC_5856_21_258.png\n",
            "DSC_5856_21_275.png\n",
            "DSC_5856_21_291.png\n",
            "DSC_5856_21_294.png\n",
            "DSC_5856_21_296.png\n",
            "DSC_5856_21_304.png\n",
            "DSC_5856_21_305.png\n",
            "DSC_5856_21_306.png\n",
            "DSC_5856_21_317.png\n",
            "DSC_5856_21_325.png\n",
            "DSC_5856_21_326.png\n",
            "DSC_5856_21_340.png\n",
            "DSC_5856_21_343.png\n",
            "DSC_5856_21_348.png\n",
            "DSC_5856_21_349.png\n",
            "DSC_5856_21_355.png\n",
            "DSC_5856_21_360.png\n",
            "DSC_5856_21_361.png\n",
            "DSC_5856_21_362.png\n",
            "DSC_5856_21_363.png\n",
            "DSC_5856_21_365.png\n",
            "DSC_5856_21_374.png\n",
            "DSC_5856_21_377.png\n",
            "DSC_5856_21_387.png\n",
            "DSC_5856_21_389.png\n",
            "DSC_5856_21_398.png\n",
            "DSC_5856_21_400.png\n",
            "DSC_5856_21_408.png\n",
            "DSC_5856_21_414.png\n",
            "DSC_5856_21_417.png\n",
            "DSC_5856_21_432.png\n",
            "DSC_5856_21_438.png\n",
            "DSC_5856_21_444.png\n",
            "DSC_5856_21_458.png\n",
            "DSC_5856_21_462.png\n",
            "DSC_5856_21_473.png\n",
            "DSC_5856_21_475.png\n",
            "DSC_5856_21_481.png\n",
            "DSC_5856_21_482.png\n",
            "DSC_5856_21_493.png\n",
            "DSC_5856_21_495.png\n",
            "DSC_5856_21_496.png\n",
            "DSC_5856_25_0.png\n",
            "DSC_5856_25_2.png\n",
            "DSC_5856_25_5.png\n",
            "DSC_5856_25_7.png\n",
            "DSC_5856_25_19.png\n",
            "DSC_5856_25_21.png\n",
            "DSC_5856_25_30.png\n",
            "DSC_5856_25_31.png\n",
            "DSC_5856_25_33.png\n",
            "DSC_5856_25_36.png\n",
            "DSC_5856_25_40.png\n",
            "DSC_5856_25_41.png\n",
            "DSC_5856_25_49.png\n",
            "DSC_5856_25_58.png\n",
            "DSC_5856_25_60.png\n",
            "DSC_5856_25_64.png\n",
            "DSC_5856_25_68.png\n",
            "DSC_5856_25_78.png\n",
            "DSC_5856_25_80.png\n",
            "DSC_5856_25_81.png\n",
            "DSC_5856_25_84.png\n",
            "DSC_5856_25_94.png\n",
            "DSC_5856_25_100.png\n",
            "DSC_5856_25_101.png\n",
            "DSC_5856_25_102.png\n",
            "DSC_5856_25_103.png\n",
            "DSC_5856_25_106.png\n",
            "DSC_5856_25_110.png\n",
            "DSC_5856_25_114.png\n",
            "DSC_5856_25_123.png\n",
            "DSC_5856_25_124.png\n",
            "DSC_5856_25_133.png\n",
            "DSC_5856_25_135.png\n",
            "DSC_5856_25_138.png\n",
            "DSC_5856_25_146.png\n",
            "DSC_5856_25_154.png\n",
            "DSC_5856_25_162.png\n",
            "DSC_5856_25_164.png\n",
            "DSC_5856_25_173.png\n",
            "DSC_5856_25_181.png\n",
            "DSC_5856_25_196.png\n",
            "DSC_5856_25_199.png\n",
            "DSC_5856_25_201.png\n",
            "DSC_5856_25_208.png\n",
            "DSC_5856_25_210.png\n",
            "DSC_5856_25_214.png\n",
            "DSC_5856_25_217.png\n",
            "DSC_5856_25_220.png\n",
            "DSC_5856_25_222.png\n",
            "DSC_5856_25_229.png\n",
            "DSC_5856_25_230.png\n",
            "DSC_5856_25_233.png\n",
            "DSC_5856_25_238.png\n",
            "DSC_5856_25_241.png\n",
            "DSC_5856_25_251.png\n",
            "DSC_5856_25_252.png\n",
            "DSC_5856_25_254.png\n",
            "DSC_5856_25_258.png\n",
            "DSC_5856_25_265.png\n",
            "DSC_5856_25_269.png\n",
            "DSC_5856_25_271.png\n",
            "DSC_5856_25_272.png\n",
            "DSC_5856_25_280.png\n",
            "DSC_5856_25_288.png\n",
            "DSC_5856_25_291.png\n",
            "DSC_5856_25_295.png\n",
            "DSC_5856_25_298.png\n",
            "DSC_5856_25_302.png\n",
            "DSC_5856_25_307.png\n",
            "DSC_5856_25_315.png\n",
            "DSC_5856_25_333.png\n",
            "DSC_5856_25_339.png\n",
            "DSC_5856_25_361.png\n",
            "DSC_5856_25_363.png\n",
            "DSC_5856_25_364.png\n",
            "DSC_5856_25_366.png\n",
            "DSC_5856_25_368.png\n",
            "DSC_5856_25_369.png\n",
            "DSC_5856_25_371.png\n",
            "DSC_5856_25_373.png\n",
            "DSC_5856_25_374.png\n",
            "DSC_5856_25_378.png\n",
            "DSC_5856_25_401.png\n",
            "DSC_5856_25_404.png\n",
            "DSC_5856_25_405.png\n",
            "DSC_5856_25_413.png\n",
            "DSC_5856_25_420.png\n",
            "DSC_5856_25_429.png\n",
            "DSC_5856_25_442.png\n",
            "DSC_5856_25_443.png\n",
            "DSC_5856_25_445.png\n",
            "DSC_5856_25_447.png\n",
            "DSC_5856_25_449.png\n",
            "DSC_5856_25_456.png\n",
            "DSC_5856_25_459.png\n",
            "DSC_5856_25_464.png\n",
            "DSC_5856_25_472.png\n",
            "DSC_5856_25_476.png\n",
            "DSC_5856_25_487.png\n",
            "DSC_5856_25_495.png\n",
            "DSC_5856_11_0.png\n",
            "DSC_5856_11_3.png\n",
            "DSC_5856_11_6.png\n",
            "DSC_5856_11_11.png\n",
            "DSC_5856_11_12.png\n",
            "DSC_5856_11_13.png\n",
            "DSC_5856_11_14.png\n",
            "DSC_5856_11_15.png\n",
            "DSC_5856_11_18.png\n",
            "DSC_5856_11_22.png\n",
            "DSC_5856_11_28.png\n",
            "DSC_5856_11_30.png\n",
            "DSC_5856_11_35.png\n",
            "DSC_5856_11_43.png\n",
            "DSC_5856_11_55.png\n",
            "DSC_5856_11_56.png\n",
            "DSC_5856_11_65.png\n",
            "DSC_5856_11_68.png\n",
            "DSC_5856_11_71.png\n",
            "DSC_5856_11_73.png\n",
            "DSC_5856_11_75.png\n",
            "DSC_5856_11_81.png\n",
            "DSC_5856_11_83.png\n",
            "DSC_5856_11_88.png\n",
            "DSC_5856_11_93.png\n",
            "DSC_5856_11_100.png\n",
            "DSC_5856_11_102.png\n",
            "DSC_5856_11_107.png\n",
            "DSC_5856_11_117.png\n",
            "DSC_5856_11_119.png\n",
            "DSC_5856_11_120.png\n",
            "DSC_5856_11_129.png\n",
            "DSC_5856_11_132.png\n",
            "DSC_5856_11_146.png\n",
            "DSC_5856_11_147.png\n",
            "DSC_5856_11_148.png\n",
            "DSC_5856_11_150.png\n",
            "DSC_5856_11_151.png\n",
            "DSC_5856_11_154.png\n",
            "DSC_5856_11_159.png\n",
            "DSC_5856_11_161.png\n",
            "DSC_5856_11_177.png\n",
            "DSC_5856_11_179.png\n",
            "DSC_5856_11_180.png\n",
            "DSC_5856_11_182.png\n",
            "DSC_5856_11_187.png\n",
            "DSC_5856_11_191.png\n",
            "DSC_5856_11_195.png\n",
            "DSC_5856_11_201.png\n",
            "DSC_5856_11_203.png\n",
            "DSC_5856_11_204.png\n",
            "DSC_5856_11_206.png\n",
            "DSC_5856_11_212.png\n",
            "DSC_5856_11_213.png\n",
            "DSC_5856_11_215.png\n",
            "DSC_5856_11_219.png\n",
            "DSC_5856_11_222.png\n",
            "DSC_5856_11_224.png\n",
            "DSC_5856_11_226.png\n",
            "DSC_5856_11_228.png\n",
            "DSC_5856_11_229.png\n",
            "DSC_5856_11_237.png\n",
            "DSC_5856_11_245.png\n",
            "DSC_5856_11_253.png\n",
            "DSC_5856_11_254.png\n",
            "DSC_5856_11_272.png\n",
            "DSC_5856_11_277.png\n",
            "DSC_5856_11_281.png\n",
            "DSC_5856_11_283.png\n",
            "DSC_5856_11_288.png\n",
            "DSC_5856_11_295.png\n",
            "DSC_5856_11_298.png\n",
            "DSC_5856_11_300.png\n",
            "DSC_5856_11_311.png\n",
            "DSC_5856_11_315.png\n",
            "DSC_5856_11_318.png\n",
            "DSC_5856_11_320.png\n",
            "DSC_5856_11_321.png\n",
            "DSC_5856_11_327.png\n",
            "DSC_5856_11_329.png\n",
            "DSC_5856_11_330.png\n",
            "DSC_5856_11_333.png\n",
            "DSC_5856_11_337.png\n",
            "DSC_5856_11_338.png\n",
            "DSC_5856_11_350.png\n",
            "DSC_5856_11_351.png\n",
            "DSC_5856_11_353.png\n",
            "DSC_5856_11_357.png\n",
            "DSC_5856_11_383.png\n",
            "DSC_5856_11_395.png\n",
            "DSC_5856_11_400.png\n",
            "DSC_5856_11_401.png\n",
            "DSC_5856_11_405.png\n",
            "DSC_5856_11_407.png\n",
            "DSC_5856_11_413.png\n",
            "DSC_5856_11_425.png\n",
            "DSC_5856_11_426.png\n",
            "DSC_5856_11_441.png\n",
            "DSC_5856_11_452.png\n",
            "DSC_5856_11_464.png\n",
            "DSC_5856_11_466.png\n",
            "DSC_5856_11_469.png\n",
            "DSC_5856_11_471.png\n",
            "DSC_5856_11_474.png\n",
            "DSC_5856_11_485.png\n",
            "DSC_5856_11_487.png\n",
            "DSC_5856_11_488.png\n",
            "DSC_5856_11_492.png\n",
            "DSC_5856_2_1.png\n",
            "DSC_5856_2_6.png\n",
            "DSC_5856_2_15.png\n",
            "DSC_5856_2_16.png\n",
            "DSC_5856_2_22.png\n",
            "DSC_5856_2_23.png\n",
            "DSC_5856_2_27.png\n",
            "DSC_5856_2_32.png\n",
            "DSC_5856_2_33.png\n",
            "DSC_5856_2_34.png\n",
            "DSC_5856_2_42.png\n",
            "DSC_5856_2_51.png\n",
            "DSC_5856_2_52.png\n",
            "DSC_5856_2_58.png\n",
            "DSC_5856_2_61.png\n",
            "DSC_5856_2_66.png\n",
            "DSC_5856_2_68.png\n",
            "DSC_5856_2_72.png\n",
            "DSC_5856_2_76.png\n",
            "DSC_5856_2_77.png\n",
            "DSC_5856_2_90.png\n",
            "DSC_5856_2_93.png\n",
            "DSC_5856_2_99.png\n",
            "DSC_5856_2_102.png\n",
            "DSC_5856_2_103.png\n",
            "DSC_5856_2_105.png\n",
            "DSC_5856_2_110.png\n",
            "DSC_5856_2_118.png\n",
            "DSC_5856_2_119.png\n",
            "DSC_5856_2_121.png\n",
            "DSC_5856_2_124.png\n",
            "DSC_5856_2_126.png\n",
            "DSC_5856_2_127.png\n",
            "DSC_5856_2_129.png\n",
            "DSC_5856_2_134.png\n",
            "DSC_5856_2_136.png\n",
            "DSC_5856_2_145.png\n",
            "DSC_5856_2_158.png\n",
            "DSC_5856_2_159.png\n",
            "DSC_5856_2_160.png\n",
            "DSC_5856_2_163.png\n",
            "DSC_5856_2_166.png\n",
            "DSC_5856_2_172.png\n",
            "DSC_5856_2_174.png\n",
            "DSC_5856_2_177.png\n",
            "DSC_5856_2_178.png\n",
            "DSC_5856_2_180.png\n",
            "DSC_5856_2_181.png\n",
            "DSC_5856_2_184.png\n",
            "DSC_5856_2_185.png\n",
            "DSC_5856_2_187.png\n",
            "DSC_5856_2_201.png\n",
            "DSC_5856_2_203.png\n",
            "DSC_5856_2_209.png\n",
            "DSC_5856_2_213.png\n",
            "DSC_5856_2_214.png\n",
            "DSC_5856_2_216.png\n",
            "DSC_5856_2_224.png\n",
            "DSC_5856_2_226.png\n",
            "DSC_5856_2_227.png\n",
            "DSC_5856_2_228.png\n",
            "DSC_5856_2_229.png\n",
            "DSC_5856_2_233.png\n",
            "DSC_5856_2_248.png\n",
            "DSC_5856_2_256.png\n",
            "DSC_5856_2_257.png\n",
            "DSC_5856_2_258.png\n",
            "DSC_5856_2_275.png\n",
            "DSC_5856_2_279.png\n",
            "DSC_5856_2_293.png\n",
            "DSC_5856_2_297.png\n",
            "DSC_5856_2_299.png\n",
            "DSC_5856_2_309.png\n",
            "DSC_5856_2_310.png\n",
            "DSC_5856_2_312.png\n",
            "DSC_5856_2_314.png\n",
            "DSC_5856_2_324.png\n",
            "DSC_5856_2_325.png\n",
            "DSC_5856_2_326.png\n",
            "DSC_5856_2_327.png\n",
            "DSC_5856_2_334.png\n",
            "DSC_5856_2_336.png\n",
            "DSC_5856_2_338.png\n",
            "DSC_5856_2_341.png\n",
            "DSC_5856_2_343.png\n",
            "DSC_5856_2_345.png\n",
            "DSC_5856_2_346.png\n",
            "DSC_5856_2_350.png\n",
            "DSC_5856_2_351.png\n",
            "DSC_5856_2_355.png\n",
            "DSC_5856_2_358.png\n",
            "DSC_5856_2_359.png\n",
            "DSC_5856_2_360.png\n",
            "DSC_5856_2_361.png\n",
            "DSC_5856_2_363.png\n",
            "DSC_5856_2_371.png\n",
            "DSC_5856_2_374.png\n",
            "DSC_5856_2_376.png\n",
            "DSC_5856_2_377.png\n",
            "DSC_5856_2_381.png\n",
            "DSC_5856_2_386.png\n",
            "DSC_5856_2_391.png\n",
            "DSC_5856_2_400.png\n",
            "DSC_5856_2_404.png\n",
            "DSC_5856_2_407.png\n",
            "DSC_5856_2_408.png\n",
            "DSC_5856_2_410.png\n",
            "DSC_5856_2_412.png\n",
            "DSC_5856_2_426.png\n",
            "DSC_5856_2_437.png\n",
            "DSC_5856_2_441.png\n",
            "DSC_5856_2_458.png\n",
            "DSC_5856_2_461.png\n",
            "DSC_5856_2_464.png\n",
            "DSC_5856_2_468.png\n",
            "DSC_5856_2_473.png\n",
            "DSC_5856_2_475.png\n",
            "DSC_5856_2_476.png\n",
            "DSC_5856_2_477.png\n",
            "DSC_5856_2_478.png\n",
            "DSC_5856_2_479.png\n",
            "DSC_5856_2_481.png\n",
            "DSC_5856_2_485.png\n",
            "DSC_5856_2_486.png\n",
            "DSC_5856_2_491.png\n",
            "DSC_5856_2_496.png\n",
            "DSC_5856_2_497.png\n",
            "DSC_5856_31_0.png\n",
            "DSC_5856_31_3.png\n",
            "DSC_5856_31_10.png\n",
            "DSC_5856_31_11.png\n",
            "DSC_5856_31_13.png\n",
            "DSC_5856_31_17.png\n",
            "DSC_5856_31_20.png\n",
            "DSC_5856_31_25.png\n",
            "DSC_5856_31_26.png\n",
            "DSC_5856_31_30.png\n",
            "DSC_5856_31_33.png\n",
            "DSC_5856_31_40.png\n",
            "DSC_5856_31_52.png\n",
            "DSC_5856_31_53.png\n",
            "DSC_5856_31_56.png\n",
            "DSC_5856_31_60.png\n",
            "DSC_5856_31_62.png\n",
            "DSC_5856_31_63.png\n",
            "DSC_5856_31_64.png\n",
            "DSC_5856_31_69.png\n",
            "DSC_5856_31_71.png\n",
            "DSC_5856_31_76.png\n",
            "DSC_5856_31_82.png\n",
            "DSC_5856_31_83.png\n",
            "DSC_5856_31_86.png\n",
            "DSC_5856_31_87.png\n",
            "DSC_5856_31_89.png\n",
            "DSC_5856_31_91.png\n",
            "DSC_5856_31_92.png\n",
            "DSC_5856_31_95.png\n",
            "DSC_5856_31_101.png\n",
            "DSC_5856_31_103.png\n",
            "DSC_5856_31_111.png\n",
            "DSC_5856_31_112.png\n",
            "DSC_5856_31_113.png\n",
            "DSC_5856_31_119.png\n",
            "DSC_5856_31_121.png\n",
            "DSC_5856_31_123.png\n",
            "DSC_5856_31_126.png\n",
            "DSC_5856_31_131.png\n",
            "DSC_5856_31_135.png\n",
            "DSC_5856_31_140.png\n",
            "DSC_5856_31_141.png\n",
            "DSC_5856_31_142.png\n",
            "DSC_5856_31_148.png\n",
            "DSC_5856_31_155.png\n",
            "DSC_5856_31_156.png\n",
            "DSC_5856_31_158.png\n",
            "DSC_5856_31_161.png\n",
            "DSC_5856_31_166.png\n",
            "DSC_5856_31_169.png\n",
            "DSC_5856_31_175.png\n",
            "DSC_5856_31_176.png\n",
            "DSC_5856_31_180.png\n",
            "DSC_5856_31_183.png\n",
            "DSC_5856_31_187.png\n",
            "DSC_5856_31_196.png\n",
            "DSC_5856_31_197.png\n",
            "DSC_5856_31_198.png\n",
            "DSC_5856_31_199.png\n",
            "DSC_5856_31_204.png\n",
            "DSC_5856_31_207.png\n",
            "DSC_5856_31_210.png\n",
            "DSC_5856_31_218.png\n",
            "DSC_5856_31_219.png\n",
            "DSC_5856_31_225.png\n",
            "DSC_5856_31_228.png\n",
            "DSC_5856_31_230.png\n",
            "DSC_5856_31_235.png\n",
            "DSC_5856_31_239.png\n",
            "DSC_5856_31_240.png\n",
            "DSC_5856_31_251.png\n",
            "DSC_5856_31_253.png\n",
            "DSC_5856_31_263.png\n",
            "DSC_5856_31_269.png\n",
            "DSC_5856_31_275.png\n",
            "DSC_5856_31_277.png\n",
            "DSC_5856_31_288.png\n",
            "DSC_5856_31_292.png\n",
            "DSC_5856_31_296.png\n",
            "DSC_5856_31_300.png\n",
            "DSC_5856_31_301.png\n",
            "DSC_5856_31_304.png\n",
            "DSC_5856_31_312.png\n",
            "DSC_5856_31_314.png\n",
            "DSC_5856_31_317.png\n",
            "DSC_5856_31_323.png\n",
            "DSC_5856_31_327.png\n",
            "DSC_5856_31_329.png\n",
            "DSC_5856_31_331.png\n",
            "DSC_5856_31_333.png\n",
            "DSC_5856_31_335.png\n",
            "DSC_5856_31_339.png\n",
            "DSC_5856_31_348.png\n",
            "DSC_5856_31_350.png\n",
            "DSC_5856_31_351.png\n",
            "DSC_5856_31_352.png\n",
            "DSC_5856_31_355.png\n",
            "DSC_5856_31_366.png\n",
            "DSC_5856_31_369.png\n",
            "DSC_5856_31_372.png\n",
            "DSC_5856_31_373.png\n",
            "DSC_5856_31_375.png\n",
            "DSC_5856_31_383.png\n",
            "DSC_5856_31_385.png\n",
            "DSC_5856_31_387.png\n",
            "DSC_5856_31_388.png\n",
            "DSC_5856_31_389.png\n",
            "DSC_5856_31_390.png\n",
            "DSC_5856_31_393.png\n",
            "DSC_5856_31_397.png\n",
            "DSC_5856_31_402.png\n",
            "DSC_5856_31_404.png\n",
            "DSC_5856_31_406.png\n",
            "DSC_5856_31_407.png\n",
            "DSC_5856_31_413.png\n",
            "DSC_5856_31_416.png\n",
            "DSC_5856_31_418.png\n",
            "DSC_5856_31_419.png\n",
            "DSC_5856_31_422.png\n",
            "DSC_5856_31_423.png\n",
            "DSC_5856_31_424.png\n",
            "DSC_5856_31_431.png\n",
            "DSC_5856_31_432.png\n",
            "DSC_5856_31_436.png\n",
            "DSC_5856_31_437.png\n",
            "DSC_5856_31_441.png\n",
            "DSC_5856_31_442.png\n",
            "DSC_5856_31_444.png\n",
            "DSC_5856_31_445.png\n",
            "DSC_5856_31_453.png\n",
            "DSC_5856_31_454.png\n",
            "DSC_5856_31_456.png\n",
            "DSC_5856_31_459.png\n",
            "DSC_5856_31_464.png\n",
            "DSC_5856_31_474.png\n",
            "DSC_5856_31_476.png\n",
            "DSC_5856_31_484.png\n",
            "DSC_5856_31_488.png\n",
            "DSC_5856_31_489.png\n",
            "DSC_5856_31_490.png\n",
            "DSC_5856_31_493.png\n",
            "DSC_5856_31_496.png\n",
            "DSC_5856_31_498.png\n",
            "DSC_5856_52_3.png\n",
            "DSC_5856_52_8.png\n",
            "DSC_5856_52_9.png\n",
            "DSC_5856_52_12.png\n",
            "DSC_5856_52_17.png\n",
            "DSC_5856_52_23.png\n",
            "DSC_5856_52_30.png\n",
            "DSC_5856_52_32.png\n",
            "DSC_5856_52_39.png\n",
            "DSC_5856_52_41.png\n",
            "DSC_5856_52_47.png\n",
            "DSC_5856_52_50.png\n",
            "DSC_5856_52_55.png\n",
            "DSC_5856_52_57.png\n",
            "DSC_5856_52_62.png\n",
            "DSC_5856_52_67.png\n",
            "DSC_5856_52_68.png\n",
            "DSC_5856_52_70.png\n",
            "DSC_5856_52_89.png\n",
            "DSC_5856_52_91.png\n",
            "DSC_5856_52_92.png\n",
            "DSC_5856_52_100.png\n",
            "DSC_5856_52_102.png\n",
            "DSC_5856_52_106.png\n",
            "DSC_5856_52_108.png\n",
            "DSC_5856_52_128.png\n",
            "DSC_5856_52_130.png\n",
            "DSC_5856_52_132.png\n",
            "DSC_5856_52_135.png\n",
            "DSC_5856_52_151.png\n",
            "DSC_5856_52_158.png\n",
            "DSC_5856_52_160.png\n",
            "DSC_5856_52_162.png\n",
            "DSC_5856_52_165.png\n",
            "DSC_5856_52_171.png\n",
            "DSC_5856_52_181.png\n",
            "DSC_5856_52_191.png\n",
            "DSC_5856_52_196.png\n",
            "DSC_5856_52_197.png\n",
            "DSC_5856_52_202.png\n",
            "DSC_5856_52_206.png\n",
            "DSC_5856_52_209.png\n",
            "DSC_5856_52_210.png\n",
            "DSC_5856_52_211.png\n",
            "DSC_5856_52_220.png\n",
            "DSC_5856_52_223.png\n",
            "DSC_5856_52_225.png\n",
            "DSC_5856_52_230.png\n",
            "DSC_5856_52_232.png\n",
            "DSC_5856_52_236.png\n",
            "DSC_5856_52_241.png\n",
            "DSC_5856_52_244.png\n",
            "DSC_5856_52_246.png\n",
            "DSC_5856_52_252.png\n",
            "DSC_5856_52_264.png\n",
            "DSC_5856_52_267.png\n",
            "DSC_5856_52_268.png\n",
            "DSC_5856_52_271.png\n",
            "DSC_5856_52_273.png\n",
            "DSC_5856_52_283.png\n",
            "DSC_5856_52_285.png\n",
            "DSC_5856_52_287.png\n",
            "DSC_5856_52_290.png\n",
            "DSC_5856_52_292.png\n",
            "DSC_5856_52_293.png\n",
            "DSC_5856_52_295.png\n",
            "DSC_5856_52_298.png\n",
            "DSC_5856_52_302.png\n",
            "DSC_5856_52_310.png\n",
            "DSC_5856_52_316.png\n",
            "DSC_5856_52_324.png\n",
            "DSC_5856_52_326.png\n",
            "DSC_5856_52_328.png\n",
            "DSC_5856_52_334.png\n",
            "DSC_5856_52_337.png\n",
            "DSC_5856_52_342.png\n",
            "DSC_5856_52_344.png\n",
            "DSC_5856_52_348.png\n",
            "DSC_5856_52_356.png\n",
            "DSC_5856_52_360.png\n",
            "DSC_5856_52_362.png\n",
            "DSC_5856_52_367.png\n",
            "DSC_5856_52_369.png\n",
            "DSC_5856_52_371.png\n",
            "DSC_5856_52_372.png\n",
            "DSC_5856_52_373.png\n",
            "DSC_5856_52_379.png\n",
            "DSC_5856_52_385.png\n",
            "DSC_5856_52_394.png\n",
            "DSC_5856_52_397.png\n",
            "DSC_5856_52_402.png\n",
            "DSC_5856_52_404.png\n",
            "DSC_5856_52_415.png\n",
            "DSC_5856_52_422.png\n",
            "DSC_5856_52_430.png\n",
            "DSC_5856_52_432.png\n",
            "DSC_5856_52_446.png\n",
            "DSC_5856_52_451.png\n",
            "DSC_5856_52_452.png\n",
            "DSC_5856_52_457.png\n",
            "DSC_5856_52_460.png\n",
            "DSC_5856_52_462.png\n",
            "DSC_5856_52_463.png\n",
            "DSC_5856_52_465.png\n",
            "DSC_5856_52_472.png\n",
            "DSC_5856_52_473.png\n",
            "DSC_5856_52_475.png\n",
            "DSC_5856_52_479.png\n",
            "DSC_5856_52_482.png\n",
            "DSC_5856_52_483.png\n",
            "DSC_5856_1_404.png\n",
            "DSC_5856_1_413.png\n",
            "DSC_5856_1_831.png\n",
            "DSC_5856_14_37.png\n",
            "DSC_5856_14_71.png\n",
            "DSC_5856_14_133.png\n",
            "DSC_5856_14_322.png\n",
            "DSC_5856_14_588.png\n",
            "DSC_5856_12_517.png\n",
            "DSC_5856_12_534.png\n",
            "DSC_5856_12_583.png\n",
            "DSC_5856_12_589.png\n",
            "DSC_5856_12_592.png\n",
            "DSC_5856_12_602.png\n",
            "DSC_5856_12_604.png\n",
            "DSC_5856_12_610.png\n",
            "DSC_5856_12_614.png\n",
            "DSC_5856_12_628.png\n",
            "DSC_5856_12_633.png\n",
            "DSC_5856_12_634.png\n",
            "DSC_5856_12_636.png\n",
            "DSC_5856_12_648.png\n",
            "DSC_5856_12_656.png\n",
            "DSC_5856_12_677.png\n",
            "DSC_5856_12_709.png\n",
            "DSC_5856_12_733.png\n",
            "DSC_5856_12_737.png\n",
            "DSC_5856_12_742.png\n",
            "DSC_5856_12_760.png\n",
            "DSC_5856_12_763.png\n",
            "DSC_5856_12_764.png\n",
            "DSC_5856_12_765.png\n",
            "DSC_5856_12_837.png\n",
            "DSC_5856_12_840.png\n",
            "DSC_5856_12_842.png\n",
            "DSC_5856_12_883.png\n",
            "DSC_5856_12_909.png\n",
            "DSC_5856_12_945.png\n",
            "DSC_5856_12_963.png\n",
            "DSC_5856_12_964.png\n",
            "DSC_5856_12_975.png\n",
            "DSC_5856_12_991.png\n",
            "DSC_5856_12_1010.png\n",
            "DSC_5856_12_1046.png\n",
            "DSC_5856_12_1063.png\n",
            "DSC_5856_12_1090.png\n",
            "DSC_5856_12_1108.png\n",
            "DSC_5856_12_1115.png\n",
            "DSC_5856_12_1133.png\n",
            "DSC_5856_12_1137.png\n",
            "DSC_5856_12_1146.png\n",
            "DSC_5856_12_1161.png\n",
            "DSC_5856_12_1184.png\n",
            "DSC_5856_12_1185.png\n",
            "DSC_5856_12_1189.png\n",
            "DSC_5856_12_1215.png\n",
            "DSC_5856_12_1242.png\n",
            "DSC_5856_12_1262.png\n",
            "DSC_5856_12_1264.png\n",
            "DSC_5856_12_1274.png\n",
            "DSC_5856_12_1280.png\n",
            "DSC_5856_12_1282.png\n",
            "DSC_5856_12_1377.png\n",
            "DSC_5856_12_1400.png\n",
            "DSC_5856_12_1406.png\n",
            "DSC_5856_12_1419.png\n",
            "DSC_5856_13_531.png\n",
            "DSC_5856_13_599.png\n",
            "DSC_5856_13_607.png\n",
            "DSC_5856_13_660.png\n",
            "DSC_5856_13_671.png\n",
            "DSC_5856_13_726.png\n",
            "DSC_5856_13_769.png\n",
            "DSC_5856_13_795.png\n",
            "DSC_5856_13_798.png\n",
            "DSC_5856_13_799.png\n",
            "DSC_5856_13_826.png\n",
            "DSC_5856_13_837.png\n",
            "DSC_5856_13_863.png\n",
            "DSC_5856_13_865.png\n",
            "DSC_5856_13_874.png\n",
            "DSC_5856_13_876.png\n",
            "DSC_5856_13_890.png\n",
            "DSC_5856_13_905.png\n",
            "DSC_5856_13_936.png\n",
            "DSC_5856_13_962.png\n",
            "DSC_5856_13_990.png\n",
            "DSC_5856_13_992.png\n",
            "DSC_5856_13_1011.png\n",
            "DSC_5856_13_1015.png\n",
            "DSC_5856_13_1028.png\n",
            "DSC_5856_13_1042.png\n",
            "DSC_5856_13_1050.png\n",
            "DSC_5856_13_1052.png\n",
            "DSC_5856_13_1103.png\n",
            "DSC_5856_13_1109.png\n",
            "DSC_5856_13_1116.png\n",
            "DSC_5856_13_1142.png\n",
            "DSC_5856_13_1148.png\n",
            "DSC_5856_13_1187.png\n",
            "DSC_5856_13_1193.png\n",
            "DSC_5856_13_1238.png\n",
            "DSC_5856_13_1241.png\n",
            "DSC_5856_13_1261.png\n",
            "DSC_5856_13_1274.png\n",
            "DSC_5856_13_1296.png\n",
            "DSC_5856_13_1298.png\n",
            "DSC_5856_13_1315.png\n",
            "DSC_5856_13_1318.png\n",
            "DSC_5856_13_1342.png\n",
            "DSC_5856_13_1343.png\n",
            "DSC_5856_13_1353.png\n",
            "DSC_5856_13_1380.png\n",
            "DSC_5856_13_1417.png\n",
            "DSC_5856_13_1419.png\n",
            "DSC_5856_13_1420.png\n",
            "DSC_5856_13_1449.png\n",
            "DSC_5856_13_1471.png\n",
            "DSC_5856_21_503.png\n",
            "DSC_5856_21_507.png\n",
            "DSC_5856_21_514.png\n",
            "DSC_5856_21_537.png\n",
            "DSC_5856_21_539.png\n",
            "DSC_5856_21_541.png\n",
            "DSC_5856_21_542.png\n",
            "DSC_5856_21_562.png\n",
            "DSC_5856_21_567.png\n",
            "DSC_5856_21_570.png\n",
            "DSC_5856_21_574.png\n",
            "DSC_5856_21_576.png\n",
            "DSC_5856_21_587.png\n",
            "DSC_5856_21_588.png\n",
            "DSC_5856_21_596.png\n",
            "DSC_5856_21_601.png\n",
            "DSC_5856_21_612.png\n",
            "DSC_5856_21_616.png\n",
            "DSC_5856_21_624.png\n",
            "DSC_5856_21_635.png\n",
            "DSC_5856_21_655.png\n",
            "DSC_5856_21_658.png\n",
            "DSC_5856_21_680.png\n",
            "DSC_5856_21_691.png\n",
            "DSC_5856_21_699.png\n",
            "DSC_5856_21_704.png\n",
            "DSC_5856_21_705.png\n",
            "DSC_5856_21_708.png\n",
            "DSC_5856_21_718.png\n",
            "DSC_5856_21_722.png\n",
            "DSC_5856_21_725.png\n",
            "DSC_5856_21_728.png\n",
            "DSC_5856_21_736.png\n",
            "DSC_5856_21_743.png\n",
            "DSC_5856_21_746.png\n",
            "DSC_5856_21_761.png\n",
            "DSC_5856_21_769.png\n",
            "DSC_5856_21_788.png\n",
            "DSC_5856_21_790.png\n",
            "DSC_5856_21_803.png\n",
            "DSC_5856_21_806.png\n",
            "DSC_5856_21_809.png\n",
            "DSC_5856_21_815.png\n",
            "DSC_5856_21_818.png\n",
            "DSC_5856_21_819.png\n",
            "DSC_5856_21_820.png\n",
            "DSC_5856_21_829.png\n",
            "DSC_5856_21_830.png\n",
            "DSC_5856_21_834.png\n",
            "DSC_5856_21_844.png\n",
            "DSC_5856_21_855.png\n",
            "DSC_5856_21_861.png\n",
            "DSC_5856_21_862.png\n",
            "DSC_5856_21_864.png\n",
            "DSC_5856_21_871.png\n",
            "DSC_5856_21_880.png\n",
            "DSC_5856_21_886.png\n",
            "DSC_5856_21_897.png\n",
            "DSC_5856_21_899.png\n",
            "DSC_5856_21_903.png\n",
            "DSC_5856_21_905.png\n",
            "DSC_5856_21_915.png\n",
            "DSC_5856_21_926.png\n",
            "DSC_5856_21_938.png\n",
            "DSC_5856_21_944.png\n",
            "DSC_5856_21_982.png\n",
            "DSC_5856_21_988.png\n",
            "DSC_5856_21_989.png\n",
            "DSC_5856_21_997.png\n",
            "DSC_5856_21_1002.png\n",
            "DSC_5856_21_1004.png\n",
            "DSC_5856_21_1008.png\n",
            "DSC_5856_21_1014.png\n",
            "DSC_5856_21_1015.png\n",
            "DSC_5856_21_1022.png\n",
            "DSC_5856_21_1024.png\n",
            "DSC_5856_21_1038.png\n",
            "DSC_5856_21_1039.png\n",
            "DSC_5856_21_1041.png\n",
            "DSC_5856_21_1043.png\n",
            "DSC_5856_21_1047.png\n",
            "DSC_5856_21_1058.png\n",
            "DSC_5856_21_1065.png\n",
            "DSC_5856_21_1068.png\n",
            "DSC_5856_21_1071.png\n",
            "DSC_5856_21_1078.png\n",
            "DSC_5856_21_1083.png\n",
            "DSC_5856_21_1093.png\n",
            "DSC_5856_21_1094.png\n",
            "DSC_5856_21_1100.png\n",
            "DSC_5856_21_1107.png\n",
            "DSC_5856_21_1122.png\n",
            "DSC_5856_21_1125.png\n",
            "DSC_5856_21_1134.png\n",
            "DSC_5856_21_1144.png\n",
            "DSC_5856_21_1163.png\n",
            "DSC_5856_21_1164.png\n",
            "DSC_5856_21_1166.png\n",
            "DSC_5856_21_1170.png\n",
            "DSC_5856_21_1172.png\n",
            "DSC_5856_21_1200.png\n",
            "DSC_5856_21_1202.png\n",
            "DSC_5856_21_1203.png\n",
            "DSC_5856_21_1204.png\n",
            "DSC_5856_21_1209.png\n",
            "DSC_5856_21_1210.png\n",
            "DSC_5856_21_1211.png\n",
            "DSC_5856_21_1218.png\n",
            "DSC_5856_21_1227.png\n",
            "DSC_5856_21_1231.png\n",
            "DSC_5856_21_1237.png\n",
            "DSC_5856_21_1246.png\n",
            "DSC_5856_21_1253.png\n",
            "DSC_5856_21_1257.png\n",
            "DSC_5856_21_1258.png\n",
            "DSC_5856_21_1263.png\n",
            "DSC_5856_21_1270.png\n",
            "DSC_5856_21_1273.png\n",
            "DSC_5856_21_1283.png\n",
            "DSC_5856_21_1288.png\n",
            "DSC_5856_21_1292.png\n",
            "DSC_5856_21_1295.png\n",
            "DSC_5856_21_1301.png\n",
            "DSC_5856_21_1302.png\n",
            "DSC_5856_21_1308.png\n",
            "DSC_5856_21_1315.png\n",
            "DSC_5856_21_1316.png\n",
            "DSC_5856_21_1317.png\n",
            "DSC_5856_21_1322.png\n",
            "DSC_5856_21_1324.png\n",
            "DSC_5856_21_1332.png\n",
            "DSC_5856_21_1336.png\n",
            "DSC_5856_21_1341.png\n",
            "DSC_5856_21_1345.png\n",
            "DSC_5856_21_1354.png\n",
            "DSC_5856_21_1357.png\n",
            "DSC_5856_21_1365.png\n",
            "DSC_5856_21_1374.png\n",
            "DSC_5856_21_1376.png\n",
            "DSC_5856_21_1386.png\n",
            "DSC_5856_21_1389.png\n",
            "DSC_5856_21_1390.png\n",
            "DSC_5856_21_1394.png\n",
            "DSC_5856_21_1411.png\n",
            "DSC_5856_21_1439.png\n",
            "DSC_5856_21_1447.png\n",
            "DSC_5856_21_1451.png\n",
            "DSC_5856_21_1458.png\n",
            "DSC_5856_21_1469.png\n",
            "DSC_5856_21_1479.png\n",
            "DSC_5856_21_1489.png\n",
            "DSC_5856_21_1492.png\n",
            "DSC_5856_21_1494.png\n",
            "DSC_5856_21_1495.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5LefXAJvnxs",
        "outputId": "cd00b1fa-5a53-4b56-ac3e-5150a2345200"
      },
      "source": [
        "data1=df[df['5']==2]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']\r\n",
        "a=data1.values\r\n",
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "for ln in data:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/2/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_12.png\n",
            "DSC_5856_12_28.png\n",
            "DSC_5856_12_30.png\n",
            "DSC_5856_12_32.png\n",
            "DSC_5856_12_36.png\n",
            "DSC_5856_12_48.png\n",
            "DSC_5856_12_50.png\n",
            "DSC_5856_12_53.png\n",
            "DSC_5856_12_60.png\n",
            "DSC_5856_12_79.png\n",
            "DSC_5856_12_86.png\n",
            "DSC_5856_12_98.png\n",
            "DSC_5856_12_100.png\n",
            "DSC_5856_12_106.png\n",
            "DSC_5856_12_113.png\n",
            "DSC_5856_12_122.png\n",
            "DSC_5856_12_164.png\n",
            "DSC_5856_12_168.png\n",
            "DSC_5856_12_170.png\n",
            "DSC_5856_12_176.png\n",
            "DSC_5856_12_190.png\n",
            "DSC_5856_12_191.png\n",
            "DSC_5856_12_192.png\n",
            "DSC_5856_12_193.png\n",
            "DSC_5856_12_209.png\n",
            "DSC_5856_12_218.png\n",
            "DSC_5856_12_226.png\n",
            "DSC_5856_12_229.png\n",
            "DSC_5856_12_238.png\n",
            "DSC_5856_12_239.png\n",
            "DSC_5856_12_273.png\n",
            "DSC_5856_12_293.png\n",
            "DSC_5856_12_311.png\n",
            "DSC_5856_12_320.png\n",
            "DSC_5856_12_330.png\n",
            "DSC_5856_12_335.png\n",
            "DSC_5856_12_354.png\n",
            "DSC_5856_12_356.png\n",
            "DSC_5856_12_369.png\n",
            "DSC_5856_12_370.png\n",
            "DSC_5856_12_382.png\n",
            "DSC_5856_12_383.png\n",
            "DSC_5856_12_391.png\n",
            "DSC_5856_12_395.png\n",
            "DSC_5856_12_398.png\n",
            "DSC_5856_12_416.png\n",
            "DSC_5856_12_418.png\n",
            "DSC_5856_12_430.png\n",
            "DSC_5856_12_433.png\n",
            "DSC_5856_12_434.png\n",
            "DSC_5856_12_442.png\n",
            "DSC_5856_12_451.png\n",
            "DSC_5856_12_494.png\n",
            "DSC_5856_13_4.png\n",
            "DSC_5856_13_6.png\n",
            "DSC_5856_13_32.png\n",
            "DSC_5856_13_36.png\n",
            "DSC_5856_13_37.png\n",
            "DSC_5856_13_44.png\n",
            "DSC_5856_13_45.png\n",
            "DSC_5856_13_53.png\n",
            "DSC_5856_13_56.png\n",
            "DSC_5856_13_63.png\n",
            "DSC_5856_13_98.png\n",
            "DSC_5856_13_99.png\n",
            "DSC_5856_13_103.png\n",
            "DSC_5856_13_115.png\n",
            "DSC_5856_13_132.png\n",
            "DSC_5856_13_134.png\n",
            "DSC_5856_13_142.png\n",
            "DSC_5856_13_143.png\n",
            "DSC_5856_13_148.png\n",
            "DSC_5856_13_163.png\n",
            "DSC_5856_13_179.png\n",
            "DSC_5856_13_205.png\n",
            "DSC_5856_13_208.png\n",
            "DSC_5856_13_218.png\n",
            "DSC_5856_13_231.png\n",
            "DSC_5856_13_237.png\n",
            "DSC_5856_13_239.png\n",
            "DSC_5856_13_248.png\n",
            "DSC_5856_13_251.png\n",
            "DSC_5856_13_252.png\n",
            "DSC_5856_13_268.png\n",
            "DSC_5856_13_284.png\n",
            "DSC_5856_13_289.png\n",
            "DSC_5856_13_313.png\n",
            "DSC_5856_13_314.png\n",
            "DSC_5856_13_325.png\n",
            "DSC_5856_13_329.png\n",
            "DSC_5856_13_330.png\n",
            "DSC_5856_13_356.png\n",
            "DSC_5856_13_364.png\n",
            "DSC_5856_13_365.png\n",
            "DSC_5856_13_369.png\n",
            "DSC_5856_13_378.png\n",
            "DSC_5856_13_382.png\n",
            "DSC_5856_13_383.png\n",
            "DSC_5856_13_386.png\n",
            "DSC_5856_13_401.png\n",
            "DSC_5856_13_403.png\n",
            "DSC_5856_13_404.png\n",
            "DSC_5856_13_410.png\n",
            "DSC_5856_13_411.png\n",
            "DSC_5856_13_414.png\n",
            "DSC_5856_13_418.png\n",
            "DSC_5856_13_421.png\n",
            "DSC_5856_13_423.png\n",
            "DSC_5856_13_427.png\n",
            "DSC_5856_13_430.png\n",
            "DSC_5856_13_437.png\n",
            "DSC_5856_13_441.png\n",
            "DSC_5856_13_445.png\n",
            "DSC_5856_13_464.png\n",
            "DSC_5856_13_473.png\n",
            "DSC_5856_13_476.png\n",
            "DSC_5856_13_480.png\n",
            "DSC_5856_21_1.png\n",
            "DSC_5856_21_9.png\n",
            "DSC_5856_21_10.png\n",
            "DSC_5856_21_16.png\n",
            "DSC_5856_21_22.png\n",
            "DSC_5856_21_33.png\n",
            "DSC_5856_21_51.png\n",
            "DSC_5856_21_52.png\n",
            "DSC_5856_21_55.png\n",
            "DSC_5856_21_69.png\n",
            "DSC_5856_21_74.png\n",
            "DSC_5856_21_77.png\n",
            "DSC_5856_21_82.png\n",
            "DSC_5856_21_144.png\n",
            "DSC_5856_21_162.png\n",
            "DSC_5856_21_166.png\n",
            "DSC_5856_21_169.png\n",
            "DSC_5856_21_171.png\n",
            "DSC_5856_21_186.png\n",
            "DSC_5856_21_190.png\n",
            "DSC_5856_21_211.png\n",
            "DSC_5856_21_222.png\n",
            "DSC_5856_21_223.png\n",
            "DSC_5856_21_230.png\n",
            "DSC_5856_21_242.png\n",
            "DSC_5856_21_243.png\n",
            "DSC_5856_21_246.png\n",
            "DSC_5856_21_254.png\n",
            "DSC_5856_21_259.png\n",
            "DSC_5856_21_262.png\n",
            "DSC_5856_21_265.png\n",
            "DSC_5856_21_270.png\n",
            "DSC_5856_21_274.png\n",
            "DSC_5856_21_292.png\n",
            "DSC_5856_21_302.png\n",
            "DSC_5856_21_316.png\n",
            "DSC_5856_21_327.png\n",
            "DSC_5856_21_336.png\n",
            "DSC_5856_21_344.png\n",
            "DSC_5856_21_350.png\n",
            "DSC_5856_21_366.png\n",
            "DSC_5856_21_380.png\n",
            "DSC_5856_21_388.png\n",
            "DSC_5856_21_419.png\n",
            "DSC_5856_21_420.png\n",
            "DSC_5856_21_421.png\n",
            "DSC_5856_21_422.png\n",
            "DSC_5856_21_426.png\n",
            "DSC_5856_21_436.png\n",
            "DSC_5856_21_443.png\n",
            "DSC_5856_21_445.png\n",
            "DSC_5856_21_451.png\n",
            "DSC_5856_21_457.png\n",
            "DSC_5856_21_460.png\n",
            "DSC_5856_21_463.png\n",
            "DSC_5856_21_465.png\n",
            "DSC_5856_21_466.png\n",
            "DSC_5856_21_467.png\n",
            "DSC_5856_21_469.png\n",
            "DSC_5856_21_472.png\n",
            "DSC_5856_21_474.png\n",
            "DSC_5856_21_477.png\n",
            "DSC_5856_21_499.png\n",
            "DSC_5856_25_3.png\n",
            "DSC_5856_25_12.png\n",
            "DSC_5856_25_16.png\n",
            "DSC_5856_25_17.png\n",
            "DSC_5856_25_22.png\n",
            "DSC_5856_25_34.png\n",
            "DSC_5856_25_35.png\n",
            "DSC_5856_25_38.png\n",
            "DSC_5856_25_47.png\n",
            "DSC_5856_25_55.png\n",
            "DSC_5856_25_57.png\n",
            "DSC_5856_25_65.png\n",
            "DSC_5856_25_72.png\n",
            "DSC_5856_25_74.png\n",
            "DSC_5856_25_75.png\n",
            "DSC_5856_25_82.png\n",
            "DSC_5856_25_86.png\n",
            "DSC_5856_25_87.png\n",
            "DSC_5856_25_90.png\n",
            "DSC_5856_25_93.png\n",
            "DSC_5856_25_95.png\n",
            "DSC_5856_25_104.png\n",
            "DSC_5856_25_111.png\n",
            "DSC_5856_25_117.png\n",
            "DSC_5856_25_121.png\n",
            "DSC_5856_25_131.png\n",
            "DSC_5856_25_144.png\n",
            "DSC_5856_25_145.png\n",
            "DSC_5856_25_155.png\n",
            "DSC_5856_25_169.png\n",
            "DSC_5856_25_170.png\n",
            "DSC_5856_25_171.png\n",
            "DSC_5856_25_176.png\n",
            "DSC_5856_25_185.png\n",
            "DSC_5856_25_186.png\n",
            "DSC_5856_25_191.png\n",
            "DSC_5856_25_198.png\n",
            "DSC_5856_25_215.png\n",
            "DSC_5856_25_216.png\n",
            "DSC_5856_25_218.png\n",
            "DSC_5856_25_219.png\n",
            "DSC_5856_25_225.png\n",
            "DSC_5856_25_232.png\n",
            "DSC_5856_25_234.png\n",
            "DSC_5856_25_239.png\n",
            "DSC_5856_25_240.png\n",
            "DSC_5856_25_260.png\n",
            "DSC_5856_25_270.png\n",
            "DSC_5856_25_277.png\n",
            "DSC_5856_25_286.png\n",
            "DSC_5856_25_290.png\n",
            "DSC_5856_25_299.png\n",
            "DSC_5856_25_306.png\n",
            "DSC_5856_25_312.png\n",
            "DSC_5856_25_322.png\n",
            "DSC_5856_25_328.png\n",
            "DSC_5856_25_334.png\n",
            "DSC_5856_25_344.png\n",
            "DSC_5856_25_351.png\n",
            "DSC_5856_25_353.png\n",
            "DSC_5856_25_377.png\n",
            "DSC_5856_25_380.png\n",
            "DSC_5856_25_383.png\n",
            "DSC_5856_25_388.png\n",
            "DSC_5856_25_389.png\n",
            "DSC_5856_25_418.png\n",
            "DSC_5856_25_421.png\n",
            "DSC_5856_25_434.png\n",
            "DSC_5856_25_438.png\n",
            "DSC_5856_25_440.png\n",
            "DSC_5856_25_468.png\n",
            "DSC_5856_25_474.png\n",
            "DSC_5856_25_475.png\n",
            "DSC_5856_25_483.png\n",
            "DSC_5856_25_490.png\n",
            "DSC_5856_11_7.png\n",
            "DSC_5856_11_19.png\n",
            "DSC_5856_11_27.png\n",
            "DSC_5856_11_29.png\n",
            "DSC_5856_11_31.png\n",
            "DSC_5856_11_32.png\n",
            "DSC_5856_11_40.png\n",
            "DSC_5856_11_50.png\n",
            "DSC_5856_11_58.png\n",
            "DSC_5856_11_61.png\n",
            "DSC_5856_11_69.png\n",
            "DSC_5856_11_76.png\n",
            "DSC_5856_11_78.png\n",
            "DSC_5856_11_80.png\n",
            "DSC_5856_11_82.png\n",
            "DSC_5856_11_89.png\n",
            "DSC_5856_11_96.png\n",
            "DSC_5856_11_104.png\n",
            "DSC_5856_11_106.png\n",
            "DSC_5856_11_114.png\n",
            "DSC_5856_11_123.png\n",
            "DSC_5856_11_124.png\n",
            "DSC_5856_11_128.png\n",
            "DSC_5856_11_135.png\n",
            "DSC_5856_11_139.png\n",
            "DSC_5856_11_140.png\n",
            "DSC_5856_11_145.png\n",
            "DSC_5856_11_160.png\n",
            "DSC_5856_11_166.png\n",
            "DSC_5856_11_167.png\n",
            "DSC_5856_11_181.png\n",
            "DSC_5856_11_185.png\n",
            "DSC_5856_11_188.png\n",
            "DSC_5856_11_196.png\n",
            "DSC_5856_11_209.png\n",
            "DSC_5856_11_225.png\n",
            "DSC_5856_11_232.png\n",
            "DSC_5856_11_233.png\n",
            "DSC_5856_11_242.png\n",
            "DSC_5856_11_244.png\n",
            "DSC_5856_11_250.png\n",
            "DSC_5856_11_255.png\n",
            "DSC_5856_11_258.png\n",
            "DSC_5856_11_259.png\n",
            "DSC_5856_11_265.png\n",
            "DSC_5856_11_271.png\n",
            "DSC_5856_11_273.png\n",
            "DSC_5856_11_274.png\n",
            "DSC_5856_11_278.png\n",
            "DSC_5856_11_285.png\n",
            "DSC_5856_11_292.png\n",
            "DSC_5856_11_297.png\n",
            "DSC_5856_11_301.png\n",
            "DSC_5856_11_304.png\n",
            "DSC_5856_11_305.png\n",
            "DSC_5856_11_323.png\n",
            "DSC_5856_11_331.png\n",
            "DSC_5856_11_340.png\n",
            "DSC_5856_11_341.png\n",
            "DSC_5856_11_355.png\n",
            "DSC_5856_11_356.png\n",
            "DSC_5856_11_368.png\n",
            "DSC_5856_11_369.png\n",
            "DSC_5856_11_371.png\n",
            "DSC_5856_11_372.png\n",
            "DSC_5856_11_373.png\n",
            "DSC_5856_11_374.png\n",
            "DSC_5856_11_375.png\n",
            "DSC_5856_11_377.png\n",
            "DSC_5856_11_379.png\n",
            "DSC_5856_11_384.png\n",
            "DSC_5856_11_396.png\n",
            "DSC_5856_11_404.png\n",
            "DSC_5856_11_409.png\n",
            "DSC_5856_11_417.png\n",
            "DSC_5856_11_419.png\n",
            "DSC_5856_11_420.png\n",
            "DSC_5856_11_422.png\n",
            "DSC_5856_11_433.png\n",
            "DSC_5856_11_468.png\n",
            "DSC_5856_11_475.png\n",
            "DSC_5856_11_476.png\n",
            "DSC_5856_11_481.png\n",
            "DSC_5856_11_486.png\n",
            "DSC_5856_11_495.png\n",
            "DSC_5856_11_498.png\n",
            "DSC_5856_2_13.png\n",
            "DSC_5856_2_14.png\n",
            "DSC_5856_2_19.png\n",
            "DSC_5856_2_21.png\n",
            "DSC_5856_2_24.png\n",
            "DSC_5856_2_25.png\n",
            "DSC_5856_2_38.png\n",
            "DSC_5856_2_43.png\n",
            "DSC_5856_2_44.png\n",
            "DSC_5856_2_47.png\n",
            "DSC_5856_2_53.png\n",
            "DSC_5856_2_55.png\n",
            "DSC_5856_2_59.png\n",
            "DSC_5856_2_64.png\n",
            "DSC_5856_2_75.png\n",
            "DSC_5856_2_78.png\n",
            "DSC_5856_2_79.png\n",
            "DSC_5856_2_82.png\n",
            "DSC_5856_2_84.png\n",
            "DSC_5856_2_96.png\n",
            "DSC_5856_2_113.png\n",
            "DSC_5856_2_114.png\n",
            "DSC_5856_2_120.png\n",
            "DSC_5856_2_133.png\n",
            "DSC_5856_2_137.png\n",
            "DSC_5856_2_139.png\n",
            "DSC_5856_2_146.png\n",
            "DSC_5856_2_147.png\n",
            "DSC_5856_2_151.png\n",
            "DSC_5856_2_153.png\n",
            "DSC_5856_2_154.png\n",
            "DSC_5856_2_157.png\n",
            "DSC_5856_2_162.png\n",
            "DSC_5856_2_165.png\n",
            "DSC_5856_2_167.png\n",
            "DSC_5856_2_170.png\n",
            "DSC_5856_2_182.png\n",
            "DSC_5856_2_193.png\n",
            "DSC_5856_2_196.png\n",
            "DSC_5856_2_208.png\n",
            "DSC_5856_2_211.png\n",
            "DSC_5856_2_212.png\n",
            "DSC_5856_2_215.png\n",
            "DSC_5856_2_217.png\n",
            "DSC_5856_2_219.png\n",
            "DSC_5856_2_222.png\n",
            "DSC_5856_2_231.png\n",
            "DSC_5856_2_236.png\n",
            "DSC_5856_2_238.png\n",
            "DSC_5856_2_240.png\n",
            "DSC_5856_2_242.png\n",
            "DSC_5856_2_247.png\n",
            "DSC_5856_2_254.png\n",
            "DSC_5856_2_259.png\n",
            "DSC_5856_2_262.png\n",
            "DSC_5856_2_265.png\n",
            "DSC_5856_2_270.png\n",
            "DSC_5856_2_272.png\n",
            "DSC_5856_2_274.png\n",
            "DSC_5856_2_276.png\n",
            "DSC_5856_2_278.png\n",
            "DSC_5856_2_294.png\n",
            "DSC_5856_2_298.png\n",
            "DSC_5856_2_301.png\n",
            "DSC_5856_2_304.png\n",
            "DSC_5856_2_307.png\n",
            "DSC_5856_2_315.png\n",
            "DSC_5856_2_319.png\n",
            "DSC_5856_2_322.png\n",
            "DSC_5856_2_323.png\n",
            "DSC_5856_2_331.png\n",
            "DSC_5856_2_332.png\n",
            "DSC_5856_2_337.png\n",
            "DSC_5856_2_342.png\n",
            "DSC_5856_2_349.png\n",
            "DSC_5856_2_354.png\n",
            "DSC_5856_2_365.png\n",
            "DSC_5856_2_373.png\n",
            "DSC_5856_2_378.png\n",
            "DSC_5856_2_379.png\n",
            "DSC_5856_2_387.png\n",
            "DSC_5856_2_392.png\n",
            "DSC_5856_2_399.png\n",
            "DSC_5856_2_409.png\n",
            "DSC_5856_2_411.png\n",
            "DSC_5856_2_414.png\n",
            "DSC_5856_2_415.png\n",
            "DSC_5856_2_419.png\n",
            "DSC_5856_2_420.png\n",
            "DSC_5856_2_435.png\n",
            "DSC_5856_2_438.png\n",
            "DSC_5856_2_440.png\n",
            "DSC_5856_2_443.png\n",
            "DSC_5856_2_447.png\n",
            "DSC_5856_2_452.png\n",
            "DSC_5856_2_453.png\n",
            "DSC_5856_2_454.png\n",
            "DSC_5856_2_459.png\n",
            "DSC_5856_2_469.png\n",
            "DSC_5856_2_480.png\n",
            "DSC_5856_2_483.png\n",
            "DSC_5856_2_484.png\n",
            "DSC_5856_2_488.png\n",
            "DSC_5856_2_489.png\n",
            "DSC_5856_2_492.png\n",
            "DSC_5856_2_499.png\n",
            "DSC_5856_31_15.png\n",
            "DSC_5856_31_21.png\n",
            "DSC_5856_31_29.png\n",
            "DSC_5856_31_38.png\n",
            "DSC_5856_31_42.png\n",
            "DSC_5856_31_44.png\n",
            "DSC_5856_31_57.png\n",
            "DSC_5856_31_84.png\n",
            "DSC_5856_31_93.png\n",
            "DSC_5856_31_100.png\n",
            "DSC_5856_31_102.png\n",
            "DSC_5856_31_105.png\n",
            "DSC_5856_31_128.png\n",
            "DSC_5856_31_129.png\n",
            "DSC_5856_31_133.png\n",
            "DSC_5856_31_137.png\n",
            "DSC_5856_31_139.png\n",
            "DSC_5856_31_151.png\n",
            "DSC_5856_31_159.png\n",
            "DSC_5856_31_170.png\n",
            "DSC_5856_31_179.png\n",
            "DSC_5856_31_182.png\n",
            "DSC_5856_31_186.png\n",
            "DSC_5856_31_191.png\n",
            "DSC_5856_31_195.png\n",
            "DSC_5856_31_203.png\n",
            "DSC_5856_31_214.png\n",
            "DSC_5856_31_223.png\n",
            "DSC_5856_31_234.png\n",
            "DSC_5856_31_237.png\n",
            "DSC_5856_31_238.png\n",
            "DSC_5856_31_245.png\n",
            "DSC_5856_31_247.png\n",
            "DSC_5856_31_260.png\n",
            "DSC_5856_31_266.png\n",
            "DSC_5856_31_271.png\n",
            "DSC_5856_31_273.png\n",
            "DSC_5856_31_274.png\n",
            "DSC_5856_31_284.png\n",
            "DSC_5856_31_295.png\n",
            "DSC_5856_31_297.png\n",
            "DSC_5856_31_309.png\n",
            "DSC_5856_31_316.png\n",
            "DSC_5856_31_319.png\n",
            "DSC_5856_31_322.png\n",
            "DSC_5856_31_325.png\n",
            "DSC_5856_31_364.png\n",
            "DSC_5856_31_367.png\n",
            "DSC_5856_31_391.png\n",
            "DSC_5856_31_412.png\n",
            "DSC_5856_31_428.png\n",
            "DSC_5856_31_461.png\n",
            "DSC_5856_31_463.png\n",
            "DSC_5856_31_491.png\n",
            "DSC_5856_31_492.png\n",
            "DSC_5856_31_494.png\n",
            "DSC_5856_52_1.png\n",
            "DSC_5856_52_2.png\n",
            "DSC_5856_52_20.png\n",
            "DSC_5856_52_40.png\n",
            "DSC_5856_52_48.png\n",
            "DSC_5856_52_53.png\n",
            "DSC_5856_52_58.png\n",
            "DSC_5856_52_61.png\n",
            "DSC_5856_52_65.png\n",
            "DSC_5856_52_74.png\n",
            "DSC_5856_52_93.png\n",
            "DSC_5856_52_95.png\n",
            "DSC_5856_52_97.png\n",
            "DSC_5856_52_101.png\n",
            "DSC_5856_52_107.png\n",
            "DSC_5856_52_111.png\n",
            "DSC_5856_52_122.png\n",
            "DSC_5856_52_125.png\n",
            "DSC_5856_52_126.png\n",
            "DSC_5856_52_129.png\n",
            "DSC_5856_52_134.png\n",
            "DSC_5856_52_138.png\n",
            "DSC_5856_52_144.png\n",
            "DSC_5856_52_145.png\n",
            "DSC_5856_52_148.png\n",
            "DSC_5856_52_149.png\n",
            "DSC_5856_52_150.png\n",
            "DSC_5856_52_169.png\n",
            "DSC_5856_52_179.png\n",
            "DSC_5856_52_183.png\n",
            "DSC_5856_52_186.png\n",
            "DSC_5856_52_203.png\n",
            "DSC_5856_52_240.png\n",
            "DSC_5856_52_243.png\n",
            "DSC_5856_52_250.png\n",
            "DSC_5856_52_259.png\n",
            "DSC_5856_52_260.png\n",
            "DSC_5856_52_262.png\n",
            "DSC_5856_52_263.png\n",
            "DSC_5856_52_265.png\n",
            "DSC_5856_52_266.png\n",
            "DSC_5856_52_282.png\n",
            "DSC_5856_52_288.png\n",
            "DSC_5856_52_296.png\n",
            "DSC_5856_52_299.png\n",
            "DSC_5856_52_315.png\n",
            "DSC_5856_52_318.png\n",
            "DSC_5856_52_319.png\n",
            "DSC_5856_52_327.png\n",
            "DSC_5856_52_333.png\n",
            "DSC_5856_52_339.png\n",
            "DSC_5856_52_347.png\n",
            "DSC_5856_52_352.png\n",
            "DSC_5856_52_359.png\n",
            "DSC_5856_52_375.png\n",
            "DSC_5856_52_380.png\n",
            "DSC_5856_52_387.png\n",
            "DSC_5856_52_396.png\n",
            "DSC_5856_52_400.png\n",
            "DSC_5856_52_411.png\n",
            "DSC_5856_52_412.png\n",
            "DSC_5856_52_420.png\n",
            "DSC_5856_52_424.png\n",
            "DSC_5856_52_426.png\n",
            "DSC_5856_52_428.png\n",
            "DSC_5856_52_439.png\n",
            "DSC_5856_52_443.png\n",
            "DSC_5856_52_448.png\n",
            "DSC_5856_52_454.png\n",
            "DSC_5856_52_458.png\n",
            "DSC_5856_52_471.png\n",
            "DSC_5856_52_478.png\n",
            "DSC_5856_52_480.png\n",
            "DSC_5856_52_484.png\n",
            "DSC_5856_52_494.png\n",
            "DSC_5856_52_498.png\n",
            "DSC_5856_1_8.png\n",
            "DSC_5856_1_25.png\n",
            "DSC_5856_1_29.png\n",
            "DSC_5856_1_47.png\n",
            "DSC_5856_1_53.png\n",
            "DSC_5856_1_59.png\n",
            "DSC_5856_1_75.png\n",
            "DSC_5856_1_94.png\n",
            "DSC_5856_1_99.png\n",
            "DSC_5856_1_100.png\n",
            "DSC_5856_1_120.png\n",
            "DSC_5856_1_129.png\n",
            "DSC_5856_1_134.png\n",
            "DSC_5856_1_179.png\n",
            "DSC_5856_1_204.png\n",
            "DSC_5856_1_310.png\n",
            "DSC_5856_1_368.png\n",
            "DSC_5856_1_371.png\n",
            "DSC_5856_1_384.png\n",
            "DSC_5856_1_400.png\n",
            "DSC_5856_1_410.png\n",
            "DSC_5856_1_437.png\n",
            "DSC_5856_1_510.png\n",
            "DSC_5856_1_521.png\n",
            "DSC_5856_1_524.png\n",
            "DSC_5856_1_530.png\n",
            "DSC_5856_1_533.png\n",
            "DSC_5856_1_546.png\n",
            "DSC_5856_1_556.png\n",
            "DSC_5856_1_575.png\n",
            "DSC_5856_1_588.png\n",
            "DSC_5856_1_591.png\n",
            "DSC_5856_1_601.png\n",
            "DSC_5856_1_644.png\n",
            "DSC_5856_1_672.png\n",
            "DSC_5856_1_676.png\n",
            "DSC_5856_1_748.png\n",
            "DSC_5856_1_754.png\n",
            "DSC_5856_1_794.png\n",
            "DSC_5856_1_799.png\n",
            "DSC_5856_1_826.png\n",
            "DSC_5856_1_828.png\n",
            "DSC_5856_1_853.png\n",
            "DSC_5856_1_871.png\n",
            "DSC_5856_1_875.png\n",
            "DSC_5856_1_877.png\n",
            "DSC_5856_1_898.png\n",
            "DSC_5856_1_900.png\n",
            "DSC_5856_1_921.png\n",
            "DSC_5856_1_923.png\n",
            "DSC_5856_1_927.png\n",
            "DSC_5856_1_951.png\n",
            "DSC_5856_1_957.png\n",
            "DSC_5856_1_973.png\n",
            "DSC_5856_1_976.png\n",
            "DSC_5856_14_0.png\n",
            "DSC_5856_14_27.png\n",
            "DSC_5856_14_28.png\n",
            "DSC_5856_14_33.png\n",
            "DSC_5856_14_36.png\n",
            "DSC_5856_14_38.png\n",
            "DSC_5856_14_52.png\n",
            "DSC_5856_14_56.png\n",
            "DSC_5856_14_57.png\n",
            "DSC_5856_14_61.png\n",
            "DSC_5856_14_79.png\n",
            "DSC_5856_14_82.png\n",
            "DSC_5856_14_94.png\n",
            "DSC_5856_14_98.png\n",
            "DSC_5856_14_109.png\n",
            "DSC_5856_14_118.png\n",
            "DSC_5856_14_120.png\n",
            "DSC_5856_14_123.png\n",
            "DSC_5856_14_126.png\n",
            "DSC_5856_14_151.png\n",
            "DSC_5856_14_156.png\n",
            "DSC_5856_14_158.png\n",
            "DSC_5856_14_159.png\n",
            "DSC_5856_14_168.png\n",
            "DSC_5856_14_175.png\n",
            "DSC_5856_14_200.png\n",
            "DSC_5856_14_210.png\n",
            "DSC_5856_14_237.png\n",
            "DSC_5856_14_239.png\n",
            "DSC_5856_14_251.png\n",
            "DSC_5856_14_254.png\n",
            "DSC_5856_14_260.png\n",
            "DSC_5856_14_263.png\n",
            "DSC_5856_14_289.png\n",
            "DSC_5856_14_292.png\n",
            "DSC_5856_14_294.png\n",
            "DSC_5856_14_295.png\n",
            "DSC_5856_14_303.png\n",
            "DSC_5856_14_315.png\n",
            "DSC_5856_14_320.png\n",
            "DSC_5856_14_333.png\n",
            "DSC_5856_14_337.png\n",
            "DSC_5856_14_339.png\n",
            "DSC_5856_14_346.png\n",
            "DSC_5856_14_348.png\n",
            "DSC_5856_14_354.png\n",
            "DSC_5856_14_355.png\n",
            "DSC_5856_14_357.png\n",
            "DSC_5856_14_372.png\n",
            "DSC_5856_14_374.png\n",
            "DSC_5856_14_379.png\n",
            "DSC_5856_14_384.png\n",
            "DSC_5856_14_388.png\n",
            "DSC_5856_14_391.png\n",
            "DSC_5856_14_398.png\n",
            "DSC_5856_14_422.png\n",
            "DSC_5856_14_423.png\n",
            "DSC_5856_14_425.png\n",
            "DSC_5856_14_426.png\n",
            "DSC_5856_14_431.png\n",
            "DSC_5856_14_437.png\n",
            "DSC_5856_14_456.png\n",
            "DSC_5856_14_468.png\n",
            "DSC_5856_14_473.png\n",
            "DSC_5856_14_484.png\n",
            "DSC_5856_14_488.png\n",
            "DSC_5856_14_492.png\n",
            "DSC_5856_14_536.png\n",
            "DSC_5856_14_556.png\n",
            "DSC_5856_14_572.png\n",
            "DSC_5856_14_595.png\n",
            "DSC_5856_12_506.png\n",
            "DSC_5856_12_513.png\n",
            "DSC_5856_12_520.png\n",
            "DSC_5856_12_525.png\n",
            "DSC_5856_12_555.png\n",
            "DSC_5856_12_575.png\n",
            "DSC_5856_12_582.png\n",
            "DSC_5856_12_584.png\n",
            "DSC_5856_12_603.png\n",
            "DSC_5856_12_611.png\n",
            "DSC_5856_12_642.png\n",
            "DSC_5856_12_644.png\n",
            "DSC_5856_12_654.png\n",
            "DSC_5856_12_662.png\n",
            "DSC_5856_12_667.png\n",
            "DSC_5856_12_671.png\n",
            "DSC_5856_12_684.png\n",
            "DSC_5856_12_686.png\n",
            "DSC_5856_12_687.png\n",
            "DSC_5856_12_698.png\n",
            "DSC_5856_12_703.png\n",
            "DSC_5856_12_706.png\n",
            "DSC_5856_12_716.png\n",
            "DSC_5856_12_721.png\n",
            "DSC_5856_12_727.png\n",
            "DSC_5856_12_739.png\n",
            "DSC_5856_12_751.png\n",
            "DSC_5856_12_771.png\n",
            "DSC_5856_12_775.png\n",
            "DSC_5856_12_789.png\n",
            "DSC_5856_12_800.png\n",
            "DSC_5856_12_801.png\n",
            "DSC_5856_12_804.png\n",
            "DSC_5856_12_806.png\n",
            "DSC_5856_12_811.png\n",
            "DSC_5856_12_828.png\n",
            "DSC_5856_12_835.png\n",
            "DSC_5856_12_843.png\n",
            "DSC_5856_12_850.png\n",
            "DSC_5856_12_853.png\n",
            "DSC_5856_12_862.png\n",
            "DSC_5856_12_867.png\n",
            "DSC_5856_12_887.png\n",
            "DSC_5856_12_895.png\n",
            "DSC_5856_12_898.png\n",
            "DSC_5856_12_911.png\n",
            "DSC_5856_12_919.png\n",
            "DSC_5856_12_922.png\n",
            "DSC_5856_12_923.png\n",
            "DSC_5856_12_925.png\n",
            "DSC_5856_12_932.png\n",
            "DSC_5856_12_933.png\n",
            "DSC_5856_12_949.png\n",
            "DSC_5856_12_957.png\n",
            "DSC_5856_12_999.png\n",
            "DSC_5856_12_1001.png\n",
            "DSC_5856_12_1005.png\n",
            "DSC_5856_12_1019.png\n",
            "DSC_5856_12_1042.png\n",
            "DSC_5856_12_1045.png\n",
            "DSC_5856_12_1052.png\n",
            "DSC_5856_12_1055.png\n",
            "DSC_5856_12_1081.png\n",
            "DSC_5856_12_1131.png\n",
            "DSC_5856_12_1142.png\n",
            "DSC_5856_12_1143.png\n",
            "DSC_5856_12_1168.png\n",
            "DSC_5856_12_1169.png\n",
            "DSC_5856_12_1175.png\n",
            "DSC_5856_12_1183.png\n",
            "DSC_5856_12_1198.png\n",
            "DSC_5856_12_1201.png\n",
            "DSC_5856_12_1223.png\n",
            "DSC_5856_12_1233.png\n",
            "DSC_5856_12_1305.png\n",
            "DSC_5856_12_1313.png\n",
            "DSC_5856_12_1330.png\n",
            "DSC_5856_12_1346.png\n",
            "DSC_5856_12_1349.png\n",
            "DSC_5856_12_1364.png\n",
            "DSC_5856_12_1367.png\n",
            "DSC_5856_12_1370.png\n",
            "DSC_5856_12_1375.png\n",
            "DSC_5856_12_1376.png\n",
            "DSC_5856_12_1420.png\n",
            "DSC_5856_12_1424.png\n",
            "DSC_5856_12_1425.png\n",
            "DSC_5856_12_1429.png\n",
            "DSC_5856_12_1436.png\n",
            "DSC_5856_12_1445.png\n",
            "DSC_5856_12_1446.png\n",
            "DSC_5856_12_1455.png\n",
            "DSC_5856_12_1466.png\n",
            "DSC_5856_12_1467.png\n",
            "DSC_5856_12_1490.png\n",
            "DSC_5856_12_1491.png\n",
            "DSC_5856_13_520.png\n",
            "DSC_5856_13_524.png\n",
            "DSC_5856_13_525.png\n",
            "DSC_5856_13_527.png\n",
            "DSC_5856_13_528.png\n",
            "DSC_5856_13_530.png\n",
            "DSC_5856_13_533.png\n",
            "DSC_5856_13_543.png\n",
            "DSC_5856_13_548.png\n",
            "DSC_5856_13_559.png\n",
            "DSC_5856_13_563.png\n",
            "DSC_5856_13_590.png\n",
            "DSC_5856_13_600.png\n",
            "DSC_5856_13_604.png\n",
            "DSC_5856_13_641.png\n",
            "DSC_5856_13_647.png\n",
            "DSC_5856_13_663.png\n",
            "DSC_5856_13_668.png\n",
            "DSC_5856_13_679.png\n",
            "DSC_5856_13_684.png\n",
            "DSC_5856_13_691.png\n",
            "DSC_5856_13_692.png\n",
            "DSC_5856_13_696.png\n",
            "DSC_5856_13_701.png\n",
            "DSC_5856_13_706.png\n",
            "DSC_5856_13_712.png\n",
            "DSC_5856_13_714.png\n",
            "DSC_5856_13_719.png\n",
            "DSC_5856_13_750.png\n",
            "DSC_5856_13_754.png\n",
            "DSC_5856_13_756.png\n",
            "DSC_5856_13_782.png\n",
            "DSC_5856_13_785.png\n",
            "DSC_5856_13_789.png\n",
            "DSC_5856_13_791.png\n",
            "DSC_5856_13_792.png\n",
            "DSC_5856_13_801.png\n",
            "DSC_5856_13_819.png\n",
            "DSC_5856_13_820.png\n",
            "DSC_5856_13_821.png\n",
            "DSC_5856_13_831.png\n",
            "DSC_5856_13_871.png\n",
            "DSC_5856_13_898.png\n",
            "DSC_5856_13_901.png\n",
            "DSC_5856_13_910.png\n",
            "DSC_5856_13_912.png\n",
            "DSC_5856_13_923.png\n",
            "DSC_5856_13_927.png\n",
            "DSC_5856_13_931.png\n",
            "DSC_5856_13_941.png\n",
            "DSC_5856_13_948.png\n",
            "DSC_5856_13_950.png\n",
            "DSC_5856_13_957.png\n",
            "DSC_5856_13_970.png\n",
            "DSC_5856_13_973.png\n",
            "DSC_5856_13_976.png\n",
            "DSC_5856_13_983.png\n",
            "DSC_5856_13_999.png\n",
            "DSC_5856_13_1000.png\n",
            "DSC_5856_13_1005.png\n",
            "DSC_5856_13_1023.png\n",
            "DSC_5856_13_1027.png\n",
            "DSC_5856_13_1053.png\n",
            "DSC_5856_13_1105.png\n",
            "DSC_5856_13_1110.png\n",
            "DSC_5856_13_1126.png\n",
            "DSC_5856_13_1129.png\n",
            "DSC_5856_13_1150.png\n",
            "DSC_5856_13_1151.png\n",
            "DSC_5856_13_1156.png\n",
            "DSC_5856_13_1159.png\n",
            "DSC_5856_13_1175.png\n",
            "DSC_5856_13_1182.png\n",
            "DSC_5856_13_1186.png\n",
            "DSC_5856_13_1188.png\n",
            "DSC_5856_13_1192.png\n",
            "DSC_5856_13_1194.png\n",
            "DSC_5856_13_1212.png\n",
            "DSC_5856_13_1234.png\n",
            "DSC_5856_13_1240.png\n",
            "DSC_5856_13_1243.png\n",
            "DSC_5856_13_1303.png\n",
            "DSC_5856_13_1306.png\n",
            "DSC_5856_13_1307.png\n",
            "DSC_5856_13_1308.png\n",
            "DSC_5856_13_1319.png\n",
            "DSC_5856_13_1325.png\n",
            "DSC_5856_13_1339.png\n",
            "DSC_5856_13_1344.png\n",
            "DSC_5856_13_1348.png\n",
            "DSC_5856_13_1359.png\n",
            "DSC_5856_13_1383.png\n",
            "DSC_5856_13_1384.png\n",
            "DSC_5856_13_1390.png\n",
            "DSC_5856_13_1421.png\n",
            "DSC_5856_13_1424.png\n",
            "DSC_5856_13_1433.png\n",
            "DSC_5856_13_1436.png\n",
            "DSC_5856_13_1480.png\n",
            "DSC_5856_13_1481.png\n",
            "DSC_5856_13_1499.png\n",
            "DSC_5856_21_500.png\n",
            "DSC_5856_21_501.png\n",
            "DSC_5856_21_508.png\n",
            "DSC_5856_21_511.png\n",
            "DSC_5856_21_516.png\n",
            "DSC_5856_21_529.png\n",
            "DSC_5856_21_532.png\n",
            "DSC_5856_21_535.png\n",
            "DSC_5856_21_545.png\n",
            "DSC_5856_21_546.png\n",
            "DSC_5856_21_550.png\n",
            "DSC_5856_21_551.png\n",
            "DSC_5856_21_555.png\n",
            "DSC_5856_21_558.png\n",
            "DSC_5856_21_566.png\n",
            "DSC_5856_21_579.png\n",
            "DSC_5856_21_581.png\n",
            "DSC_5856_21_585.png\n",
            "DSC_5856_21_600.png\n",
            "DSC_5856_21_606.png\n",
            "DSC_5856_21_622.png\n",
            "DSC_5856_21_625.png\n",
            "DSC_5856_21_638.png\n",
            "DSC_5856_21_643.png\n",
            "DSC_5856_21_645.png\n",
            "DSC_5856_21_649.png\n",
            "DSC_5856_21_651.png\n",
            "DSC_5856_21_657.png\n",
            "DSC_5856_21_672.png\n",
            "DSC_5856_21_674.png\n",
            "DSC_5856_21_683.png\n",
            "DSC_5856_21_689.png\n",
            "DSC_5856_21_692.png\n",
            "DSC_5856_21_693.png\n",
            "DSC_5856_21_695.png\n",
            "DSC_5856_21_697.png\n",
            "DSC_5856_21_700.png\n",
            "DSC_5856_21_712.png\n",
            "DSC_5856_21_719.png\n",
            "DSC_5856_21_723.png\n",
            "DSC_5856_21_724.png\n",
            "DSC_5856_21_731.png\n",
            "DSC_5856_21_733.png\n",
            "DSC_5856_21_735.png\n",
            "DSC_5856_21_742.png\n",
            "DSC_5856_21_752.png\n",
            "DSC_5856_21_762.png\n",
            "DSC_5856_21_768.png\n",
            "DSC_5856_21_774.png\n",
            "DSC_5856_21_784.png\n",
            "DSC_5856_21_794.png\n",
            "DSC_5856_21_810.png\n",
            "DSC_5856_21_823.png\n",
            "DSC_5856_21_833.png\n",
            "DSC_5856_21_837.png\n",
            "DSC_5856_21_839.png\n",
            "DSC_5856_21_847.png\n",
            "DSC_5856_21_851.png\n",
            "DSC_5856_21_856.png\n",
            "DSC_5856_21_859.png\n",
            "DSC_5856_21_867.png\n",
            "DSC_5856_21_873.png\n",
            "DSC_5856_21_877.png\n",
            "DSC_5856_21_879.png\n",
            "DSC_5856_21_892.png\n",
            "DSC_5856_21_893.png\n",
            "DSC_5856_21_908.png\n",
            "DSC_5856_21_913.png\n",
            "DSC_5856_21_918.png\n",
            "DSC_5856_21_925.png\n",
            "DSC_5856_21_928.png\n",
            "DSC_5856_21_934.png\n",
            "DSC_5856_21_943.png\n",
            "DSC_5856_21_952.png\n",
            "DSC_5856_21_954.png\n",
            "DSC_5856_21_957.png\n",
            "DSC_5856_21_965.png\n",
            "DSC_5856_21_968.png\n",
            "DSC_5856_21_979.png\n",
            "DSC_5856_21_998.png\n",
            "DSC_5856_21_1000.png\n",
            "DSC_5856_21_1017.png\n",
            "DSC_5856_21_1018.png\n",
            "DSC_5856_21_1023.png\n",
            "DSC_5856_21_1035.png\n",
            "DSC_5856_21_1053.png\n",
            "DSC_5856_21_1057.png\n",
            "DSC_5856_21_1059.png\n",
            "DSC_5856_21_1064.png\n",
            "DSC_5856_21_1067.png\n",
            "DSC_5856_21_1069.png\n",
            "DSC_5856_21_1075.png\n",
            "DSC_5856_21_1086.png\n",
            "DSC_5856_21_1091.png\n",
            "DSC_5856_21_1111.png\n",
            "DSC_5856_21_1119.png\n",
            "DSC_5856_21_1123.png\n",
            "DSC_5856_21_1127.png\n",
            "DSC_5856_21_1137.png\n",
            "DSC_5856_21_1139.png\n",
            "DSC_5856_21_1140.png\n",
            "DSC_5856_21_1142.png\n",
            "DSC_5856_21_1160.png\n",
            "DSC_5856_21_1165.png\n",
            "DSC_5856_21_1169.png\n",
            "DSC_5856_21_1175.png\n",
            "DSC_5856_21_1180.png\n",
            "DSC_5856_21_1190.png\n",
            "DSC_5856_21_1195.png\n",
            "DSC_5856_21_1199.png\n",
            "DSC_5856_21_1205.png\n",
            "DSC_5856_21_1206.png\n",
            "DSC_5856_21_1207.png\n",
            "DSC_5856_21_1212.png\n",
            "DSC_5856_21_1213.png\n",
            "DSC_5856_21_1219.png\n",
            "DSC_5856_21_1233.png\n",
            "DSC_5856_21_1244.png\n",
            "DSC_5856_21_1248.png\n",
            "DSC_5856_21_1265.png\n",
            "DSC_5856_21_1274.png\n",
            "DSC_5856_21_1277.png\n",
            "DSC_5856_21_1281.png\n",
            "DSC_5856_21_1287.png\n",
            "DSC_5856_21_1291.png\n",
            "DSC_5856_21_1309.png\n",
            "DSC_5856_21_1312.png\n",
            "DSC_5856_21_1314.png\n",
            "DSC_5856_21_1319.png\n",
            "DSC_5856_21_1320.png\n",
            "DSC_5856_21_1350.png\n",
            "DSC_5856_21_1352.png\n",
            "DSC_5856_21_1355.png\n",
            "DSC_5856_21_1362.png\n",
            "DSC_5856_21_1372.png\n",
            "DSC_5856_21_1378.png\n",
            "DSC_5856_21_1384.png\n",
            "DSC_5856_21_1412.png\n",
            "DSC_5856_21_1416.png\n",
            "DSC_5856_21_1418.png\n",
            "DSC_5856_21_1424.png\n",
            "DSC_5856_21_1442.png\n",
            "DSC_5856_21_1444.png\n",
            "DSC_5856_21_1450.png\n",
            "DSC_5856_21_1460.png\n",
            "DSC_5856_21_1477.png\n",
            "DSC_5856_21_1478.png\n",
            "DSC_5856_21_1480.png\n",
            "DSC_5856_21_1491.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1-TKp74xH3v",
        "outputId": "78d363db-c9a7-4bde-aeda-a50cf90b53ec"
      },
      "source": [
        "data1=df[df['5']==3]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']\r\n",
        "a=data1.values\r\n",
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "for ln in data:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/3/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_21.png\n",
            "DSC_5856_12_81.png\n",
            "DSC_5856_12_89.png\n",
            "DSC_5856_12_93.png\n",
            "DSC_5856_12_94.png\n",
            "DSC_5856_12_105.png\n",
            "DSC_5856_12_107.png\n",
            "DSC_5856_12_111.png\n",
            "DSC_5856_12_115.png\n",
            "DSC_5856_12_116.png\n",
            "DSC_5856_12_119.png\n",
            "DSC_5856_12_124.png\n",
            "DSC_5856_12_125.png\n",
            "DSC_5856_12_127.png\n",
            "DSC_5856_12_129.png\n",
            "DSC_5856_12_130.png\n",
            "DSC_5856_12_132.png\n",
            "DSC_5856_12_160.png\n",
            "DSC_5856_12_178.png\n",
            "DSC_5856_12_181.png\n",
            "DSC_5856_12_182.png\n",
            "DSC_5856_12_204.png\n",
            "DSC_5856_12_206.png\n",
            "DSC_5856_12_207.png\n",
            "DSC_5856_12_208.png\n",
            "DSC_5856_12_220.png\n",
            "DSC_5856_12_224.png\n",
            "DSC_5856_12_234.png\n",
            "DSC_5856_12_242.png\n",
            "DSC_5856_12_255.png\n",
            "DSC_5856_12_259.png\n",
            "DSC_5856_12_264.png\n",
            "DSC_5856_12_268.png\n",
            "DSC_5856_12_271.png\n",
            "DSC_5856_12_279.png\n",
            "DSC_5856_12_289.png\n",
            "DSC_5856_12_290.png\n",
            "DSC_5856_12_300.png\n",
            "DSC_5856_12_321.png\n",
            "DSC_5856_12_343.png\n",
            "DSC_5856_12_353.png\n",
            "DSC_5856_12_366.png\n",
            "DSC_5856_12_373.png\n",
            "DSC_5856_12_394.png\n",
            "DSC_5856_12_407.png\n",
            "DSC_5856_12_413.png\n",
            "DSC_5856_12_417.png\n",
            "DSC_5856_12_426.png\n",
            "DSC_5856_12_429.png\n",
            "DSC_5856_12_449.png\n",
            "DSC_5856_12_452.png\n",
            "DSC_5856_12_469.png\n",
            "DSC_5856_12_475.png\n",
            "DSC_5856_12_486.png\n",
            "DSC_5856_13_18.png\n",
            "DSC_5856_13_25.png\n",
            "DSC_5856_13_28.png\n",
            "DSC_5856_13_29.png\n",
            "DSC_5856_13_30.png\n",
            "DSC_5856_13_35.png\n",
            "DSC_5856_13_46.png\n",
            "DSC_5856_13_49.png\n",
            "DSC_5856_13_50.png\n",
            "DSC_5856_13_61.png\n",
            "DSC_5856_13_69.png\n",
            "DSC_5856_13_72.png\n",
            "DSC_5856_13_77.png\n",
            "DSC_5856_13_79.png\n",
            "DSC_5856_13_85.png\n",
            "DSC_5856_13_86.png\n",
            "DSC_5856_13_88.png\n",
            "DSC_5856_13_89.png\n",
            "DSC_5856_13_100.png\n",
            "DSC_5856_13_104.png\n",
            "DSC_5856_13_110.png\n",
            "DSC_5856_13_111.png\n",
            "DSC_5856_13_114.png\n",
            "DSC_5856_13_117.png\n",
            "DSC_5856_13_118.png\n",
            "DSC_5856_13_119.png\n",
            "DSC_5856_13_120.png\n",
            "DSC_5856_13_122.png\n",
            "DSC_5856_13_123.png\n",
            "DSC_5856_13_133.png\n",
            "DSC_5856_13_141.png\n",
            "DSC_5856_13_150.png\n",
            "DSC_5856_13_151.png\n",
            "DSC_5856_13_153.png\n",
            "DSC_5856_13_156.png\n",
            "DSC_5856_13_160.png\n",
            "DSC_5856_13_165.png\n",
            "DSC_5856_13_176.png\n",
            "DSC_5856_13_181.png\n",
            "DSC_5856_13_185.png\n",
            "DSC_5856_13_192.png\n",
            "DSC_5856_13_209.png\n",
            "DSC_5856_13_224.png\n",
            "DSC_5856_13_232.png\n",
            "DSC_5856_13_235.png\n",
            "DSC_5856_13_236.png\n",
            "DSC_5856_13_261.png\n",
            "DSC_5856_13_264.png\n",
            "DSC_5856_13_269.png\n",
            "DSC_5856_13_270.png\n",
            "DSC_5856_13_271.png\n",
            "DSC_5856_13_272.png\n",
            "DSC_5856_13_273.png\n",
            "DSC_5856_13_276.png\n",
            "DSC_5856_13_287.png\n",
            "DSC_5856_13_291.png\n",
            "DSC_5856_13_301.png\n",
            "DSC_5856_13_310.png\n",
            "DSC_5856_13_311.png\n",
            "DSC_5856_13_319.png\n",
            "DSC_5856_13_322.png\n",
            "DSC_5856_13_324.png\n",
            "DSC_5856_13_337.png\n",
            "DSC_5856_13_339.png\n",
            "DSC_5856_13_343.png\n",
            "DSC_5856_13_346.png\n",
            "DSC_5856_13_352.png\n",
            "DSC_5856_13_354.png\n",
            "DSC_5856_13_362.png\n",
            "DSC_5856_13_377.png\n",
            "DSC_5856_13_379.png\n",
            "DSC_5856_13_381.png\n",
            "DSC_5856_13_395.png\n",
            "DSC_5856_13_396.png\n",
            "DSC_5856_13_407.png\n",
            "DSC_5856_13_416.png\n",
            "DSC_5856_13_424.png\n",
            "DSC_5856_13_431.png\n",
            "DSC_5856_13_432.png\n",
            "DSC_5856_13_442.png\n",
            "DSC_5856_13_443.png\n",
            "DSC_5856_13_446.png\n",
            "DSC_5856_13_450.png\n",
            "DSC_5856_13_453.png\n",
            "DSC_5856_13_454.png\n",
            "DSC_5856_13_468.png\n",
            "DSC_5856_13_472.png\n",
            "DSC_5856_13_489.png\n",
            "DSC_5856_13_493.png\n",
            "DSC_5856_13_494.png\n",
            "DSC_5856_13_499.png\n",
            "DSC_5856_21_17.png\n",
            "DSC_5856_21_32.png\n",
            "DSC_5856_21_38.png\n",
            "DSC_5856_21_46.png\n",
            "DSC_5856_21_47.png\n",
            "DSC_5856_21_76.png\n",
            "DSC_5856_21_91.png\n",
            "DSC_5856_21_102.png\n",
            "DSC_5856_21_103.png\n",
            "DSC_5856_21_118.png\n",
            "DSC_5856_21_119.png\n",
            "DSC_5856_21_134.png\n",
            "DSC_5856_21_140.png\n",
            "DSC_5856_21_148.png\n",
            "DSC_5856_21_157.png\n",
            "DSC_5856_21_195.png\n",
            "DSC_5856_21_224.png\n",
            "DSC_5856_21_225.png\n",
            "DSC_5856_21_228.png\n",
            "DSC_5856_21_232.png\n",
            "DSC_5856_21_240.png\n",
            "DSC_5856_21_247.png\n",
            "DSC_5856_21_252.png\n",
            "DSC_5856_21_282.png\n",
            "DSC_5856_21_293.png\n",
            "DSC_5856_21_299.png\n",
            "DSC_5856_21_307.png\n",
            "DSC_5856_21_308.png\n",
            "DSC_5856_21_311.png\n",
            "DSC_5856_21_335.png\n",
            "DSC_5856_21_358.png\n",
            "DSC_5856_21_359.png\n",
            "DSC_5856_21_392.png\n",
            "DSC_5856_21_396.png\n",
            "DSC_5856_21_399.png\n",
            "DSC_5856_21_410.png\n",
            "DSC_5856_21_423.png\n",
            "DSC_5856_21_424.png\n",
            "DSC_5856_21_447.png\n",
            "DSC_5856_21_450.png\n",
            "DSC_5856_21_452.png\n",
            "DSC_5856_21_470.png\n",
            "DSC_5856_21_488.png\n",
            "DSC_5856_21_491.png\n",
            "DSC_5856_21_494.png\n",
            "DSC_5856_25_11.png\n",
            "DSC_5856_25_26.png\n",
            "DSC_5856_25_42.png\n",
            "DSC_5856_25_43.png\n",
            "DSC_5856_25_48.png\n",
            "DSC_5856_25_50.png\n",
            "DSC_5856_25_69.png\n",
            "DSC_5856_25_71.png\n",
            "DSC_5856_25_83.png\n",
            "DSC_5856_25_89.png\n",
            "DSC_5856_25_107.png\n",
            "DSC_5856_25_108.png\n",
            "DSC_5856_25_126.png\n",
            "DSC_5856_25_130.png\n",
            "DSC_5856_25_148.png\n",
            "DSC_5856_25_156.png\n",
            "DSC_5856_25_158.png\n",
            "DSC_5856_25_180.png\n",
            "DSC_5856_25_202.png\n",
            "DSC_5856_25_236.png\n",
            "DSC_5856_25_243.png\n",
            "DSC_5856_25_247.png\n",
            "DSC_5856_25_250.png\n",
            "DSC_5856_25_253.png\n",
            "DSC_5856_25_255.png\n",
            "DSC_5856_25_267.png\n",
            "DSC_5856_25_268.png\n",
            "DSC_5856_25_274.png\n",
            "DSC_5856_25_287.png\n",
            "DSC_5856_25_296.png\n",
            "DSC_5856_25_300.png\n",
            "DSC_5856_25_301.png\n",
            "DSC_5856_25_317.png\n",
            "DSC_5856_25_330.png\n",
            "DSC_5856_25_342.png\n",
            "DSC_5856_25_347.png\n",
            "DSC_5856_25_372.png\n",
            "DSC_5856_25_375.png\n",
            "DSC_5856_25_385.png\n",
            "DSC_5856_25_387.png\n",
            "DSC_5856_25_392.png\n",
            "DSC_5856_25_393.png\n",
            "DSC_5856_25_395.png\n",
            "DSC_5856_25_396.png\n",
            "DSC_5856_25_397.png\n",
            "DSC_5856_25_415.png\n",
            "DSC_5856_25_419.png\n",
            "DSC_5856_25_423.png\n",
            "DSC_5856_25_454.png\n",
            "DSC_5856_25_477.png\n",
            "DSC_5856_25_498.png\n",
            "DSC_5856_11_8.png\n",
            "DSC_5856_11_10.png\n",
            "DSC_5856_11_17.png\n",
            "DSC_5856_11_26.png\n",
            "DSC_5856_11_33.png\n",
            "DSC_5856_11_34.png\n",
            "DSC_5856_11_36.png\n",
            "DSC_5856_11_47.png\n",
            "DSC_5856_11_49.png\n",
            "DSC_5856_11_51.png\n",
            "DSC_5856_11_52.png\n",
            "DSC_5856_11_53.png\n",
            "DSC_5856_11_59.png\n",
            "DSC_5856_11_64.png\n",
            "DSC_5856_11_66.png\n",
            "DSC_5856_11_86.png\n",
            "DSC_5856_11_90.png\n",
            "DSC_5856_11_91.png\n",
            "DSC_5856_11_125.png\n",
            "DSC_5856_11_127.png\n",
            "DSC_5856_11_136.png\n",
            "DSC_5856_11_137.png\n",
            "DSC_5856_11_144.png\n",
            "DSC_5856_11_172.png\n",
            "DSC_5856_11_175.png\n",
            "DSC_5856_11_176.png\n",
            "DSC_5856_11_178.png\n",
            "DSC_5856_11_200.png\n",
            "DSC_5856_11_202.png\n",
            "DSC_5856_11_208.png\n",
            "DSC_5856_11_216.png\n",
            "DSC_5856_11_217.png\n",
            "DSC_5856_11_218.png\n",
            "DSC_5856_11_234.png\n",
            "DSC_5856_11_236.png\n",
            "DSC_5856_11_239.png\n",
            "DSC_5856_11_240.png\n",
            "DSC_5856_11_249.png\n",
            "DSC_5856_11_299.png\n",
            "DSC_5856_11_302.png\n",
            "DSC_5856_11_306.png\n",
            "DSC_5856_11_307.png\n",
            "DSC_5856_11_312.png\n",
            "DSC_5856_11_328.png\n",
            "DSC_5856_11_335.png\n",
            "DSC_5856_11_342.png\n",
            "DSC_5856_11_343.png\n",
            "DSC_5856_11_347.png\n",
            "DSC_5856_11_348.png\n",
            "DSC_5856_11_349.png\n",
            "DSC_5856_11_352.png\n",
            "DSC_5856_11_388.png\n",
            "DSC_5856_11_389.png\n",
            "DSC_5856_11_393.png\n",
            "DSC_5856_11_397.png\n",
            "DSC_5856_11_415.png\n",
            "DSC_5856_11_418.png\n",
            "DSC_5856_11_423.png\n",
            "DSC_5856_11_440.png\n",
            "DSC_5856_11_447.png\n",
            "DSC_5856_11_456.png\n",
            "DSC_5856_11_459.png\n",
            "DSC_5856_11_463.png\n",
            "DSC_5856_11_479.png\n",
            "DSC_5856_11_484.png\n",
            "DSC_5856_11_494.png\n",
            "DSC_5856_2_0.png\n",
            "DSC_5856_2_3.png\n",
            "DSC_5856_2_5.png\n",
            "DSC_5856_2_12.png\n",
            "DSC_5856_2_17.png\n",
            "DSC_5856_2_28.png\n",
            "DSC_5856_2_46.png\n",
            "DSC_5856_2_69.png\n",
            "DSC_5856_2_74.png\n",
            "DSC_5856_2_80.png\n",
            "DSC_5856_2_98.png\n",
            "DSC_5856_2_101.png\n",
            "DSC_5856_2_107.png\n",
            "DSC_5856_2_112.png\n",
            "DSC_5856_2_122.png\n",
            "DSC_5856_2_138.png\n",
            "DSC_5856_2_140.png\n",
            "DSC_5856_2_141.png\n",
            "DSC_5856_2_142.png\n",
            "DSC_5856_2_148.png\n",
            "DSC_5856_2_152.png\n",
            "DSC_5856_2_186.png\n",
            "DSC_5856_2_188.png\n",
            "DSC_5856_2_194.png\n",
            "DSC_5856_2_195.png\n",
            "DSC_5856_2_199.png\n",
            "DSC_5856_2_205.png\n",
            "DSC_5856_2_221.png\n",
            "DSC_5856_2_225.png\n",
            "DSC_5856_2_230.png\n",
            "DSC_5856_2_234.png\n",
            "DSC_5856_2_235.png\n",
            "DSC_5856_2_245.png\n",
            "DSC_5856_2_249.png\n",
            "DSC_5856_2_263.png\n",
            "DSC_5856_2_267.png\n",
            "DSC_5856_2_268.png\n",
            "DSC_5856_2_284.png\n",
            "DSC_5856_2_287.png\n",
            "DSC_5856_2_288.png\n",
            "DSC_5856_2_289.png\n",
            "DSC_5856_2_302.png\n",
            "DSC_5856_2_305.png\n",
            "DSC_5856_2_321.png\n",
            "DSC_5856_2_333.png\n",
            "DSC_5856_2_352.png\n",
            "DSC_5856_2_390.png\n",
            "DSC_5856_2_394.png\n",
            "DSC_5856_2_421.png\n",
            "DSC_5856_2_430.png\n",
            "DSC_5856_2_431.png\n",
            "DSC_5856_2_445.png\n",
            "DSC_5856_2_449.png\n",
            "DSC_5856_2_450.png\n",
            "DSC_5856_2_470.png\n",
            "DSC_5856_2_472.png\n",
            "DSC_5856_2_474.png\n",
            "DSC_5856_31_59.png\n",
            "DSC_5856_31_61.png\n",
            "DSC_5856_31_80.png\n",
            "DSC_5856_31_146.png\n",
            "DSC_5856_31_150.png\n",
            "DSC_5856_31_154.png\n",
            "DSC_5856_31_167.png\n",
            "DSC_5856_31_200.png\n",
            "DSC_5856_31_222.png\n",
            "DSC_5856_31_254.png\n",
            "DSC_5856_31_265.png\n",
            "DSC_5856_31_276.png\n",
            "DSC_5856_31_283.png\n",
            "DSC_5856_31_315.png\n",
            "DSC_5856_31_321.png\n",
            "DSC_5856_31_337.png\n",
            "DSC_5856_31_392.png\n",
            "DSC_5856_31_485.png\n",
            "DSC_5856_52_11.png\n",
            "DSC_5856_52_15.png\n",
            "DSC_5856_52_25.png\n",
            "DSC_5856_52_26.png\n",
            "DSC_5856_52_37.png\n",
            "DSC_5856_52_44.png\n",
            "DSC_5856_52_66.png\n",
            "DSC_5856_52_80.png\n",
            "DSC_5856_52_104.png\n",
            "DSC_5856_52_119.png\n",
            "DSC_5856_52_121.png\n",
            "DSC_5856_52_136.png\n",
            "DSC_5856_52_140.png\n",
            "DSC_5856_52_154.png\n",
            "DSC_5856_52_156.png\n",
            "DSC_5856_52_172.png\n",
            "DSC_5856_52_180.png\n",
            "DSC_5856_52_187.png\n",
            "DSC_5856_52_200.png\n",
            "DSC_5856_52_205.png\n",
            "DSC_5856_52_222.png\n",
            "DSC_5856_52_228.png\n",
            "DSC_5856_52_251.png\n",
            "DSC_5856_52_257.png\n",
            "DSC_5856_52_270.png\n",
            "DSC_5856_52_274.png\n",
            "DSC_5856_52_280.png\n",
            "DSC_5856_52_284.png\n",
            "DSC_5856_52_286.png\n",
            "DSC_5856_52_294.png\n",
            "DSC_5856_52_307.png\n",
            "DSC_5856_52_331.png\n",
            "DSC_5856_52_332.png\n",
            "DSC_5856_52_350.png\n",
            "DSC_5856_52_365.png\n",
            "DSC_5856_52_368.png\n",
            "DSC_5856_52_388.png\n",
            "DSC_5856_52_393.png\n",
            "DSC_5856_52_407.png\n",
            "DSC_5856_52_410.png\n",
            "DSC_5856_52_421.png\n",
            "DSC_5856_52_423.png\n",
            "DSC_5856_52_435.png\n",
            "DSC_5856_52_436.png\n",
            "DSC_5856_52_447.png\n",
            "DSC_5856_52_467.png\n",
            "DSC_5856_52_469.png\n",
            "DSC_5856_52_485.png\n",
            "DSC_5856_52_488.png\n",
            "DSC_5856_52_490.png\n",
            "DSC_5856_52_497.png\n",
            "DSC_5856_52_499.png\n",
            "DSC_5856_1_6.png\n",
            "DSC_5856_1_22.png\n",
            "DSC_5856_1_33.png\n",
            "DSC_5856_1_36.png\n",
            "DSC_5856_1_54.png\n",
            "DSC_5856_1_55.png\n",
            "DSC_5856_1_77.png\n",
            "DSC_5856_1_101.png\n",
            "DSC_5856_1_128.png\n",
            "DSC_5856_1_132.png\n",
            "DSC_5856_1_148.png\n",
            "DSC_5856_1_149.png\n",
            "DSC_5856_1_150.png\n",
            "DSC_5856_1_160.png\n",
            "DSC_5856_1_169.png\n",
            "DSC_5856_1_173.png\n",
            "DSC_5856_1_180.png\n",
            "DSC_5856_1_191.png\n",
            "DSC_5856_1_200.png\n",
            "DSC_5856_1_226.png\n",
            "DSC_5856_1_264.png\n",
            "DSC_5856_1_293.png\n",
            "DSC_5856_1_308.png\n",
            "DSC_5856_1_315.png\n",
            "DSC_5856_1_339.png\n",
            "DSC_5856_1_353.png\n",
            "DSC_5856_1_355.png\n",
            "DSC_5856_1_365.png\n",
            "DSC_5856_1_381.png\n",
            "DSC_5856_1_387.png\n",
            "DSC_5856_1_394.png\n",
            "DSC_5856_1_407.png\n",
            "DSC_5856_1_411.png\n",
            "DSC_5856_1_424.png\n",
            "DSC_5856_1_446.png\n",
            "DSC_5856_1_467.png\n",
            "DSC_5856_1_499.png\n",
            "DSC_5856_1_505.png\n",
            "DSC_5856_1_545.png\n",
            "DSC_5856_1_547.png\n",
            "DSC_5856_1_553.png\n",
            "DSC_5856_1_572.png\n",
            "DSC_5856_1_576.png\n",
            "DSC_5856_1_578.png\n",
            "DSC_5856_1_581.png\n",
            "DSC_5856_1_598.png\n",
            "DSC_5856_1_614.png\n",
            "DSC_5856_1_631.png\n",
            "DSC_5856_1_632.png\n",
            "DSC_5856_1_642.png\n",
            "DSC_5856_1_643.png\n",
            "DSC_5856_1_650.png\n",
            "DSC_5856_1_656.png\n",
            "DSC_5856_1_663.png\n",
            "DSC_5856_1_679.png\n",
            "DSC_5856_1_686.png\n",
            "DSC_5856_1_707.png\n",
            "DSC_5856_1_709.png\n",
            "DSC_5856_1_714.png\n",
            "DSC_5856_1_723.png\n",
            "DSC_5856_1_732.png\n",
            "DSC_5856_1_734.png\n",
            "DSC_5856_1_737.png\n",
            "DSC_5856_1_747.png\n",
            "DSC_5856_1_751.png\n",
            "DSC_5856_1_755.png\n",
            "DSC_5856_1_756.png\n",
            "DSC_5856_1_801.png\n",
            "DSC_5856_1_807.png\n",
            "DSC_5856_1_843.png\n",
            "DSC_5856_1_844.png\n",
            "DSC_5856_1_847.png\n",
            "DSC_5856_1_863.png\n",
            "DSC_5856_1_874.png\n",
            "DSC_5856_1_917.png\n",
            "DSC_5856_1_918.png\n",
            "DSC_5856_1_929.png\n",
            "DSC_5856_1_931.png\n",
            "DSC_5856_1_960.png\n",
            "DSC_5856_1_975.png\n",
            "DSC_5856_1_978.png\n",
            "DSC_5856_1_983.png\n",
            "DSC_5856_1_990.png\n",
            "DSC_5856_14_4.png\n",
            "DSC_5856_14_15.png\n",
            "DSC_5856_14_22.png\n",
            "DSC_5856_14_25.png\n",
            "DSC_5856_14_32.png\n",
            "DSC_5856_14_40.png\n",
            "DSC_5856_14_41.png\n",
            "DSC_5856_14_44.png\n",
            "DSC_5856_14_45.png\n",
            "DSC_5856_14_53.png\n",
            "DSC_5856_14_63.png\n",
            "DSC_5856_14_65.png\n",
            "DSC_5856_14_74.png\n",
            "DSC_5856_14_83.png\n",
            "DSC_5856_14_90.png\n",
            "DSC_5856_14_93.png\n",
            "DSC_5856_14_101.png\n",
            "DSC_5856_14_102.png\n",
            "DSC_5856_14_108.png\n",
            "DSC_5856_14_111.png\n",
            "DSC_5856_14_113.png\n",
            "DSC_5856_14_115.png\n",
            "DSC_5856_14_121.png\n",
            "DSC_5856_14_122.png\n",
            "DSC_5856_14_135.png\n",
            "DSC_5856_14_136.png\n",
            "DSC_5856_14_139.png\n",
            "DSC_5856_14_141.png\n",
            "DSC_5856_14_142.png\n",
            "DSC_5856_14_146.png\n",
            "DSC_5856_14_154.png\n",
            "DSC_5856_14_157.png\n",
            "DSC_5856_14_160.png\n",
            "DSC_5856_14_172.png\n",
            "DSC_5856_14_179.png\n",
            "DSC_5856_14_182.png\n",
            "DSC_5856_14_196.png\n",
            "DSC_5856_14_203.png\n",
            "DSC_5856_14_205.png\n",
            "DSC_5856_14_209.png\n",
            "DSC_5856_14_215.png\n",
            "DSC_5856_14_229.png\n",
            "DSC_5856_14_231.png\n",
            "DSC_5856_14_240.png\n",
            "DSC_5856_14_242.png\n",
            "DSC_5856_14_243.png\n",
            "DSC_5856_14_244.png\n",
            "DSC_5856_14_253.png\n",
            "DSC_5856_14_269.png\n",
            "DSC_5856_14_271.png\n",
            "DSC_5856_14_279.png\n",
            "DSC_5856_14_287.png\n",
            "DSC_5856_14_298.png\n",
            "DSC_5856_14_299.png\n",
            "DSC_5856_14_307.png\n",
            "DSC_5856_14_314.png\n",
            "DSC_5856_14_334.png\n",
            "DSC_5856_14_352.png\n",
            "DSC_5856_14_368.png\n",
            "DSC_5856_14_369.png\n",
            "DSC_5856_14_373.png\n",
            "DSC_5856_14_375.png\n",
            "DSC_5856_14_377.png\n",
            "DSC_5856_14_380.png\n",
            "DSC_5856_14_385.png\n",
            "DSC_5856_14_386.png\n",
            "DSC_5856_14_387.png\n",
            "DSC_5856_14_390.png\n",
            "DSC_5856_14_393.png\n",
            "DSC_5856_14_396.png\n",
            "DSC_5856_14_405.png\n",
            "DSC_5856_14_407.png\n",
            "DSC_5856_14_411.png\n",
            "DSC_5856_14_413.png\n",
            "DSC_5856_14_414.png\n",
            "DSC_5856_14_420.png\n",
            "DSC_5856_14_427.png\n",
            "DSC_5856_14_438.png\n",
            "DSC_5856_14_446.png\n",
            "DSC_5856_14_447.png\n",
            "DSC_5856_14_449.png\n",
            "DSC_5856_14_450.png\n",
            "DSC_5856_14_452.png\n",
            "DSC_5856_14_453.png\n",
            "DSC_5856_14_464.png\n",
            "DSC_5856_14_475.png\n",
            "DSC_5856_14_476.png\n",
            "DSC_5856_14_480.png\n",
            "DSC_5856_14_486.png\n",
            "DSC_5856_14_500.png\n",
            "DSC_5856_14_512.png\n",
            "DSC_5856_14_521.png\n",
            "DSC_5856_14_527.png\n",
            "DSC_5856_14_529.png\n",
            "DSC_5856_14_531.png\n",
            "DSC_5856_14_540.png\n",
            "DSC_5856_14_543.png\n",
            "DSC_5856_14_555.png\n",
            "DSC_5856_14_559.png\n",
            "DSC_5856_14_564.png\n",
            "DSC_5856_14_565.png\n",
            "DSC_5856_14_566.png\n",
            "DSC_5856_14_571.png\n",
            "DSC_5856_14_583.png\n",
            "DSC_5856_14_586.png\n",
            "DSC_5856_14_592.png\n",
            "DSC_5856_14_599.png\n",
            "DSC_5856_14_601.png\n",
            "DSC_5856_14_602.png\n",
            "DSC_5856_14_608.png\n",
            "DSC_5856_14_609.png\n",
            "DSC_5856_14_611.png\n",
            "DSC_5856_14_612.png\n",
            "DSC_5856_14_614.png\n",
            "DSC_5856_14_643.png\n",
            "DSC_5856_14_648.png\n",
            "DSC_5856_14_655.png\n",
            "DSC_5856_14_660.png\n",
            "DSC_5856_14_668.png\n",
            "DSC_5856_14_673.png\n",
            "DSC_5856_14_684.png\n",
            "DSC_5856_14_685.png\n",
            "DSC_5856_14_686.png\n",
            "DSC_5856_14_694.png\n",
            "DSC_5856_14_696.png\n",
            "DSC_5856_14_712.png\n",
            "DSC_5856_14_719.png\n",
            "DSC_5856_14_737.png\n",
            "DSC_5856_14_739.png\n",
            "DSC_5856_14_744.png\n",
            "DSC_5856_14_745.png\n",
            "DSC_5856_14_750.png\n",
            "DSC_5856_14_755.png\n",
            "DSC_5856_14_756.png\n",
            "DSC_5856_14_758.png\n",
            "DSC_5856_14_760.png\n",
            "DSC_5856_14_766.png\n",
            "DSC_5856_14_768.png\n",
            "DSC_5856_14_772.png\n",
            "DSC_5856_14_779.png\n",
            "DSC_5856_14_785.png\n",
            "DSC_5856_14_787.png\n",
            "DSC_5856_14_788.png\n",
            "DSC_5856_14_795.png\n",
            "DSC_5856_14_796.png\n",
            "DSC_5856_14_798.png\n",
            "DSC_5856_14_808.png\n",
            "DSC_5856_14_811.png\n",
            "DSC_5856_14_812.png\n",
            "DSC_5856_14_821.png\n",
            "DSC_5856_14_838.png\n",
            "DSC_5856_14_843.png\n",
            "DSC_5856_14_845.png\n",
            "DSC_5856_14_850.png\n",
            "DSC_5856_14_852.png\n",
            "DSC_5856_14_853.png\n",
            "DSC_5856_14_858.png\n",
            "DSC_5856_14_871.png\n",
            "DSC_5856_14_875.png\n",
            "DSC_5856_14_876.png\n",
            "DSC_5856_14_882.png\n",
            "DSC_5856_14_883.png\n",
            "DSC_5856_14_889.png\n",
            "DSC_5856_14_905.png\n",
            "DSC_5856_14_921.png\n",
            "DSC_5856_14_923.png\n",
            "DSC_5856_14_925.png\n",
            "DSC_5856_14_927.png\n",
            "DSC_5856_14_931.png\n",
            "DSC_5856_14_933.png\n",
            "DSC_5856_14_936.png\n",
            "DSC_5856_14_938.png\n",
            "DSC_5856_14_957.png\n",
            "DSC_5856_14_962.png\n",
            "DSC_5856_14_973.png\n",
            "DSC_5856_14_975.png\n",
            "DSC_5856_14_981.png\n",
            "DSC_5856_14_986.png\n",
            "DSC_5856_12_500.png\n",
            "DSC_5856_12_504.png\n",
            "DSC_5856_12_507.png\n",
            "DSC_5856_12_515.png\n",
            "DSC_5856_12_516.png\n",
            "DSC_5856_12_518.png\n",
            "DSC_5856_12_519.png\n",
            "DSC_5856_12_523.png\n",
            "DSC_5856_12_540.png\n",
            "DSC_5856_12_542.png\n",
            "DSC_5856_12_551.png\n",
            "DSC_5856_12_558.png\n",
            "DSC_5856_12_564.png\n",
            "DSC_5856_12_569.png\n",
            "DSC_5856_12_572.png\n",
            "DSC_5856_12_593.png\n",
            "DSC_5856_12_595.png\n",
            "DSC_5856_12_632.png\n",
            "DSC_5856_12_637.png\n",
            "DSC_5856_12_640.png\n",
            "DSC_5856_12_643.png\n",
            "DSC_5856_12_650.png\n",
            "DSC_5856_12_670.png\n",
            "DSC_5856_12_682.png\n",
            "DSC_5856_12_688.png\n",
            "DSC_5856_12_699.png\n",
            "DSC_5856_12_702.png\n",
            "DSC_5856_12_710.png\n",
            "DSC_5856_12_712.png\n",
            "DSC_5856_12_718.png\n",
            "DSC_5856_12_719.png\n",
            "DSC_5856_12_720.png\n",
            "DSC_5856_12_766.png\n",
            "DSC_5856_12_768.png\n",
            "DSC_5856_12_777.png\n",
            "DSC_5856_12_779.png\n",
            "DSC_5856_12_786.png\n",
            "DSC_5856_12_792.png\n",
            "DSC_5856_12_798.png\n",
            "DSC_5856_12_817.png\n",
            "DSC_5856_12_824.png\n",
            "DSC_5856_12_827.png\n",
            "DSC_5856_12_849.png\n",
            "DSC_5856_12_863.png\n",
            "DSC_5856_12_869.png\n",
            "DSC_5856_12_874.png\n",
            "DSC_5856_12_875.png\n",
            "DSC_5856_12_878.png\n",
            "DSC_5856_12_881.png\n",
            "DSC_5856_12_890.png\n",
            "DSC_5856_12_891.png\n",
            "DSC_5856_12_899.png\n",
            "DSC_5856_12_900.png\n",
            "DSC_5856_12_910.png\n",
            "DSC_5856_12_924.png\n",
            "DSC_5856_12_938.png\n",
            "DSC_5856_12_960.png\n",
            "DSC_5856_12_962.png\n",
            "DSC_5856_12_987.png\n",
            "DSC_5856_12_1002.png\n",
            "DSC_5856_12_1011.png\n",
            "DSC_5856_12_1016.png\n",
            "DSC_5856_12_1025.png\n",
            "DSC_5856_12_1030.png\n",
            "DSC_5856_12_1031.png\n",
            "DSC_5856_12_1040.png\n",
            "DSC_5856_12_1044.png\n",
            "DSC_5856_12_1048.png\n",
            "DSC_5856_12_1053.png\n",
            "DSC_5856_12_1056.png\n",
            "DSC_5856_12_1077.png\n",
            "DSC_5856_12_1084.png\n",
            "DSC_5856_12_1089.png\n",
            "DSC_5856_12_1095.png\n",
            "DSC_5856_12_1096.png\n",
            "DSC_5856_12_1098.png\n",
            "DSC_5856_12_1113.png\n",
            "DSC_5856_12_1119.png\n",
            "DSC_5856_12_1130.png\n",
            "DSC_5856_12_1145.png\n",
            "DSC_5856_12_1153.png\n",
            "DSC_5856_12_1154.png\n",
            "DSC_5856_12_1155.png\n",
            "DSC_5856_12_1159.png\n",
            "DSC_5856_12_1173.png\n",
            "DSC_5856_12_1176.png\n",
            "DSC_5856_12_1190.png\n",
            "DSC_5856_12_1195.png\n",
            "DSC_5856_12_1196.png\n",
            "DSC_5856_12_1214.png\n",
            "DSC_5856_12_1217.png\n",
            "DSC_5856_12_1222.png\n",
            "DSC_5856_12_1226.png\n",
            "DSC_5856_12_1232.png\n",
            "DSC_5856_12_1238.png\n",
            "DSC_5856_12_1251.png\n",
            "DSC_5856_12_1252.png\n",
            "DSC_5856_12_1253.png\n",
            "DSC_5856_12_1255.png\n",
            "DSC_5856_12_1259.png\n",
            "DSC_5856_12_1260.png\n",
            "DSC_5856_12_1277.png\n",
            "DSC_5856_12_1283.png\n",
            "DSC_5856_12_1289.png\n",
            "DSC_5856_12_1296.png\n",
            "DSC_5856_12_1298.png\n",
            "DSC_5856_12_1319.png\n",
            "DSC_5856_12_1334.png\n",
            "DSC_5856_12_1336.png\n",
            "DSC_5856_12_1340.png\n",
            "DSC_5856_12_1342.png\n",
            "DSC_5856_12_1357.png\n",
            "DSC_5856_12_1361.png\n",
            "DSC_5856_12_1371.png\n",
            "DSC_5856_12_1385.png\n",
            "DSC_5856_12_1388.png\n",
            "DSC_5856_12_1409.png\n",
            "DSC_5856_12_1415.png\n",
            "DSC_5856_12_1430.png\n",
            "DSC_5856_12_1433.png\n",
            "DSC_5856_12_1437.png\n",
            "DSC_5856_12_1440.png\n",
            "DSC_5856_12_1443.png\n",
            "DSC_5856_12_1449.png\n",
            "DSC_5856_12_1452.png\n",
            "DSC_5856_12_1457.png\n",
            "DSC_5856_12_1471.png\n",
            "DSC_5856_12_1480.png\n",
            "DSC_5856_12_1485.png\n",
            "DSC_5856_12_1494.png\n",
            "DSC_5856_12_1499.png\n",
            "DSC_5856_13_521.png\n",
            "DSC_5856_13_529.png\n",
            "DSC_5856_13_539.png\n",
            "DSC_5856_13_540.png\n",
            "DSC_5856_13_542.png\n",
            "DSC_5856_13_544.png\n",
            "DSC_5856_13_551.png\n",
            "DSC_5856_13_553.png\n",
            "DSC_5856_13_560.png\n",
            "DSC_5856_13_561.png\n",
            "DSC_5856_13_566.png\n",
            "DSC_5856_13_572.png\n",
            "DSC_5856_13_580.png\n",
            "DSC_5856_13_584.png\n",
            "DSC_5856_13_595.png\n",
            "DSC_5856_13_596.png\n",
            "DSC_5856_13_606.png\n",
            "DSC_5856_13_608.png\n",
            "DSC_5856_13_612.png\n",
            "DSC_5856_13_614.png\n",
            "DSC_5856_13_623.png\n",
            "DSC_5856_13_626.png\n",
            "DSC_5856_13_634.png\n",
            "DSC_5856_13_640.png\n",
            "DSC_5856_13_642.png\n",
            "DSC_5856_13_655.png\n",
            "DSC_5856_13_665.png\n",
            "DSC_5856_13_666.png\n",
            "DSC_5856_13_667.png\n",
            "DSC_5856_13_676.png\n",
            "DSC_5856_13_677.png\n",
            "DSC_5856_13_694.png\n",
            "DSC_5856_13_697.png\n",
            "DSC_5856_13_722.png\n",
            "DSC_5856_13_730.png\n",
            "DSC_5856_13_732.png\n",
            "DSC_5856_13_735.png\n",
            "DSC_5856_13_748.png\n",
            "DSC_5856_13_752.png\n",
            "DSC_5856_13_755.png\n",
            "DSC_5856_13_762.png\n",
            "DSC_5856_13_765.png\n",
            "DSC_5856_13_770.png\n",
            "DSC_5856_13_771.png\n",
            "DSC_5856_13_775.png\n",
            "DSC_5856_13_776.png\n",
            "DSC_5856_13_779.png\n",
            "DSC_5856_13_781.png\n",
            "DSC_5856_13_805.png\n",
            "DSC_5856_13_807.png\n",
            "DSC_5856_13_813.png\n",
            "DSC_5856_13_816.png\n",
            "DSC_5856_13_823.png\n",
            "DSC_5856_13_824.png\n",
            "DSC_5856_13_827.png\n",
            "DSC_5856_13_829.png\n",
            "DSC_5856_13_834.png\n",
            "DSC_5856_13_836.png\n",
            "DSC_5856_13_840.png\n",
            "DSC_5856_13_858.png\n",
            "DSC_5856_13_878.png\n",
            "DSC_5856_13_897.png\n",
            "DSC_5856_13_903.png\n",
            "DSC_5856_13_904.png\n",
            "DSC_5856_13_919.png\n",
            "DSC_5856_13_921.png\n",
            "DSC_5856_13_932.png\n",
            "DSC_5856_13_933.png\n",
            "DSC_5856_13_939.png\n",
            "DSC_5856_13_964.png\n",
            "DSC_5856_13_972.png\n",
            "DSC_5856_13_980.png\n",
            "DSC_5856_13_981.png\n",
            "DSC_5856_13_985.png\n",
            "DSC_5856_13_988.png\n",
            "DSC_5856_13_998.png\n",
            "DSC_5856_13_1001.png\n",
            "DSC_5856_13_1009.png\n",
            "DSC_5856_13_1010.png\n",
            "DSC_5856_13_1019.png\n",
            "DSC_5856_13_1021.png\n",
            "DSC_5856_13_1030.png\n",
            "DSC_5856_13_1031.png\n",
            "DSC_5856_13_1034.png\n",
            "DSC_5856_13_1036.png\n",
            "DSC_5856_13_1046.png\n",
            "DSC_5856_13_1051.png\n",
            "DSC_5856_13_1056.png\n",
            "DSC_5856_13_1063.png\n",
            "DSC_5856_13_1073.png\n",
            "DSC_5856_13_1075.png\n",
            "DSC_5856_13_1076.png\n",
            "DSC_5856_13_1078.png\n",
            "DSC_5856_13_1084.png\n",
            "DSC_5856_13_1087.png\n",
            "DSC_5856_13_1089.png\n",
            "DSC_5856_13_1098.png\n",
            "DSC_5856_13_1104.png\n",
            "DSC_5856_13_1108.png\n",
            "DSC_5856_13_1125.png\n",
            "DSC_5856_13_1131.png\n",
            "DSC_5856_13_1168.png\n",
            "DSC_5856_13_1171.png\n",
            "DSC_5856_13_1183.png\n",
            "DSC_5856_13_1216.png\n",
            "DSC_5856_13_1218.png\n",
            "DSC_5856_13_1235.png\n",
            "DSC_5856_13_1244.png\n",
            "DSC_5856_13_1256.png\n",
            "DSC_5856_13_1259.png\n",
            "DSC_5856_13_1260.png\n",
            "DSC_5856_13_1268.png\n",
            "DSC_5856_13_1270.png\n",
            "DSC_5856_13_1272.png\n",
            "DSC_5856_13_1280.png\n",
            "DSC_5856_13_1289.png\n",
            "DSC_5856_13_1292.png\n",
            "DSC_5856_13_1293.png\n",
            "DSC_5856_13_1294.png\n",
            "DSC_5856_13_1316.png\n",
            "DSC_5856_13_1322.png\n",
            "DSC_5856_13_1326.png\n",
            "DSC_5856_13_1329.png\n",
            "DSC_5856_13_1337.png\n",
            "DSC_5856_13_1345.png\n",
            "DSC_5856_13_1352.png\n",
            "DSC_5856_13_1368.png\n",
            "DSC_5856_13_1374.png\n",
            "DSC_5856_13_1377.png\n",
            "DSC_5856_13_1404.png\n",
            "DSC_5856_13_1425.png\n",
            "DSC_5856_13_1426.png\n",
            "DSC_5856_13_1427.png\n",
            "DSC_5856_13_1428.png\n",
            "DSC_5856_13_1432.png\n",
            "DSC_5856_13_1440.png\n",
            "DSC_5856_13_1443.png\n",
            "DSC_5856_13_1457.png\n",
            "DSC_5856_13_1458.png\n",
            "DSC_5856_13_1477.png\n",
            "DSC_5856_13_1482.png\n",
            "DSC_5856_13_1491.png\n",
            "DSC_5856_13_1493.png\n",
            "DSC_5856_21_506.png\n",
            "DSC_5856_21_515.png\n",
            "DSC_5856_21_519.png\n",
            "DSC_5856_21_530.png\n",
            "DSC_5856_21_536.png\n",
            "DSC_5856_21_538.png\n",
            "DSC_5856_21_553.png\n",
            "DSC_5856_21_554.png\n",
            "DSC_5856_21_564.png\n",
            "DSC_5856_21_568.png\n",
            "DSC_5856_21_569.png\n",
            "DSC_5856_21_577.png\n",
            "DSC_5856_21_578.png\n",
            "DSC_5856_21_582.png\n",
            "DSC_5856_21_598.png\n",
            "DSC_5856_21_605.png\n",
            "DSC_5856_21_609.png\n",
            "DSC_5856_21_613.png\n",
            "DSC_5856_21_620.png\n",
            "DSC_5856_21_623.png\n",
            "DSC_5856_21_626.png\n",
            "DSC_5856_21_631.png\n",
            "DSC_5856_21_639.png\n",
            "DSC_5856_21_652.png\n",
            "DSC_5856_21_669.png\n",
            "DSC_5856_21_676.png\n",
            "DSC_5856_21_688.png\n",
            "DSC_5856_21_702.png\n",
            "DSC_5856_21_710.png\n",
            "DSC_5856_21_711.png\n",
            "DSC_5856_21_721.png\n",
            "DSC_5856_21_739.png\n",
            "DSC_5856_21_753.png\n",
            "DSC_5856_21_765.png\n",
            "DSC_5856_21_772.png\n",
            "DSC_5856_21_787.png\n",
            "DSC_5856_21_795.png\n",
            "DSC_5856_21_796.png\n",
            "DSC_5856_21_805.png\n",
            "DSC_5856_21_841.png\n",
            "DSC_5856_21_846.png\n",
            "DSC_5856_21_869.png\n",
            "DSC_5856_21_917.png\n",
            "DSC_5856_21_920.png\n",
            "DSC_5856_21_933.png\n",
            "DSC_5856_21_942.png\n",
            "DSC_5856_21_955.png\n",
            "DSC_5856_21_961.png\n",
            "DSC_5856_21_962.png\n",
            "DSC_5856_21_969.png\n",
            "DSC_5856_21_974.png\n",
            "DSC_5856_21_987.png\n",
            "DSC_5856_21_994.png\n",
            "DSC_5856_21_1005.png\n",
            "DSC_5856_21_1016.png\n",
            "DSC_5856_21_1021.png\n",
            "DSC_5856_21_1026.png\n",
            "DSC_5856_21_1040.png\n",
            "DSC_5856_21_1044.png\n",
            "DSC_5856_21_1046.png\n",
            "DSC_5856_21_1102.png\n",
            "DSC_5856_21_1116.png\n",
            "DSC_5856_21_1124.png\n",
            "DSC_5856_21_1150.png\n",
            "DSC_5856_21_1152.png\n",
            "DSC_5856_21_1153.png\n",
            "DSC_5856_21_1154.png\n",
            "DSC_5856_21_1181.png\n",
            "DSC_5856_21_1183.png\n",
            "DSC_5856_21_1220.png\n",
            "DSC_5856_21_1230.png\n",
            "DSC_5856_21_1236.png\n",
            "DSC_5856_21_1245.png\n",
            "DSC_5856_21_1268.png\n",
            "DSC_5856_21_1278.png\n",
            "DSC_5856_21_1279.png\n",
            "DSC_5856_21_1282.png\n",
            "DSC_5856_21_1284.png\n",
            "DSC_5856_21_1290.png\n",
            "DSC_5856_21_1293.png\n",
            "DSC_5856_21_1294.png\n",
            "DSC_5856_21_1300.png\n",
            "DSC_5856_21_1311.png\n",
            "DSC_5856_21_1328.png\n",
            "DSC_5856_21_1342.png\n",
            "DSC_5856_21_1347.png\n",
            "DSC_5856_21_1349.png\n",
            "DSC_5856_21_1359.png\n",
            "DSC_5856_21_1360.png\n",
            "DSC_5856_21_1363.png\n",
            "DSC_5856_21_1367.png\n",
            "DSC_5856_21_1385.png\n",
            "DSC_5856_21_1397.png\n",
            "DSC_5856_21_1410.png\n",
            "DSC_5856_21_1423.png\n",
            "DSC_5856_21_1436.png\n",
            "DSC_5856_21_1440.png\n",
            "DSC_5856_21_1443.png\n",
            "DSC_5856_21_1456.png\n",
            "DSC_5856_21_1468.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bevigH8rytk3",
        "outputId": "9a917e61-8e8e-4fec-d5f2-dfa27ba84db9"
      },
      "source": [
        "data1=df[df['5']==4]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']\r\n",
        "a=data1.values\r\n",
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "for ln in data:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/4/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_1.png\n",
            "DSC_5856_12_7.png\n",
            "DSC_5856_12_14.png\n",
            "DSC_5856_12_19.png\n",
            "DSC_5856_12_25.png\n",
            "DSC_5856_12_29.png\n",
            "DSC_5856_12_42.png\n",
            "DSC_5856_12_51.png\n",
            "DSC_5856_12_63.png\n",
            "DSC_5856_12_85.png\n",
            "DSC_5856_12_102.png\n",
            "DSC_5856_12_109.png\n",
            "DSC_5856_12_110.png\n",
            "DSC_5856_12_114.png\n",
            "DSC_5856_12_128.png\n",
            "DSC_5856_12_134.png\n",
            "DSC_5856_12_140.png\n",
            "DSC_5856_12_148.png\n",
            "DSC_5856_12_152.png\n",
            "DSC_5856_12_161.png\n",
            "DSC_5856_12_173.png\n",
            "DSC_5856_12_180.png\n",
            "DSC_5856_12_187.png\n",
            "DSC_5856_12_189.png\n",
            "DSC_5856_12_194.png\n",
            "DSC_5856_12_196.png\n",
            "DSC_5856_12_201.png\n",
            "DSC_5856_12_202.png\n",
            "DSC_5856_12_211.png\n",
            "DSC_5856_12_214.png\n",
            "DSC_5856_12_217.png\n",
            "DSC_5856_12_219.png\n",
            "DSC_5856_12_228.png\n",
            "DSC_5856_12_232.png\n",
            "DSC_5856_12_233.png\n",
            "DSC_5856_12_236.png\n",
            "DSC_5856_12_241.png\n",
            "DSC_5856_12_250.png\n",
            "DSC_5856_12_251.png\n",
            "DSC_5856_12_257.png\n",
            "DSC_5856_12_258.png\n",
            "DSC_5856_12_261.png\n",
            "DSC_5856_12_262.png\n",
            "DSC_5856_12_277.png\n",
            "DSC_5856_12_282.png\n",
            "DSC_5856_12_283.png\n",
            "DSC_5856_12_287.png\n",
            "DSC_5856_12_291.png\n",
            "DSC_5856_12_294.png\n",
            "DSC_5856_12_297.png\n",
            "DSC_5856_12_299.png\n",
            "DSC_5856_12_303.png\n",
            "DSC_5856_12_305.png\n",
            "DSC_5856_12_328.png\n",
            "DSC_5856_12_342.png\n",
            "DSC_5856_12_345.png\n",
            "DSC_5856_12_346.png\n",
            "DSC_5856_12_347.png\n",
            "DSC_5856_12_350.png\n",
            "DSC_5856_12_351.png\n",
            "DSC_5856_12_357.png\n",
            "DSC_5856_12_361.png\n",
            "DSC_5856_12_381.png\n",
            "DSC_5856_12_384.png\n",
            "DSC_5856_12_410.png\n",
            "DSC_5856_12_415.png\n",
            "DSC_5856_12_421.png\n",
            "DSC_5856_12_424.png\n",
            "DSC_5856_12_435.png\n",
            "DSC_5856_12_437.png\n",
            "DSC_5856_12_441.png\n",
            "DSC_5856_12_445.png\n",
            "DSC_5856_12_448.png\n",
            "DSC_5856_12_455.png\n",
            "DSC_5856_12_457.png\n",
            "DSC_5856_12_465.png\n",
            "DSC_5856_12_466.png\n",
            "DSC_5856_12_470.png\n",
            "DSC_5856_12_471.png\n",
            "DSC_5856_12_478.png\n",
            "DSC_5856_12_481.png\n",
            "DSC_5856_12_489.png\n",
            "DSC_5856_12_490.png\n",
            "DSC_5856_12_496.png\n",
            "DSC_5856_12_497.png\n",
            "DSC_5856_13_9.png\n",
            "DSC_5856_13_12.png\n",
            "DSC_5856_13_15.png\n",
            "DSC_5856_13_16.png\n",
            "DSC_5856_13_17.png\n",
            "DSC_5856_13_22.png\n",
            "DSC_5856_13_24.png\n",
            "DSC_5856_13_26.png\n",
            "DSC_5856_13_73.png\n",
            "DSC_5856_13_75.png\n",
            "DSC_5856_13_76.png\n",
            "DSC_5856_13_81.png\n",
            "DSC_5856_13_87.png\n",
            "DSC_5856_13_90.png\n",
            "DSC_5856_13_93.png\n",
            "DSC_5856_13_94.png\n",
            "DSC_5856_13_95.png\n",
            "DSC_5856_13_107.png\n",
            "DSC_5856_13_109.png\n",
            "DSC_5856_13_116.png\n",
            "DSC_5856_13_127.png\n",
            "DSC_5856_13_128.png\n",
            "DSC_5856_13_135.png\n",
            "DSC_5856_13_136.png\n",
            "DSC_5856_13_147.png\n",
            "DSC_5856_13_149.png\n",
            "DSC_5856_13_159.png\n",
            "DSC_5856_13_166.png\n",
            "DSC_5856_13_168.png\n",
            "DSC_5856_13_169.png\n",
            "DSC_5856_13_171.png\n",
            "DSC_5856_13_178.png\n",
            "DSC_5856_13_180.png\n",
            "DSC_5856_13_184.png\n",
            "DSC_5856_13_186.png\n",
            "DSC_5856_13_197.png\n",
            "DSC_5856_13_199.png\n",
            "DSC_5856_13_200.png\n",
            "DSC_5856_13_203.png\n",
            "DSC_5856_13_217.png\n",
            "DSC_5856_13_219.png\n",
            "DSC_5856_13_233.png\n",
            "DSC_5856_13_234.png\n",
            "DSC_5856_13_244.png\n",
            "DSC_5856_13_258.png\n",
            "DSC_5856_13_274.png\n",
            "DSC_5856_13_277.png\n",
            "DSC_5856_13_286.png\n",
            "DSC_5856_13_295.png\n",
            "DSC_5856_13_296.png\n",
            "DSC_5856_13_302.png\n",
            "DSC_5856_13_308.png\n",
            "DSC_5856_13_317.png\n",
            "DSC_5856_13_318.png\n",
            "DSC_5856_13_342.png\n",
            "DSC_5856_13_347.png\n",
            "DSC_5856_13_358.png\n",
            "DSC_5856_13_359.png\n",
            "DSC_5856_13_361.png\n",
            "DSC_5856_13_371.png\n",
            "DSC_5856_13_387.png\n",
            "DSC_5856_13_388.png\n",
            "DSC_5856_13_389.png\n",
            "DSC_5856_13_397.png\n",
            "DSC_5856_13_402.png\n",
            "DSC_5856_13_406.png\n",
            "DSC_5856_13_415.png\n",
            "DSC_5856_13_419.png\n",
            "DSC_5856_13_420.png\n",
            "DSC_5856_13_433.png\n",
            "DSC_5856_13_436.png\n",
            "DSC_5856_13_447.png\n",
            "DSC_5856_13_451.png\n",
            "DSC_5856_13_455.png\n",
            "DSC_5856_13_456.png\n",
            "DSC_5856_13_467.png\n",
            "DSC_5856_13_470.png\n",
            "DSC_5856_13_471.png\n",
            "DSC_5856_13_474.png\n",
            "DSC_5856_13_477.png\n",
            "DSC_5856_13_482.png\n",
            "DSC_5856_13_483.png\n",
            "DSC_5856_13_485.png\n",
            "DSC_5856_13_486.png\n",
            "DSC_5856_13_487.png\n",
            "DSC_5856_21_6.png\n",
            "DSC_5856_21_26.png\n",
            "DSC_5856_21_59.png\n",
            "DSC_5856_21_64.png\n",
            "DSC_5856_21_79.png\n",
            "DSC_5856_21_81.png\n",
            "DSC_5856_21_84.png\n",
            "DSC_5856_21_93.png\n",
            "DSC_5856_21_94.png\n",
            "DSC_5856_21_112.png\n",
            "DSC_5856_21_113.png\n",
            "DSC_5856_21_116.png\n",
            "DSC_5856_21_123.png\n",
            "DSC_5856_21_128.png\n",
            "DSC_5856_21_130.png\n",
            "DSC_5856_21_146.png\n",
            "DSC_5856_21_147.png\n",
            "DSC_5856_21_150.png\n",
            "DSC_5856_21_154.png\n",
            "DSC_5856_21_156.png\n",
            "DSC_5856_21_161.png\n",
            "DSC_5856_21_182.png\n",
            "DSC_5856_21_187.png\n",
            "DSC_5856_21_191.png\n",
            "DSC_5856_21_193.png\n",
            "DSC_5856_21_204.png\n",
            "DSC_5856_21_226.png\n",
            "DSC_5856_21_231.png\n",
            "DSC_5856_21_237.png\n",
            "DSC_5856_21_261.png\n",
            "DSC_5856_21_266.png\n",
            "DSC_5856_21_301.png\n",
            "DSC_5856_21_309.png\n",
            "DSC_5856_21_314.png\n",
            "DSC_5856_21_320.png\n",
            "DSC_5856_21_321.png\n",
            "DSC_5856_21_345.png\n",
            "DSC_5856_21_356.png\n",
            "DSC_5856_21_394.png\n",
            "DSC_5856_21_405.png\n",
            "DSC_5856_21_407.png\n",
            "DSC_5856_21_429.png\n",
            "DSC_5856_21_459.png\n",
            "DSC_5856_25_66.png\n",
            "DSC_5856_25_76.png\n",
            "DSC_5856_25_91.png\n",
            "DSC_5856_25_122.png\n",
            "DSC_5856_25_136.png\n",
            "DSC_5856_25_153.png\n",
            "DSC_5856_25_166.png\n",
            "DSC_5856_25_172.png\n",
            "DSC_5856_25_227.png\n",
            "DSC_5856_25_266.png\n",
            "DSC_5856_25_338.png\n",
            "DSC_5856_25_348.png\n",
            "DSC_5856_25_356.png\n",
            "DSC_5856_25_391.png\n",
            "DSC_5856_25_430.png\n",
            "DSC_5856_25_479.png\n",
            "DSC_5856_11_2.png\n",
            "DSC_5856_11_9.png\n",
            "DSC_5856_11_21.png\n",
            "DSC_5856_11_37.png\n",
            "DSC_5856_11_38.png\n",
            "DSC_5856_11_46.png\n",
            "DSC_5856_11_48.png\n",
            "DSC_5856_11_60.png\n",
            "DSC_5856_11_84.png\n",
            "DSC_5856_11_94.png\n",
            "DSC_5856_11_95.png\n",
            "DSC_5856_11_130.png\n",
            "DSC_5856_11_149.png\n",
            "DSC_5856_11_157.png\n",
            "DSC_5856_11_171.png\n",
            "DSC_5856_11_174.png\n",
            "DSC_5856_11_190.png\n",
            "DSC_5856_11_214.png\n",
            "DSC_5856_11_230.png\n",
            "DSC_5856_11_235.png\n",
            "DSC_5856_11_263.png\n",
            "DSC_5856_11_269.png\n",
            "DSC_5856_11_270.png\n",
            "DSC_5856_11_286.png\n",
            "DSC_5856_11_290.png\n",
            "DSC_5856_11_293.png\n",
            "DSC_5856_11_336.png\n",
            "DSC_5856_11_385.png\n",
            "DSC_5856_11_398.png\n",
            "DSC_5856_11_408.png\n",
            "DSC_5856_11_421.png\n",
            "DSC_5856_11_428.png\n",
            "DSC_5856_11_431.png\n",
            "DSC_5856_11_438.png\n",
            "DSC_5856_11_444.png\n",
            "DSC_5856_11_448.png\n",
            "DSC_5856_11_453.png\n",
            "DSC_5856_11_454.png\n",
            "DSC_5856_11_457.png\n",
            "DSC_5856_11_472.png\n",
            "DSC_5856_11_483.png\n",
            "DSC_5856_2_2.png\n",
            "DSC_5856_2_4.png\n",
            "DSC_5856_2_7.png\n",
            "DSC_5856_2_20.png\n",
            "DSC_5856_2_26.png\n",
            "DSC_5856_2_36.png\n",
            "DSC_5856_2_37.png\n",
            "DSC_5856_2_39.png\n",
            "DSC_5856_2_40.png\n",
            "DSC_5856_2_45.png\n",
            "DSC_5856_2_48.png\n",
            "DSC_5856_2_67.png\n",
            "DSC_5856_2_87.png\n",
            "DSC_5856_2_94.png\n",
            "DSC_5856_2_111.png\n",
            "DSC_5856_2_115.png\n",
            "DSC_5856_2_125.png\n",
            "DSC_5856_2_131.png\n",
            "DSC_5856_2_144.png\n",
            "DSC_5856_2_156.png\n",
            "DSC_5856_2_171.png\n",
            "DSC_5856_2_190.png\n",
            "DSC_5856_2_198.png\n",
            "DSC_5856_2_243.png\n",
            "DSC_5856_2_251.png\n",
            "DSC_5856_2_296.png\n",
            "DSC_5856_2_308.png\n",
            "DSC_5856_2_344.png\n",
            "DSC_5856_2_364.png\n",
            "DSC_5856_2_380.png\n",
            "DSC_5856_2_388.png\n",
            "DSC_5856_2_406.png\n",
            "DSC_5856_2_423.png\n",
            "DSC_5856_2_427.png\n",
            "DSC_5856_2_451.png\n",
            "DSC_5856_2_490.png\n",
            "DSC_5856_31_318.png\n",
            "DSC_5856_31_379.png\n",
            "DSC_5856_31_471.png\n",
            "DSC_5856_31_480.png\n",
            "DSC_5856_52_52.png\n",
            "DSC_5856_52_59.png\n",
            "DSC_5856_52_64.png\n",
            "DSC_5856_52_75.png\n",
            "DSC_5856_52_86.png\n",
            "DSC_5856_52_90.png\n",
            "DSC_5856_52_115.png\n",
            "DSC_5856_52_127.png\n",
            "DSC_5856_52_141.png\n",
            "DSC_5856_52_142.png\n",
            "DSC_5856_52_178.png\n",
            "DSC_5856_52_237.png\n",
            "DSC_5856_52_242.png\n",
            "DSC_5856_52_254.png\n",
            "DSC_5856_52_276.png\n",
            "DSC_5856_52_317.png\n",
            "DSC_5856_52_323.png\n",
            "DSC_5856_52_325.png\n",
            "DSC_5856_52_346.png\n",
            "DSC_5856_52_390.png\n",
            "DSC_5856_52_406.png\n",
            "DSC_5856_52_427.png\n",
            "DSC_5856_52_431.png\n",
            "DSC_5856_52_453.png\n",
            "DSC_5856_52_459.png\n",
            "DSC_5856_52_493.png\n",
            "DSC_5856_1_2.png\n",
            "DSC_5856_1_12.png\n",
            "DSC_5856_1_16.png\n",
            "DSC_5856_1_18.png\n",
            "DSC_5856_1_32.png\n",
            "DSC_5856_1_35.png\n",
            "DSC_5856_1_40.png\n",
            "DSC_5856_1_43.png\n",
            "DSC_5856_1_50.png\n",
            "DSC_5856_1_79.png\n",
            "DSC_5856_1_88.png\n",
            "DSC_5856_1_89.png\n",
            "DSC_5856_1_97.png\n",
            "DSC_5856_1_107.png\n",
            "DSC_5856_1_111.png\n",
            "DSC_5856_1_114.png\n",
            "DSC_5856_1_117.png\n",
            "DSC_5856_1_119.png\n",
            "DSC_5856_1_125.png\n",
            "DSC_5856_1_127.png\n",
            "DSC_5856_1_144.png\n",
            "DSC_5856_1_157.png\n",
            "DSC_5856_1_165.png\n",
            "DSC_5856_1_178.png\n",
            "DSC_5856_1_190.png\n",
            "DSC_5856_1_201.png\n",
            "DSC_5856_1_207.png\n",
            "DSC_5856_1_211.png\n",
            "DSC_5856_1_227.png\n",
            "DSC_5856_1_231.png\n",
            "DSC_5856_1_237.png\n",
            "DSC_5856_1_252.png\n",
            "DSC_5856_1_261.png\n",
            "DSC_5856_1_276.png\n",
            "DSC_5856_1_277.png\n",
            "DSC_5856_1_282.png\n",
            "DSC_5856_1_286.png\n",
            "DSC_5856_1_312.png\n",
            "DSC_5856_1_316.png\n",
            "DSC_5856_1_324.png\n",
            "DSC_5856_1_332.png\n",
            "DSC_5856_1_334.png\n",
            "DSC_5856_1_349.png\n",
            "DSC_5856_1_356.png\n",
            "DSC_5856_1_359.png\n",
            "DSC_5856_1_380.png\n",
            "DSC_5856_1_385.png\n",
            "DSC_5856_1_399.png\n",
            "DSC_5856_1_412.png\n",
            "DSC_5856_1_438.png\n",
            "DSC_5856_1_440.png\n",
            "DSC_5856_1_443.png\n",
            "DSC_5856_1_449.png\n",
            "DSC_5856_1_451.png\n",
            "DSC_5856_1_482.png\n",
            "DSC_5856_1_490.png\n",
            "DSC_5856_1_501.png\n",
            "DSC_5856_1_504.png\n",
            "DSC_5856_1_512.png\n",
            "DSC_5856_1_515.png\n",
            "DSC_5856_1_516.png\n",
            "DSC_5856_1_517.png\n",
            "DSC_5856_1_527.png\n",
            "DSC_5856_1_539.png\n",
            "DSC_5856_1_543.png\n",
            "DSC_5856_1_548.png\n",
            "DSC_5856_1_569.png\n",
            "DSC_5856_1_577.png\n",
            "DSC_5856_1_584.png\n",
            "DSC_5856_1_610.png\n",
            "DSC_5856_1_613.png\n",
            "DSC_5856_1_616.png\n",
            "DSC_5856_1_624.png\n",
            "DSC_5856_1_627.png\n",
            "DSC_5856_1_629.png\n",
            "DSC_5856_1_634.png\n",
            "DSC_5856_1_635.png\n",
            "DSC_5856_1_667.png\n",
            "DSC_5856_1_696.png\n",
            "DSC_5856_1_697.png\n",
            "DSC_5856_1_705.png\n",
            "DSC_5856_1_710.png\n",
            "DSC_5856_1_724.png\n",
            "DSC_5856_1_735.png\n",
            "DSC_5856_1_740.png\n",
            "DSC_5856_1_743.png\n",
            "DSC_5856_1_761.png\n",
            "DSC_5856_1_762.png\n",
            "DSC_5856_1_770.png\n",
            "DSC_5856_1_783.png\n",
            "DSC_5856_1_788.png\n",
            "DSC_5856_1_795.png\n",
            "DSC_5856_1_817.png\n",
            "DSC_5856_1_818.png\n",
            "DSC_5856_1_821.png\n",
            "DSC_5856_1_824.png\n",
            "DSC_5856_1_829.png\n",
            "DSC_5856_1_830.png\n",
            "DSC_5856_1_837.png\n",
            "DSC_5856_1_848.png\n",
            "DSC_5856_1_858.png\n",
            "DSC_5856_1_862.png\n",
            "DSC_5856_1_864.png\n",
            "DSC_5856_1_882.png\n",
            "DSC_5856_1_883.png\n",
            "DSC_5856_1_891.png\n",
            "DSC_5856_1_892.png\n",
            "DSC_5856_1_893.png\n",
            "DSC_5856_1_895.png\n",
            "DSC_5856_1_907.png\n",
            "DSC_5856_1_910.png\n",
            "DSC_5856_1_920.png\n",
            "DSC_5856_1_925.png\n",
            "DSC_5856_1_930.png\n",
            "DSC_5856_1_933.png\n",
            "DSC_5856_1_935.png\n",
            "DSC_5856_1_943.png\n",
            "DSC_5856_1_944.png\n",
            "DSC_5856_1_961.png\n",
            "DSC_5856_1_972.png\n",
            "DSC_5856_1_982.png\n",
            "DSC_5856_1_986.png\n",
            "DSC_5856_1_987.png\n",
            "DSC_5856_1_991.png\n",
            "DSC_5856_1_992.png\n",
            "DSC_5856_1_998.png\n",
            "DSC_5856_1_999.png\n",
            "DSC_5856_14_5.png\n",
            "DSC_5856_14_6.png\n",
            "DSC_5856_14_7.png\n",
            "DSC_5856_14_12.png\n",
            "DSC_5856_14_14.png\n",
            "DSC_5856_14_19.png\n",
            "DSC_5856_14_26.png\n",
            "DSC_5856_14_30.png\n",
            "DSC_5856_14_46.png\n",
            "DSC_5856_14_48.png\n",
            "DSC_5856_14_51.png\n",
            "DSC_5856_14_59.png\n",
            "DSC_5856_14_66.png\n",
            "DSC_5856_14_68.png\n",
            "DSC_5856_14_96.png\n",
            "DSC_5856_14_104.png\n",
            "DSC_5856_14_107.png\n",
            "DSC_5856_14_119.png\n",
            "DSC_5856_14_125.png\n",
            "DSC_5856_14_127.png\n",
            "DSC_5856_14_128.png\n",
            "DSC_5856_14_132.png\n",
            "DSC_5856_14_134.png\n",
            "DSC_5856_14_137.png\n",
            "DSC_5856_14_143.png\n",
            "DSC_5856_14_145.png\n",
            "DSC_5856_14_161.png\n",
            "DSC_5856_14_165.png\n",
            "DSC_5856_14_173.png\n",
            "DSC_5856_14_178.png\n",
            "DSC_5856_14_181.png\n",
            "DSC_5856_14_185.png\n",
            "DSC_5856_14_192.png\n",
            "DSC_5856_14_195.png\n",
            "DSC_5856_14_201.png\n",
            "DSC_5856_14_204.png\n",
            "DSC_5856_14_213.png\n",
            "DSC_5856_14_217.png\n",
            "DSC_5856_14_225.png\n",
            "DSC_5856_14_226.png\n",
            "DSC_5856_14_227.png\n",
            "DSC_5856_14_236.png\n",
            "DSC_5856_14_256.png\n",
            "DSC_5856_14_259.png\n",
            "DSC_5856_14_261.png\n",
            "DSC_5856_14_264.png\n",
            "DSC_5856_14_268.png\n",
            "DSC_5856_14_272.png\n",
            "DSC_5856_14_274.png\n",
            "DSC_5856_14_278.png\n",
            "DSC_5856_14_280.png\n",
            "DSC_5856_14_281.png\n",
            "DSC_5856_14_284.png\n",
            "DSC_5856_14_285.png\n",
            "DSC_5856_14_286.png\n",
            "DSC_5856_14_297.png\n",
            "DSC_5856_14_321.png\n",
            "DSC_5856_14_323.png\n",
            "DSC_5856_14_329.png\n",
            "DSC_5856_14_342.png\n",
            "DSC_5856_14_343.png\n",
            "DSC_5856_14_349.png\n",
            "DSC_5856_14_360.png\n",
            "DSC_5856_14_361.png\n",
            "DSC_5856_14_363.png\n",
            "DSC_5856_14_364.png\n",
            "DSC_5856_14_365.png\n",
            "DSC_5856_14_371.png\n",
            "DSC_5856_14_378.png\n",
            "DSC_5856_14_382.png\n",
            "DSC_5856_14_389.png\n",
            "DSC_5856_14_394.png\n",
            "DSC_5856_14_397.png\n",
            "DSC_5856_14_401.png\n",
            "DSC_5856_14_404.png\n",
            "DSC_5856_14_409.png\n",
            "DSC_5856_14_415.png\n",
            "DSC_5856_14_424.png\n",
            "DSC_5856_14_433.png\n",
            "DSC_5856_14_440.png\n",
            "DSC_5856_14_445.png\n",
            "DSC_5856_14_451.png\n",
            "DSC_5856_14_454.png\n",
            "DSC_5856_14_469.png\n",
            "DSC_5856_14_470.png\n",
            "DSC_5856_14_479.png\n",
            "DSC_5856_14_490.png\n",
            "DSC_5856_14_491.png\n",
            "DSC_5856_14_498.png\n",
            "DSC_5856_14_504.png\n",
            "DSC_5856_14_505.png\n",
            "DSC_5856_14_508.png\n",
            "DSC_5856_14_517.png\n",
            "DSC_5856_14_519.png\n",
            "DSC_5856_14_522.png\n",
            "DSC_5856_14_524.png\n",
            "DSC_5856_14_525.png\n",
            "DSC_5856_14_530.png\n",
            "DSC_5856_14_533.png\n",
            "DSC_5856_14_547.png\n",
            "DSC_5856_14_553.png\n",
            "DSC_5856_14_569.png\n",
            "DSC_5856_14_578.png\n",
            "DSC_5856_14_580.png\n",
            "DSC_5856_14_587.png\n",
            "DSC_5856_14_607.png\n",
            "DSC_5856_14_610.png\n",
            "DSC_5856_14_619.png\n",
            "DSC_5856_14_632.png\n",
            "DSC_5856_14_634.png\n",
            "DSC_5856_14_636.png\n",
            "DSC_5856_14_649.png\n",
            "DSC_5856_14_650.png\n",
            "DSC_5856_14_656.png\n",
            "DSC_5856_14_671.png\n",
            "DSC_5856_14_675.png\n",
            "DSC_5856_14_676.png\n",
            "DSC_5856_14_678.png\n",
            "DSC_5856_14_681.png\n",
            "DSC_5856_14_690.png\n",
            "DSC_5856_14_704.png\n",
            "DSC_5856_14_708.png\n",
            "DSC_5856_14_722.png\n",
            "DSC_5856_14_723.png\n",
            "DSC_5856_14_728.png\n",
            "DSC_5856_14_732.png\n",
            "DSC_5856_14_740.png\n",
            "DSC_5856_14_751.png\n",
            "DSC_5856_14_769.png\n",
            "DSC_5856_14_783.png\n",
            "DSC_5856_14_786.png\n",
            "DSC_5856_14_797.png\n",
            "DSC_5856_14_803.png\n",
            "DSC_5856_14_805.png\n",
            "DSC_5856_14_806.png\n",
            "DSC_5856_14_807.png\n",
            "DSC_5856_14_814.png\n",
            "DSC_5856_14_836.png\n",
            "DSC_5856_14_837.png\n",
            "DSC_5856_14_839.png\n",
            "DSC_5856_14_847.png\n",
            "DSC_5856_14_855.png\n",
            "DSC_5856_14_863.png\n",
            "DSC_5856_14_872.png\n",
            "DSC_5856_14_874.png\n",
            "DSC_5856_14_879.png\n",
            "DSC_5856_14_888.png\n",
            "DSC_5856_14_890.png\n",
            "DSC_5856_14_906.png\n",
            "DSC_5856_14_907.png\n",
            "DSC_5856_14_909.png\n",
            "DSC_5856_14_929.png\n",
            "DSC_5856_14_937.png\n",
            "DSC_5856_14_940.png\n",
            "DSC_5856_14_943.png\n",
            "DSC_5856_14_951.png\n",
            "DSC_5856_14_956.png\n",
            "DSC_5856_14_960.png\n",
            "DSC_5856_14_961.png\n",
            "DSC_5856_14_967.png\n",
            "DSC_5856_14_976.png\n",
            "DSC_5856_14_979.png\n",
            "DSC_5856_14_989.png\n",
            "DSC_5856_14_998.png\n",
            "DSC_5856_14_999.png\n",
            "DSC_5856_15_12.png\n",
            "DSC_5856_12_505.png\n",
            "DSC_5856_12_521.png\n",
            "DSC_5856_12_532.png\n",
            "DSC_5856_12_537.png\n",
            "DSC_5856_12_539.png\n",
            "DSC_5856_12_545.png\n",
            "DSC_5856_12_549.png\n",
            "DSC_5856_12_550.png\n",
            "DSC_5856_12_552.png\n",
            "DSC_5856_12_553.png\n",
            "DSC_5856_12_554.png\n",
            "DSC_5856_12_557.png\n",
            "DSC_5856_12_562.png\n",
            "DSC_5856_12_563.png\n",
            "DSC_5856_12_566.png\n",
            "DSC_5856_12_571.png\n",
            "DSC_5856_12_578.png\n",
            "DSC_5856_12_586.png\n",
            "DSC_5856_12_596.png\n",
            "DSC_5856_12_598.png\n",
            "DSC_5856_12_601.png\n",
            "DSC_5856_12_605.png\n",
            "DSC_5856_12_606.png\n",
            "DSC_5856_12_609.png\n",
            "DSC_5856_12_615.png\n",
            "DSC_5856_12_617.png\n",
            "DSC_5856_12_629.png\n",
            "DSC_5856_12_638.png\n",
            "DSC_5856_12_639.png\n",
            "DSC_5856_12_641.png\n",
            "DSC_5856_12_652.png\n",
            "DSC_5856_12_655.png\n",
            "DSC_5856_12_665.png\n",
            "DSC_5856_12_668.png\n",
            "DSC_5856_12_669.png\n",
            "DSC_5856_12_681.png\n",
            "DSC_5856_12_693.png\n",
            "DSC_5856_12_697.png\n",
            "DSC_5856_12_700.png\n",
            "DSC_5856_12_714.png\n",
            "DSC_5856_12_715.png\n",
            "DSC_5856_12_728.png\n",
            "DSC_5856_12_729.png\n",
            "DSC_5856_12_730.png\n",
            "DSC_5856_12_734.png\n",
            "DSC_5856_12_738.png\n",
            "DSC_5856_12_740.png\n",
            "DSC_5856_12_747.png\n",
            "DSC_5856_12_749.png\n",
            "DSC_5856_12_757.png\n",
            "DSC_5856_12_759.png\n",
            "DSC_5856_12_773.png\n",
            "DSC_5856_12_778.png\n",
            "DSC_5856_12_782.png\n",
            "DSC_5856_12_787.png\n",
            "DSC_5856_12_788.png\n",
            "DSC_5856_12_791.png\n",
            "DSC_5856_12_795.png\n",
            "DSC_5856_12_802.png\n",
            "DSC_5856_12_803.png\n",
            "DSC_5856_12_805.png\n",
            "DSC_5856_12_813.png\n",
            "DSC_5856_12_814.png\n",
            "DSC_5856_12_821.png\n",
            "DSC_5856_12_822.png\n",
            "DSC_5856_12_832.png\n",
            "DSC_5856_12_839.png\n",
            "DSC_5856_12_841.png\n",
            "DSC_5856_12_846.png\n",
            "DSC_5856_12_858.png\n",
            "DSC_5856_12_859.png\n",
            "DSC_5856_12_882.png\n",
            "DSC_5856_12_884.png\n",
            "DSC_5856_12_886.png\n",
            "DSC_5856_12_894.png\n",
            "DSC_5856_12_904.png\n",
            "DSC_5856_12_905.png\n",
            "DSC_5856_12_931.png\n",
            "DSC_5856_12_936.png\n",
            "DSC_5856_12_943.png\n",
            "DSC_5856_12_951.png\n",
            "DSC_5856_12_956.png\n",
            "DSC_5856_12_959.png\n",
            "DSC_5856_12_967.png\n",
            "DSC_5856_12_972.png\n",
            "DSC_5856_12_973.png\n",
            "DSC_5856_12_981.png\n",
            "DSC_5856_12_994.png\n",
            "DSC_5856_12_1000.png\n",
            "DSC_5856_12_1003.png\n",
            "DSC_5856_12_1004.png\n",
            "DSC_5856_12_1006.png\n",
            "DSC_5856_12_1022.png\n",
            "DSC_5856_12_1032.png\n",
            "DSC_5856_12_1034.png\n",
            "DSC_5856_12_1036.png\n",
            "DSC_5856_12_1051.png\n",
            "DSC_5856_12_1060.png\n",
            "DSC_5856_12_1062.png\n",
            "DSC_5856_12_1066.png\n",
            "DSC_5856_12_1067.png\n",
            "DSC_5856_12_1074.png\n",
            "DSC_5856_12_1092.png\n",
            "DSC_5856_12_1105.png\n",
            "DSC_5856_12_1116.png\n",
            "DSC_5856_12_1120.png\n",
            "DSC_5856_12_1121.png\n",
            "DSC_5856_12_1126.png\n",
            "DSC_5856_12_1127.png\n",
            "DSC_5856_12_1148.png\n",
            "DSC_5856_12_1149.png\n",
            "DSC_5856_12_1151.png\n",
            "DSC_5856_12_1156.png\n",
            "DSC_5856_12_1157.png\n",
            "DSC_5856_12_1171.png\n",
            "DSC_5856_12_1178.png\n",
            "DSC_5856_12_1179.png\n",
            "DSC_5856_12_1181.png\n",
            "DSC_5856_12_1188.png\n",
            "DSC_5856_12_1225.png\n",
            "DSC_5856_12_1228.png\n",
            "DSC_5856_12_1246.png\n",
            "DSC_5856_12_1257.png\n",
            "DSC_5856_12_1276.png\n",
            "DSC_5856_12_1279.png\n",
            "DSC_5856_12_1281.png\n",
            "DSC_5856_12_1285.png\n",
            "DSC_5856_12_1286.png\n",
            "DSC_5856_12_1291.png\n",
            "DSC_5856_12_1299.png\n",
            "DSC_5856_12_1306.png\n",
            "DSC_5856_12_1309.png\n",
            "DSC_5856_12_1317.png\n",
            "DSC_5856_12_1320.png\n",
            "DSC_5856_12_1324.png\n",
            "DSC_5856_12_1325.png\n",
            "DSC_5856_12_1327.png\n",
            "DSC_5856_12_1328.png\n",
            "DSC_5856_12_1331.png\n",
            "DSC_5856_12_1344.png\n",
            "DSC_5856_12_1355.png\n",
            "DSC_5856_12_1356.png\n",
            "DSC_5856_12_1358.png\n",
            "DSC_5856_12_1359.png\n",
            "DSC_5856_12_1366.png\n",
            "DSC_5856_12_1373.png\n",
            "DSC_5856_12_1378.png\n",
            "DSC_5856_12_1379.png\n",
            "DSC_5856_12_1383.png\n",
            "DSC_5856_12_1390.png\n",
            "DSC_5856_12_1394.png\n",
            "DSC_5856_12_1396.png\n",
            "DSC_5856_12_1402.png\n",
            "DSC_5856_12_1410.png\n",
            "DSC_5856_12_1421.png\n",
            "DSC_5856_12_1423.png\n",
            "DSC_5856_12_1435.png\n",
            "DSC_5856_12_1454.png\n",
            "DSC_5856_12_1460.png\n",
            "DSC_5856_12_1468.png\n",
            "DSC_5856_12_1474.png\n",
            "DSC_5856_12_1476.png\n",
            "DSC_5856_12_1479.png\n",
            "DSC_5856_12_1497.png\n",
            "DSC_5856_12_1498.png\n",
            "DSC_5856_13_501.png\n",
            "DSC_5856_13_502.png\n",
            "DSC_5856_13_504.png\n",
            "DSC_5856_13_505.png\n",
            "DSC_5856_13_510.png\n",
            "DSC_5856_13_511.png\n",
            "DSC_5856_13_518.png\n",
            "DSC_5856_13_536.png\n",
            "DSC_5856_13_547.png\n",
            "DSC_5856_13_557.png\n",
            "DSC_5856_13_562.png\n",
            "DSC_5856_13_564.png\n",
            "DSC_5856_13_565.png\n",
            "DSC_5856_13_568.png\n",
            "DSC_5856_13_569.png\n",
            "DSC_5856_13_577.png\n",
            "DSC_5856_13_578.png\n",
            "DSC_5856_13_579.png\n",
            "DSC_5856_13_586.png\n",
            "DSC_5856_13_591.png\n",
            "DSC_5856_13_594.png\n",
            "DSC_5856_13_597.png\n",
            "DSC_5856_13_619.png\n",
            "DSC_5856_13_621.png\n",
            "DSC_5856_13_622.png\n",
            "DSC_5856_13_624.png\n",
            "DSC_5856_13_630.png\n",
            "DSC_5856_13_632.png\n",
            "DSC_5856_13_633.png\n",
            "DSC_5856_13_646.png\n",
            "DSC_5856_13_650.png\n",
            "DSC_5856_13_658.png\n",
            "DSC_5856_13_659.png\n",
            "DSC_5856_13_664.png\n",
            "DSC_5856_13_695.png\n",
            "DSC_5856_13_709.png\n",
            "DSC_5856_13_711.png\n",
            "DSC_5856_13_716.png\n",
            "DSC_5856_13_724.png\n",
            "DSC_5856_13_727.png\n",
            "DSC_5856_13_738.png\n",
            "DSC_5856_13_742.png\n",
            "DSC_5856_13_747.png\n",
            "DSC_5856_13_749.png\n",
            "DSC_5856_13_758.png\n",
            "DSC_5856_13_764.png\n",
            "DSC_5856_13_780.png\n",
            "DSC_5856_13_796.png\n",
            "DSC_5856_13_804.png\n",
            "DSC_5856_13_808.png\n",
            "DSC_5856_13_814.png\n",
            "DSC_5856_13_817.png\n",
            "DSC_5856_13_818.png\n",
            "DSC_5856_13_825.png\n",
            "DSC_5856_13_835.png\n",
            "DSC_5856_13_843.png\n",
            "DSC_5856_13_847.png\n",
            "DSC_5856_13_848.png\n",
            "DSC_5856_13_872.png\n",
            "DSC_5856_13_873.png\n",
            "DSC_5856_13_882.png\n",
            "DSC_5856_13_888.png\n",
            "DSC_5856_13_891.png\n",
            "DSC_5856_13_900.png\n",
            "DSC_5856_13_917.png\n",
            "DSC_5856_13_922.png\n",
            "DSC_5856_13_925.png\n",
            "DSC_5856_13_929.png\n",
            "DSC_5856_13_930.png\n",
            "DSC_5856_13_934.png\n",
            "DSC_5856_13_935.png\n",
            "DSC_5856_13_943.png\n",
            "DSC_5856_13_949.png\n",
            "DSC_5856_13_951.png\n",
            "DSC_5856_13_953.png\n",
            "DSC_5856_13_967.png\n",
            "DSC_5856_13_975.png\n",
            "DSC_5856_13_979.png\n",
            "DSC_5856_13_982.png\n",
            "DSC_5856_13_984.png\n",
            "DSC_5856_13_989.png\n",
            "DSC_5856_13_993.png\n",
            "DSC_5856_13_1003.png\n",
            "DSC_5856_13_1013.png\n",
            "DSC_5856_13_1014.png\n",
            "DSC_5856_13_1020.png\n",
            "DSC_5856_13_1024.png\n",
            "DSC_5856_13_1040.png\n",
            "DSC_5856_13_1041.png\n",
            "DSC_5856_13_1045.png\n",
            "DSC_5856_13_1047.png\n",
            "DSC_5856_13_1062.png\n",
            "DSC_5856_13_1068.png\n",
            "DSC_5856_13_1072.png\n",
            "DSC_5856_13_1074.png\n",
            "DSC_5856_13_1091.png\n",
            "DSC_5856_13_1102.png\n",
            "DSC_5856_13_1122.png\n",
            "DSC_5856_13_1127.png\n",
            "DSC_5856_13_1128.png\n",
            "DSC_5856_13_1130.png\n",
            "DSC_5856_13_1139.png\n",
            "DSC_5856_13_1141.png\n",
            "DSC_5856_13_1143.png\n",
            "DSC_5856_13_1157.png\n",
            "DSC_5856_13_1162.png\n",
            "DSC_5856_13_1170.png\n",
            "DSC_5856_13_1176.png\n",
            "DSC_5856_13_1189.png\n",
            "DSC_5856_13_1191.png\n",
            "DSC_5856_13_1209.png\n",
            "DSC_5856_13_1210.png\n",
            "DSC_5856_13_1215.png\n",
            "DSC_5856_13_1217.png\n",
            "DSC_5856_13_1224.png\n",
            "DSC_5856_13_1227.png\n",
            "DSC_5856_13_1229.png\n",
            "DSC_5856_13_1233.png\n",
            "DSC_5856_13_1237.png\n",
            "DSC_5856_13_1245.png\n",
            "DSC_5856_13_1248.png\n",
            "DSC_5856_13_1251.png\n",
            "DSC_5856_13_1252.png\n",
            "DSC_5856_13_1253.png\n",
            "DSC_5856_13_1255.png\n",
            "DSC_5856_13_1258.png\n",
            "DSC_5856_13_1262.png\n",
            "DSC_5856_13_1264.png\n",
            "DSC_5856_13_1266.png\n",
            "DSC_5856_13_1273.png\n",
            "DSC_5856_13_1283.png\n",
            "DSC_5856_13_1286.png\n",
            "DSC_5856_13_1300.png\n",
            "DSC_5856_13_1302.png\n",
            "DSC_5856_13_1304.png\n",
            "DSC_5856_13_1305.png\n",
            "DSC_5856_13_1310.png\n",
            "DSC_5856_13_1312.png\n",
            "DSC_5856_13_1330.png\n",
            "DSC_5856_13_1347.png\n",
            "DSC_5856_13_1356.png\n",
            "DSC_5856_13_1358.png\n",
            "DSC_5856_13_1361.png\n",
            "DSC_5856_13_1362.png\n",
            "DSC_5856_13_1363.png\n",
            "DSC_5856_13_1370.png\n",
            "DSC_5856_13_1375.png\n",
            "DSC_5856_13_1387.png\n",
            "DSC_5856_13_1389.png\n",
            "DSC_5856_13_1391.png\n",
            "DSC_5856_13_1392.png\n",
            "DSC_5856_13_1393.png\n",
            "DSC_5856_13_1397.png\n",
            "DSC_5856_13_1403.png\n",
            "DSC_5856_13_1405.png\n",
            "DSC_5856_13_1408.png\n",
            "DSC_5856_13_1410.png\n",
            "DSC_5856_13_1413.png\n",
            "DSC_5856_13_1415.png\n",
            "DSC_5856_13_1416.png\n",
            "DSC_5856_13_1430.png\n",
            "DSC_5856_13_1444.png\n",
            "DSC_5856_13_1445.png\n",
            "DSC_5856_13_1446.png\n",
            "DSC_5856_13_1452.png\n",
            "DSC_5856_13_1462.png\n",
            "DSC_5856_13_1463.png\n",
            "DSC_5856_13_1464.png\n",
            "DSC_5856_13_1465.png\n",
            "DSC_5856_13_1466.png\n",
            "DSC_5856_13_1469.png\n",
            "DSC_5856_13_1472.png\n",
            "DSC_5856_13_1475.png\n",
            "DSC_5856_13_1476.png\n",
            "DSC_5856_13_1484.png\n",
            "DSC_5856_13_1487.png\n",
            "DSC_5856_13_1488.png\n",
            "DSC_5856_13_1489.png\n",
            "DSC_5856_13_1497.png\n",
            "DSC_5856_13_1498.png\n",
            "DSC_5856_21_520.png\n",
            "DSC_5856_21_523.png\n",
            "DSC_5856_21_544.png\n",
            "DSC_5856_21_547.png\n",
            "DSC_5856_21_552.png\n",
            "DSC_5856_21_575.png\n",
            "DSC_5856_21_615.png\n",
            "DSC_5856_21_634.png\n",
            "DSC_5856_21_644.png\n",
            "DSC_5856_21_666.png\n",
            "DSC_5856_21_668.png\n",
            "DSC_5856_21_670.png\n",
            "DSC_5856_21_682.png\n",
            "DSC_5856_21_696.png\n",
            "DSC_5856_21_698.png\n",
            "DSC_5856_21_703.png\n",
            "DSC_5856_21_706.png\n",
            "DSC_5856_21_720.png\n",
            "DSC_5856_21_726.png\n",
            "DSC_5856_21_732.png\n",
            "DSC_5856_21_738.png\n",
            "DSC_5856_21_758.png\n",
            "DSC_5856_21_759.png\n",
            "DSC_5856_21_767.png\n",
            "DSC_5856_21_780.png\n",
            "DSC_5856_21_781.png\n",
            "DSC_5856_21_800.png\n",
            "DSC_5856_21_828.png\n",
            "DSC_5856_21_842.png\n",
            "DSC_5856_21_860.png\n",
            "DSC_5856_21_868.png\n",
            "DSC_5856_21_870.png\n",
            "DSC_5856_21_874.png\n",
            "DSC_5856_21_876.png\n",
            "DSC_5856_21_901.png\n",
            "DSC_5856_21_914.png\n",
            "DSC_5856_21_916.png\n",
            "DSC_5856_21_932.png\n",
            "DSC_5856_21_937.png\n",
            "DSC_5856_21_940.png\n",
            "DSC_5856_21_945.png\n",
            "DSC_5856_21_958.png\n",
            "DSC_5856_21_964.png\n",
            "DSC_5856_21_973.png\n",
            "DSC_5856_21_977.png\n",
            "DSC_5856_21_996.png\n",
            "DSC_5856_21_1010.png\n",
            "DSC_5856_21_1028.png\n",
            "DSC_5856_21_1029.png\n",
            "DSC_5856_21_1030.png\n",
            "DSC_5856_21_1031.png\n",
            "DSC_5856_21_1049.png\n",
            "DSC_5856_21_1056.png\n",
            "DSC_5856_21_1072.png\n",
            "DSC_5856_21_1080.png\n",
            "DSC_5856_21_1115.png\n",
            "DSC_5856_21_1126.png\n",
            "DSC_5856_21_1128.png\n",
            "DSC_5856_21_1132.png\n",
            "DSC_5856_21_1143.png\n",
            "DSC_5856_21_1177.png\n",
            "DSC_5856_21_1179.png\n",
            "DSC_5856_21_1182.png\n",
            "DSC_5856_21_1192.png\n",
            "DSC_5856_21_1198.png\n",
            "DSC_5856_21_1216.png\n",
            "DSC_5856_21_1226.png\n",
            "DSC_5856_21_1229.png\n",
            "DSC_5856_21_1232.png\n",
            "DSC_5856_21_1238.png\n",
            "DSC_5856_21_1240.png\n",
            "DSC_5856_21_1260.png\n",
            "DSC_5856_21_1262.png\n",
            "DSC_5856_21_1267.png\n",
            "DSC_5856_21_1275.png\n",
            "DSC_5856_21_1280.png\n",
            "DSC_5856_21_1289.png\n",
            "DSC_5856_21_1296.png\n",
            "DSC_5856_21_1297.png\n",
            "DSC_5856_21_1304.png\n",
            "DSC_5856_21_1307.png\n",
            "DSC_5856_21_1313.png\n",
            "DSC_5856_21_1343.png\n",
            "DSC_5856_21_1346.png\n",
            "DSC_5856_21_1364.png\n",
            "DSC_5856_21_1371.png\n",
            "DSC_5856_21_1379.png\n",
            "DSC_5856_21_1387.png\n",
            "DSC_5856_21_1398.png\n",
            "DSC_5856_21_1404.png\n",
            "DSC_5856_21_1409.png\n",
            "DSC_5856_21_1420.png\n",
            "DSC_5856_21_1421.png\n",
            "DSC_5856_21_1428.png\n",
            "DSC_5856_21_1429.png\n",
            "DSC_5856_21_1437.png\n",
            "DSC_5856_21_1441.png\n",
            "DSC_5856_21_1449.png\n",
            "DSC_5856_21_1459.png\n",
            "DSC_5856_21_1467.png\n",
            "DSC_5856_21_1472.png\n",
            "DSC_5856_21_1474.png\n",
            "DSC_5856_21_1482.png\n",
            "DSC_5856_21_1486.png\n",
            "DSC_5856_21_1487.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQcVhzHM0Jxu",
        "outputId": "a8706591-6737-47d1-f2e7-c79245f2400e"
      },
      "source": [
        "data1=df[df['5']==5]['E:/smallcut/data\\smallImages\\images\\DSC_5856_12\\DSC_5856_12_0.png']\r\n",
        "a=data1.values\r\n",
        "import numpy as np\r\n",
        "shape = len(a)\r\n",
        "data = [a[i][a[i].rindex('\\\\')+1:len(a[i])] for i in range(shape)]\r\n",
        "for ln in data:\r\n",
        "\t\timg_file = ln.rstrip('\\n')\r\n",
        "\t\tprint(img_file)\r\n",
        "\t\tsrc_path = 'data/' + img_file\r\n",
        "\t\tdst_path = 'data/5/' + img_file\r\n",
        "\t\tshutil.copy(src_path, dst_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DSC_5856_12_3.png\n",
            "DSC_5856_12_4.png\n",
            "DSC_5856_12_5.png\n",
            "DSC_5856_12_8.png\n",
            "DSC_5856_12_9.png\n",
            "DSC_5856_12_13.png\n",
            "DSC_5856_12_15.png\n",
            "DSC_5856_12_16.png\n",
            "DSC_5856_12_17.png\n",
            "DSC_5856_12_18.png\n",
            "DSC_5856_12_20.png\n",
            "DSC_5856_12_22.png\n",
            "DSC_5856_12_23.png\n",
            "DSC_5856_12_24.png\n",
            "DSC_5856_12_26.png\n",
            "DSC_5856_12_27.png\n",
            "DSC_5856_12_31.png\n",
            "DSC_5856_12_33.png\n",
            "DSC_5856_12_34.png\n",
            "DSC_5856_12_35.png\n",
            "DSC_5856_12_37.png\n",
            "DSC_5856_12_38.png\n",
            "DSC_5856_12_39.png\n",
            "DSC_5856_12_43.png\n",
            "DSC_5856_12_44.png\n",
            "DSC_5856_12_45.png\n",
            "DSC_5856_12_47.png\n",
            "DSC_5856_12_54.png\n",
            "DSC_5856_12_55.png\n",
            "DSC_5856_12_56.png\n",
            "DSC_5856_12_58.png\n",
            "DSC_5856_12_59.png\n",
            "DSC_5856_12_61.png\n",
            "DSC_5856_12_62.png\n",
            "DSC_5856_12_64.png\n",
            "DSC_5856_12_67.png\n",
            "DSC_5856_12_69.png\n",
            "DSC_5856_12_70.png\n",
            "DSC_5856_12_71.png\n",
            "DSC_5856_12_72.png\n",
            "DSC_5856_12_73.png\n",
            "DSC_5856_12_74.png\n",
            "DSC_5856_12_75.png\n",
            "DSC_5856_12_76.png\n",
            "DSC_5856_12_77.png\n",
            "DSC_5856_12_78.png\n",
            "DSC_5856_12_80.png\n",
            "DSC_5856_12_82.png\n",
            "DSC_5856_12_83.png\n",
            "DSC_5856_12_84.png\n",
            "DSC_5856_12_87.png\n",
            "DSC_5856_12_88.png\n",
            "DSC_5856_12_91.png\n",
            "DSC_5856_12_92.png\n",
            "DSC_5856_12_96.png\n",
            "DSC_5856_12_97.png\n",
            "DSC_5856_12_99.png\n",
            "DSC_5856_12_101.png\n",
            "DSC_5856_12_112.png\n",
            "DSC_5856_12_117.png\n",
            "DSC_5856_12_118.png\n",
            "DSC_5856_12_120.png\n",
            "DSC_5856_12_131.png\n",
            "DSC_5856_12_137.png\n",
            "DSC_5856_12_138.png\n",
            "DSC_5856_12_139.png\n",
            "DSC_5856_12_141.png\n",
            "DSC_5856_12_142.png\n",
            "DSC_5856_12_144.png\n",
            "DSC_5856_12_146.png\n",
            "DSC_5856_12_147.png\n",
            "DSC_5856_12_149.png\n",
            "DSC_5856_12_150.png\n",
            "DSC_5856_12_151.png\n",
            "DSC_5856_12_153.png\n",
            "DSC_5856_12_154.png\n",
            "DSC_5856_12_155.png\n",
            "DSC_5856_12_156.png\n",
            "DSC_5856_12_159.png\n",
            "DSC_5856_12_162.png\n",
            "DSC_5856_12_163.png\n",
            "DSC_5856_12_166.png\n",
            "DSC_5856_12_167.png\n",
            "DSC_5856_12_169.png\n",
            "DSC_5856_12_171.png\n",
            "DSC_5856_12_174.png\n",
            "DSC_5856_12_175.png\n",
            "DSC_5856_12_177.png\n",
            "DSC_5856_12_179.png\n",
            "DSC_5856_12_183.png\n",
            "DSC_5856_12_184.png\n",
            "DSC_5856_12_185.png\n",
            "DSC_5856_12_186.png\n",
            "DSC_5856_12_188.png\n",
            "DSC_5856_12_195.png\n",
            "DSC_5856_12_197.png\n",
            "DSC_5856_12_198.png\n",
            "DSC_5856_12_199.png\n",
            "DSC_5856_12_200.png\n",
            "DSC_5856_12_203.png\n",
            "DSC_5856_12_205.png\n",
            "DSC_5856_12_212.png\n",
            "DSC_5856_12_213.png\n",
            "DSC_5856_12_215.png\n",
            "DSC_5856_12_216.png\n",
            "DSC_5856_12_221.png\n",
            "DSC_5856_12_222.png\n",
            "DSC_5856_12_223.png\n",
            "DSC_5856_12_225.png\n",
            "DSC_5856_12_227.png\n",
            "DSC_5856_12_230.png\n",
            "DSC_5856_12_235.png\n",
            "DSC_5856_12_237.png\n",
            "DSC_5856_12_240.png\n",
            "DSC_5856_12_243.png\n",
            "DSC_5856_12_245.png\n",
            "DSC_5856_12_246.png\n",
            "DSC_5856_12_247.png\n",
            "DSC_5856_12_248.png\n",
            "DSC_5856_12_249.png\n",
            "DSC_5856_12_252.png\n",
            "DSC_5856_12_253.png\n",
            "DSC_5856_12_254.png\n",
            "DSC_5856_12_256.png\n",
            "DSC_5856_12_263.png\n",
            "DSC_5856_12_265.png\n",
            "DSC_5856_12_267.png\n",
            "DSC_5856_12_270.png\n",
            "DSC_5856_12_272.png\n",
            "DSC_5856_12_274.png\n",
            "DSC_5856_12_275.png\n",
            "DSC_5856_12_276.png\n",
            "DSC_5856_12_278.png\n",
            "DSC_5856_12_284.png\n",
            "DSC_5856_12_288.png\n",
            "DSC_5856_12_292.png\n",
            "DSC_5856_12_295.png\n",
            "DSC_5856_12_296.png\n",
            "DSC_5856_12_298.png\n",
            "DSC_5856_12_301.png\n",
            "DSC_5856_12_302.png\n",
            "DSC_5856_12_304.png\n",
            "DSC_5856_12_306.png\n",
            "DSC_5856_12_307.png\n",
            "DSC_5856_12_308.png\n",
            "DSC_5856_12_309.png\n",
            "DSC_5856_12_312.png\n",
            "DSC_5856_12_313.png\n",
            "DSC_5856_12_314.png\n",
            "DSC_5856_12_315.png\n",
            "DSC_5856_12_316.png\n",
            "DSC_5856_12_317.png\n",
            "DSC_5856_12_318.png\n",
            "DSC_5856_12_319.png\n",
            "DSC_5856_12_323.png\n",
            "DSC_5856_12_324.png\n",
            "DSC_5856_12_325.png\n",
            "DSC_5856_12_326.png\n",
            "DSC_5856_12_327.png\n",
            "DSC_5856_12_329.png\n",
            "DSC_5856_12_331.png\n",
            "DSC_5856_12_332.png\n",
            "DSC_5856_12_333.png\n",
            "DSC_5856_12_336.png\n",
            "DSC_5856_12_337.png\n",
            "DSC_5856_12_338.png\n",
            "DSC_5856_12_339.png\n",
            "DSC_5856_12_340.png\n",
            "DSC_5856_12_341.png\n",
            "DSC_5856_12_344.png\n",
            "DSC_5856_12_348.png\n",
            "DSC_5856_12_349.png\n",
            "DSC_5856_12_352.png\n",
            "DSC_5856_12_355.png\n",
            "DSC_5856_12_358.png\n",
            "DSC_5856_12_359.png\n",
            "DSC_5856_12_360.png\n",
            "DSC_5856_12_362.png\n",
            "DSC_5856_12_363.png\n",
            "DSC_5856_12_365.png\n",
            "DSC_5856_12_371.png\n",
            "DSC_5856_12_374.png\n",
            "DSC_5856_12_376.png\n",
            "DSC_5856_12_377.png\n",
            "DSC_5856_12_378.png\n",
            "DSC_5856_12_379.png\n",
            "DSC_5856_12_380.png\n",
            "DSC_5856_12_386.png\n",
            "DSC_5856_12_387.png\n",
            "DSC_5856_12_388.png\n",
            "DSC_5856_12_389.png\n",
            "DSC_5856_12_390.png\n",
            "DSC_5856_12_392.png\n",
            "DSC_5856_12_396.png\n",
            "DSC_5856_12_399.png\n",
            "DSC_5856_12_400.png\n",
            "DSC_5856_12_402.png\n",
            "DSC_5856_12_403.png\n",
            "DSC_5856_12_406.png\n",
            "DSC_5856_12_408.png\n",
            "DSC_5856_12_409.png\n",
            "DSC_5856_12_412.png\n",
            "DSC_5856_12_414.png\n",
            "DSC_5856_12_419.png\n",
            "DSC_5856_12_420.png\n",
            "DSC_5856_12_422.png\n",
            "DSC_5856_12_423.png\n",
            "DSC_5856_12_425.png\n",
            "DSC_5856_12_427.png\n",
            "DSC_5856_12_428.png\n",
            "DSC_5856_12_431.png\n",
            "DSC_5856_12_432.png\n",
            "DSC_5856_12_436.png\n",
            "DSC_5856_12_438.png\n",
            "DSC_5856_12_439.png\n",
            "DSC_5856_12_440.png\n",
            "DSC_5856_12_443.png\n",
            "DSC_5856_12_444.png\n",
            "DSC_5856_12_446.png\n",
            "DSC_5856_12_447.png\n",
            "DSC_5856_12_453.png\n",
            "DSC_5856_12_454.png\n",
            "DSC_5856_12_458.png\n",
            "DSC_5856_12_459.png\n",
            "DSC_5856_12_460.png\n",
            "DSC_5856_12_461.png\n",
            "DSC_5856_12_462.png\n",
            "DSC_5856_12_463.png\n",
            "DSC_5856_12_467.png\n",
            "DSC_5856_12_468.png\n",
            "DSC_5856_12_472.png\n",
            "DSC_5856_12_473.png\n",
            "DSC_5856_12_474.png\n",
            "DSC_5856_12_477.png\n",
            "DSC_5856_12_479.png\n",
            "DSC_5856_12_480.png\n",
            "DSC_5856_12_482.png\n",
            "DSC_5856_12_483.png\n",
            "DSC_5856_12_484.png\n",
            "DSC_5856_12_485.png\n",
            "DSC_5856_12_488.png\n",
            "DSC_5856_12_491.png\n",
            "DSC_5856_12_492.png\n",
            "DSC_5856_12_493.png\n",
            "DSC_5856_12_495.png\n",
            "DSC_5856_12_499.png\n",
            "DSC_5856_13_0.png\n",
            "DSC_5856_13_1.png\n",
            "DSC_5856_13_2.png\n",
            "DSC_5856_13_3.png\n",
            "DSC_5856_13_5.png\n",
            "DSC_5856_13_7.png\n",
            "DSC_5856_13_8.png\n",
            "DSC_5856_13_11.png\n",
            "DSC_5856_13_13.png\n",
            "DSC_5856_13_14.png\n",
            "DSC_5856_13_19.png\n",
            "DSC_5856_13_20.png\n",
            "DSC_5856_13_21.png\n",
            "DSC_5856_13_23.png\n",
            "DSC_5856_13_27.png\n",
            "DSC_5856_13_31.png\n",
            "DSC_5856_13_33.png\n",
            "DSC_5856_13_34.png\n",
            "DSC_5856_13_38.png\n",
            "DSC_5856_13_39.png\n",
            "DSC_5856_13_40.png\n",
            "DSC_5856_13_41.png\n",
            "DSC_5856_13_42.png\n",
            "DSC_5856_13_47.png\n",
            "DSC_5856_13_48.png\n",
            "DSC_5856_13_54.png\n",
            "DSC_5856_13_55.png\n",
            "DSC_5856_13_57.png\n",
            "DSC_5856_13_58.png\n",
            "DSC_5856_13_62.png\n",
            "DSC_5856_13_64.png\n",
            "DSC_5856_13_65.png\n",
            "DSC_5856_13_66.png\n",
            "DSC_5856_13_67.png\n",
            "DSC_5856_13_68.png\n",
            "DSC_5856_13_70.png\n",
            "DSC_5856_13_71.png\n",
            "DSC_5856_13_78.png\n",
            "DSC_5856_13_80.png\n",
            "DSC_5856_13_82.png\n",
            "DSC_5856_13_83.png\n",
            "DSC_5856_13_84.png\n",
            "DSC_5856_13_91.png\n",
            "DSC_5856_13_92.png\n",
            "DSC_5856_13_96.png\n",
            "DSC_5856_13_97.png\n",
            "DSC_5856_13_101.png\n",
            "DSC_5856_13_102.png\n",
            "DSC_5856_13_105.png\n",
            "DSC_5856_13_106.png\n",
            "DSC_5856_13_108.png\n",
            "DSC_5856_13_112.png\n",
            "DSC_5856_13_121.png\n",
            "DSC_5856_13_124.png\n",
            "DSC_5856_13_125.png\n",
            "DSC_5856_13_126.png\n",
            "DSC_5856_13_129.png\n",
            "DSC_5856_13_130.png\n",
            "DSC_5856_13_131.png\n",
            "DSC_5856_13_138.png\n",
            "DSC_5856_13_139.png\n",
            "DSC_5856_13_144.png\n",
            "DSC_5856_13_145.png\n",
            "DSC_5856_13_146.png\n",
            "DSC_5856_13_152.png\n",
            "DSC_5856_13_154.png\n",
            "DSC_5856_13_155.png\n",
            "DSC_5856_13_158.png\n",
            "DSC_5856_13_162.png\n",
            "DSC_5856_13_164.png\n",
            "DSC_5856_13_170.png\n",
            "DSC_5856_13_172.png\n",
            "DSC_5856_13_173.png\n",
            "DSC_5856_13_174.png\n",
            "DSC_5856_13_175.png\n",
            "DSC_5856_13_177.png\n",
            "DSC_5856_13_183.png\n",
            "DSC_5856_13_187.png\n",
            "DSC_5856_13_188.png\n",
            "DSC_5856_13_189.png\n",
            "DSC_5856_13_190.png\n",
            "DSC_5856_13_191.png\n",
            "DSC_5856_13_193.png\n",
            "DSC_5856_13_194.png\n",
            "DSC_5856_13_196.png\n",
            "DSC_5856_13_198.png\n",
            "DSC_5856_13_201.png\n",
            "DSC_5856_13_202.png\n",
            "DSC_5856_13_206.png\n",
            "DSC_5856_13_210.png\n",
            "DSC_5856_13_211.png\n",
            "DSC_5856_13_212.png\n",
            "DSC_5856_13_213.png\n",
            "DSC_5856_13_214.png\n",
            "DSC_5856_13_215.png\n",
            "DSC_5856_13_216.png\n",
            "DSC_5856_13_220.png\n",
            "DSC_5856_13_221.png\n",
            "DSC_5856_13_222.png\n",
            "DSC_5856_13_223.png\n",
            "DSC_5856_13_225.png\n",
            "DSC_5856_13_228.png\n",
            "DSC_5856_13_229.png\n",
            "DSC_5856_13_230.png\n",
            "DSC_5856_13_238.png\n",
            "DSC_5856_13_240.png\n",
            "DSC_5856_13_241.png\n",
            "DSC_5856_13_246.png\n",
            "DSC_5856_13_247.png\n",
            "DSC_5856_13_249.png\n",
            "DSC_5856_13_250.png\n",
            "DSC_5856_13_253.png\n",
            "DSC_5856_13_254.png\n",
            "DSC_5856_13_257.png\n",
            "DSC_5856_13_260.png\n",
            "DSC_5856_13_262.png\n",
            "DSC_5856_13_263.png\n",
            "DSC_5856_13_265.png\n",
            "DSC_5856_13_275.png\n",
            "DSC_5856_13_278.png\n",
            "DSC_5856_13_279.png\n",
            "DSC_5856_13_280.png\n",
            "DSC_5856_13_281.png\n",
            "DSC_5856_13_282.png\n",
            "DSC_5856_13_283.png\n",
            "DSC_5856_13_285.png\n",
            "DSC_5856_13_288.png\n",
            "DSC_5856_13_290.png\n",
            "DSC_5856_13_292.png\n",
            "DSC_5856_13_293.png\n",
            "DSC_5856_13_298.png\n",
            "DSC_5856_13_299.png\n",
            "DSC_5856_13_303.png\n",
            "DSC_5856_13_304.png\n",
            "DSC_5856_13_306.png\n",
            "DSC_5856_13_307.png\n",
            "DSC_5856_13_309.png\n",
            "DSC_5856_13_312.png\n",
            "DSC_5856_13_315.png\n",
            "DSC_5856_13_316.png\n",
            "DSC_5856_13_320.png\n",
            "DSC_5856_13_326.png\n",
            "DSC_5856_13_327.png\n",
            "DSC_5856_13_328.png\n",
            "DSC_5856_13_332.png\n",
            "DSC_5856_13_333.png\n",
            "DSC_5856_13_334.png\n",
            "DSC_5856_13_336.png\n",
            "DSC_5856_13_340.png\n",
            "DSC_5856_13_341.png\n",
            "DSC_5856_13_344.png\n",
            "DSC_5856_13_345.png\n",
            "DSC_5856_13_348.png\n",
            "DSC_5856_13_349.png\n",
            "DSC_5856_13_350.png\n",
            "DSC_5856_13_351.png\n",
            "DSC_5856_13_355.png\n",
            "DSC_5856_13_357.png\n",
            "DSC_5856_13_360.png\n",
            "DSC_5856_13_363.png\n",
            "DSC_5856_13_366.png\n",
            "DSC_5856_13_367.png\n",
            "DSC_5856_13_368.png\n",
            "DSC_5856_13_370.png\n",
            "DSC_5856_13_372.png\n",
            "DSC_5856_13_373.png\n",
            "DSC_5856_13_374.png\n",
            "DSC_5856_13_375.png\n",
            "DSC_5856_13_376.png\n",
            "DSC_5856_13_384.png\n",
            "DSC_5856_13_385.png\n",
            "DSC_5856_13_391.png\n",
            "DSC_5856_13_392.png\n",
            "DSC_5856_13_393.png\n",
            "DSC_5856_13_394.png\n",
            "DSC_5856_13_399.png\n",
            "DSC_5856_13_400.png\n",
            "DSC_5856_13_405.png\n",
            "DSC_5856_13_408.png\n",
            "DSC_5856_13_409.png\n",
            "DSC_5856_13_412.png\n",
            "DSC_5856_13_417.png\n",
            "DSC_5856_13_422.png\n",
            "DSC_5856_13_425.png\n",
            "DSC_5856_13_428.png\n",
            "DSC_5856_13_429.png\n",
            "DSC_5856_13_434.png\n",
            "DSC_5856_13_435.png\n",
            "DSC_5856_13_438.png\n",
            "DSC_5856_13_439.png\n",
            "DSC_5856_13_440.png\n",
            "DSC_5856_13_444.png\n",
            "DSC_5856_13_448.png\n",
            "DSC_5856_13_449.png\n",
            "DSC_5856_13_457.png\n",
            "DSC_5856_13_458.png\n",
            "DSC_5856_13_459.png\n",
            "DSC_5856_13_460.png\n",
            "DSC_5856_13_461.png\n",
            "DSC_5856_13_462.png\n",
            "DSC_5856_13_463.png\n",
            "DSC_5856_13_465.png\n",
            "DSC_5856_13_466.png\n",
            "DSC_5856_13_475.png\n",
            "DSC_5856_13_478.png\n",
            "DSC_5856_13_479.png\n",
            "DSC_5856_13_481.png\n",
            "DSC_5856_13_484.png\n",
            "DSC_5856_13_488.png\n",
            "DSC_5856_13_491.png\n",
            "DSC_5856_13_492.png\n",
            "DSC_5856_13_495.png\n",
            "DSC_5856_13_496.png\n",
            "DSC_5856_13_497.png\n",
            "DSC_5856_13_498.png\n",
            "DSC_5856_21_2.png\n",
            "DSC_5856_21_4.png\n",
            "DSC_5856_21_11.png\n",
            "DSC_5856_21_13.png\n",
            "DSC_5856_21_15.png\n",
            "DSC_5856_21_18.png\n",
            "DSC_5856_21_21.png\n",
            "DSC_5856_21_25.png\n",
            "DSC_5856_21_28.png\n",
            "DSC_5856_21_29.png\n",
            "DSC_5856_21_30.png\n",
            "DSC_5856_21_35.png\n",
            "DSC_5856_21_36.png\n",
            "DSC_5856_21_37.png\n",
            "DSC_5856_21_40.png\n",
            "DSC_5856_21_41.png\n",
            "DSC_5856_21_44.png\n",
            "DSC_5856_21_45.png\n",
            "DSC_5856_21_49.png\n",
            "DSC_5856_21_50.png\n",
            "DSC_5856_21_53.png\n",
            "DSC_5856_21_54.png\n",
            "DSC_5856_21_56.png\n",
            "DSC_5856_21_57.png\n",
            "DSC_5856_21_60.png\n",
            "DSC_5856_21_61.png\n",
            "DSC_5856_21_63.png\n",
            "DSC_5856_21_65.png\n",
            "DSC_5856_21_66.png\n",
            "DSC_5856_21_68.png\n",
            "DSC_5856_21_73.png\n",
            "DSC_5856_21_75.png\n",
            "DSC_5856_21_85.png\n",
            "DSC_5856_21_86.png\n",
            "DSC_5856_21_88.png\n",
            "DSC_5856_21_89.png\n",
            "DSC_5856_21_90.png\n",
            "DSC_5856_21_95.png\n",
            "DSC_5856_21_98.png\n",
            "DSC_5856_21_99.png\n",
            "DSC_5856_21_100.png\n",
            "DSC_5856_21_101.png\n",
            "DSC_5856_21_104.png\n",
            "DSC_5856_21_105.png\n",
            "DSC_5856_21_108.png\n",
            "DSC_5856_21_109.png\n",
            "DSC_5856_21_117.png\n",
            "DSC_5856_21_120.png\n",
            "DSC_5856_21_121.png\n",
            "DSC_5856_21_125.png\n",
            "DSC_5856_21_126.png\n",
            "DSC_5856_21_127.png\n",
            "DSC_5856_21_129.png\n",
            "DSC_5856_21_132.png\n",
            "DSC_5856_21_133.png\n",
            "DSC_5856_21_135.png\n",
            "DSC_5856_21_136.png\n",
            "DSC_5856_21_137.png\n",
            "DSC_5856_21_141.png\n",
            "DSC_5856_21_142.png\n",
            "DSC_5856_21_143.png\n",
            "DSC_5856_21_145.png\n",
            "DSC_5856_21_151.png\n",
            "DSC_5856_21_158.png\n",
            "DSC_5856_21_159.png\n",
            "DSC_5856_21_160.png\n",
            "DSC_5856_21_163.png\n",
            "DSC_5856_21_164.png\n",
            "DSC_5856_21_165.png\n",
            "DSC_5856_21_167.png\n",
            "DSC_5856_21_168.png\n",
            "DSC_5856_21_170.png\n",
            "DSC_5856_21_172.png\n",
            "DSC_5856_21_175.png\n",
            "DSC_5856_21_176.png\n",
            "DSC_5856_21_178.png\n",
            "DSC_5856_21_179.png\n",
            "DSC_5856_21_180.png\n",
            "DSC_5856_21_181.png\n",
            "DSC_5856_21_189.png\n",
            "DSC_5856_21_192.png\n",
            "DSC_5856_21_196.png\n",
            "DSC_5856_21_200.png\n",
            "DSC_5856_21_202.png\n",
            "DSC_5856_21_203.png\n",
            "DSC_5856_21_205.png\n",
            "DSC_5856_21_207.png\n",
            "DSC_5856_21_209.png\n",
            "DSC_5856_21_210.png\n",
            "DSC_5856_21_214.png\n",
            "DSC_5856_21_218.png\n",
            "DSC_5856_21_219.png\n",
            "DSC_5856_21_220.png\n",
            "DSC_5856_21_234.png\n",
            "DSC_5856_21_235.png\n",
            "DSC_5856_21_238.png\n",
            "DSC_5856_21_239.png\n",
            "DSC_5856_21_241.png\n",
            "DSC_5856_21_244.png\n",
            "DSC_5856_21_245.png\n",
            "DSC_5856_21_248.png\n",
            "DSC_5856_21_250.png\n",
            "DSC_5856_21_255.png\n",
            "DSC_5856_21_260.png\n",
            "DSC_5856_21_264.png\n",
            "DSC_5856_21_269.png\n",
            "DSC_5856_21_271.png\n",
            "DSC_5856_21_273.png\n",
            "DSC_5856_21_277.png\n",
            "DSC_5856_21_279.png\n",
            "DSC_5856_21_280.png\n",
            "DSC_5856_21_281.png\n",
            "DSC_5856_21_283.png\n",
            "DSC_5856_21_284.png\n",
            "DSC_5856_21_285.png\n",
            "DSC_5856_21_286.png\n",
            "DSC_5856_21_289.png\n",
            "DSC_5856_21_290.png\n",
            "DSC_5856_21_295.png\n",
            "DSC_5856_21_297.png\n",
            "DSC_5856_21_298.png\n",
            "DSC_5856_21_300.png\n",
            "DSC_5856_21_303.png\n",
            "DSC_5856_21_310.png\n",
            "DSC_5856_21_315.png\n",
            "DSC_5856_21_318.png\n",
            "DSC_5856_21_322.png\n",
            "DSC_5856_21_323.png\n",
            "DSC_5856_21_328.png\n",
            "DSC_5856_21_329.png\n",
            "DSC_5856_21_330.png\n",
            "DSC_5856_21_331.png\n",
            "DSC_5856_21_334.png\n",
            "DSC_5856_21_337.png\n",
            "DSC_5856_21_346.png\n",
            "DSC_5856_21_347.png\n",
            "DSC_5856_21_351.png\n",
            "DSC_5856_21_352.png\n",
            "DSC_5856_21_353.png\n",
            "DSC_5856_21_354.png\n",
            "DSC_5856_21_357.png\n",
            "DSC_5856_21_364.png\n",
            "DSC_5856_21_367.png\n",
            "DSC_5856_21_368.png\n",
            "DSC_5856_21_369.png\n",
            "DSC_5856_21_370.png\n",
            "DSC_5856_21_371.png\n",
            "DSC_5856_21_372.png\n",
            "DSC_5856_21_373.png\n",
            "DSC_5856_21_375.png\n",
            "DSC_5856_21_378.png\n",
            "DSC_5856_21_379.png\n",
            "DSC_5856_21_381.png\n",
            "DSC_5856_21_382.png\n",
            "DSC_5856_21_383.png\n",
            "DSC_5856_21_384.png\n",
            "DSC_5856_21_385.png\n",
            "DSC_5856_21_386.png\n",
            "DSC_5856_21_390.png\n",
            "DSC_5856_21_391.png\n",
            "DSC_5856_21_393.png\n",
            "DSC_5856_21_395.png\n",
            "DSC_5856_21_397.png\n",
            "DSC_5856_21_401.png\n",
            "DSC_5856_21_404.png\n",
            "DSC_5856_21_406.png\n",
            "DSC_5856_21_411.png\n",
            "DSC_5856_21_413.png\n",
            "DSC_5856_21_416.png\n",
            "DSC_5856_21_418.png\n",
            "DSC_5856_21_425.png\n",
            "DSC_5856_21_427.png\n",
            "DSC_5856_21_431.png\n",
            "DSC_5856_21_433.png\n",
            "DSC_5856_21_434.png\n",
            "DSC_5856_21_437.png\n",
            "DSC_5856_21_441.png\n",
            "DSC_5856_21_442.png\n",
            "DSC_5856_21_446.png\n",
            "DSC_5856_21_449.png\n",
            "DSC_5856_21_453.png\n",
            "DSC_5856_21_455.png\n",
            "DSC_5856_21_456.png\n",
            "DSC_5856_21_464.png\n",
            "DSC_5856_21_471.png\n",
            "DSC_5856_21_476.png\n",
            "DSC_5856_21_480.png\n",
            "DSC_5856_21_485.png\n",
            "DSC_5856_21_486.png\n",
            "DSC_5856_21_487.png\n",
            "DSC_5856_21_492.png\n",
            "DSC_5856_21_498.png\n",
            "DSC_5856_25_115.png\n",
            "DSC_5856_25_273.png\n",
            "DSC_5856_25_292.png\n",
            "DSC_5856_25_313.png\n",
            "DSC_5856_25_367.png\n",
            "DSC_5856_25_403.png\n",
            "DSC_5856_25_422.png\n",
            "DSC_5856_25_436.png\n",
            "DSC_5856_25_457.png\n",
            "DSC_5856_25_461.png\n",
            "DSC_5856_25_491.png\n",
            "DSC_5856_25_494.png\n",
            "DSC_5856_11_4.png\n",
            "DSC_5856_11_20.png\n",
            "DSC_5856_11_23.png\n",
            "DSC_5856_11_24.png\n",
            "DSC_5856_11_25.png\n",
            "DSC_5856_11_39.png\n",
            "DSC_5856_11_42.png\n",
            "DSC_5856_11_45.png\n",
            "DSC_5856_11_57.png\n",
            "DSC_5856_11_62.png\n",
            "DSC_5856_11_67.png\n",
            "DSC_5856_11_70.png\n",
            "DSC_5856_11_77.png\n",
            "DSC_5856_11_87.png\n",
            "DSC_5856_11_92.png\n",
            "DSC_5856_11_97.png\n",
            "DSC_5856_11_105.png\n",
            "DSC_5856_11_118.png\n",
            "DSC_5856_11_121.png\n",
            "DSC_5856_11_126.png\n",
            "DSC_5856_11_131.png\n",
            "DSC_5856_11_138.png\n",
            "DSC_5856_11_141.png\n",
            "DSC_5856_11_142.png\n",
            "DSC_5856_11_152.png\n",
            "DSC_5856_11_155.png\n",
            "DSC_5856_11_156.png\n",
            "DSC_5856_11_158.png\n",
            "DSC_5856_11_165.png\n",
            "DSC_5856_11_169.png\n",
            "DSC_5856_11_170.png\n",
            "DSC_5856_11_183.png\n",
            "DSC_5856_11_184.png\n",
            "DSC_5856_11_194.png\n",
            "DSC_5856_11_197.png\n",
            "DSC_5856_11_198.png\n",
            "DSC_5856_11_199.png\n",
            "DSC_5856_11_205.png\n",
            "DSC_5856_11_210.png\n",
            "DSC_5856_11_211.png\n",
            "DSC_5856_11_221.png\n",
            "DSC_5856_11_223.png\n",
            "DSC_5856_11_231.png\n",
            "DSC_5856_11_238.png\n",
            "DSC_5856_11_246.png\n",
            "DSC_5856_11_247.png\n",
            "DSC_5856_11_252.png\n",
            "DSC_5856_11_256.png\n",
            "DSC_5856_11_257.png\n",
            "DSC_5856_11_260.png\n",
            "DSC_5856_11_266.png\n",
            "DSC_5856_11_276.png\n",
            "DSC_5856_11_280.png\n",
            "DSC_5856_11_308.png\n",
            "DSC_5856_11_309.png\n",
            "DSC_5856_11_310.png\n",
            "DSC_5856_11_314.png\n",
            "DSC_5856_11_316.png\n",
            "DSC_5856_11_319.png\n",
            "DSC_5856_11_324.png\n",
            "DSC_5856_11_325.png\n",
            "DSC_5856_11_326.png\n",
            "DSC_5856_11_332.png\n",
            "DSC_5856_11_359.png\n",
            "DSC_5856_11_360.png\n",
            "DSC_5856_11_362.png\n",
            "DSC_5856_11_364.png\n",
            "DSC_5856_11_365.png\n",
            "DSC_5856_11_366.png\n",
            "DSC_5856_11_367.png\n",
            "DSC_5856_11_390.png\n",
            "DSC_5856_11_391.png\n",
            "DSC_5856_11_392.png\n",
            "DSC_5856_11_402.png\n",
            "DSC_5856_11_406.png\n",
            "DSC_5856_11_410.png\n",
            "DSC_5856_11_411.png\n",
            "DSC_5856_11_412.png\n",
            "DSC_5856_11_414.png\n",
            "DSC_5856_11_427.png\n",
            "DSC_5856_11_432.png\n",
            "DSC_5856_11_435.png\n",
            "DSC_5856_11_439.png\n",
            "DSC_5856_11_443.png\n",
            "DSC_5856_11_450.png\n",
            "DSC_5856_11_451.png\n",
            "DSC_5856_11_458.png\n",
            "DSC_5856_11_460.png\n",
            "DSC_5856_11_461.png\n",
            "DSC_5856_11_462.png\n",
            "DSC_5856_11_465.png\n",
            "DSC_5856_11_467.png\n",
            "DSC_5856_11_470.png\n",
            "DSC_5856_11_473.png\n",
            "DSC_5856_11_478.png\n",
            "DSC_5856_11_482.png\n",
            "DSC_5856_11_489.png\n",
            "DSC_5856_11_491.png\n",
            "DSC_5856_11_493.png\n",
            "DSC_5856_11_496.png\n",
            "DSC_5856_11_497.png\n",
            "DSC_5856_11_499.png\n",
            "DSC_5856_2_83.png\n",
            "DSC_5856_2_106.png\n",
            "DSC_5856_2_155.png\n",
            "DSC_5856_2_169.png\n",
            "DSC_5856_2_253.png\n",
            "DSC_5856_2_281.png\n",
            "DSC_5856_2_317.png\n",
            "DSC_5856_2_339.png\n",
            "DSC_5856_2_368.png\n",
            "DSC_5856_2_417.png\n",
            "DSC_5856_2_433.png\n",
            "DSC_5856_2_436.png\n",
            "DSC_5856_2_439.png\n",
            "DSC_5856_2_448.png\n",
            "DSC_5856_52_4.png\n",
            "DSC_5856_52_10.png\n",
            "DSC_5856_52_24.png\n",
            "DSC_5856_52_45.png\n",
            "DSC_5856_52_51.png\n",
            "DSC_5856_52_77.png\n",
            "DSC_5856_52_82.png\n",
            "DSC_5856_52_83.png\n",
            "DSC_5856_52_112.png\n",
            "DSC_5856_52_113.png\n",
            "DSC_5856_52_118.png\n",
            "DSC_5856_52_137.png\n",
            "DSC_5856_52_146.png\n",
            "DSC_5856_52_147.png\n",
            "DSC_5856_52_152.png\n",
            "DSC_5856_52_167.png\n",
            "DSC_5856_52_182.png\n",
            "DSC_5856_52_193.png\n",
            "DSC_5856_52_194.png\n",
            "DSC_5856_52_195.png\n",
            "DSC_5856_52_235.png\n",
            "DSC_5856_52_245.png\n",
            "DSC_5856_52_253.png\n",
            "DSC_5856_52_297.png\n",
            "DSC_5856_52_301.png\n",
            "DSC_5856_52_314.png\n",
            "DSC_5856_52_321.png\n",
            "DSC_5856_52_335.png\n",
            "DSC_5856_52_345.png\n",
            "DSC_5856_52_378.png\n",
            "DSC_5856_52_398.png\n",
            "DSC_5856_52_399.png\n",
            "DSC_5856_52_445.png\n",
            "DSC_5856_52_486.png\n",
            "DSC_5856_52_489.png\n",
            "DSC_5856_52_492.png\n",
            "DSC_5856_1_19.png\n",
            "DSC_5856_1_24.png\n",
            "DSC_5856_1_287.png\n",
            "DSC_5856_1_305.png\n",
            "DSC_5856_1_343.png\n",
            "DSC_5856_1_348.png\n",
            "DSC_5856_1_350.png\n",
            "DSC_5856_1_409.png\n",
            "DSC_5856_1_416.png\n",
            "DSC_5856_1_432.png\n",
            "DSC_5856_1_466.png\n",
            "DSC_5856_1_473.png\n",
            "DSC_5856_1_489.png\n",
            "DSC_5856_1_503.png\n",
            "DSC_5856_1_535.png\n",
            "DSC_5856_1_542.png\n",
            "DSC_5856_1_596.png\n",
            "DSC_5856_1_605.png\n",
            "DSC_5856_1_654.png\n",
            "DSC_5856_1_655.png\n",
            "DSC_5856_1_657.png\n",
            "DSC_5856_1_678.png\n",
            "DSC_5856_1_699.png\n",
            "DSC_5856_1_718.png\n",
            "DSC_5856_1_766.png\n",
            "DSC_5856_1_771.png\n",
            "DSC_5856_1_772.png\n",
            "DSC_5856_1_803.png\n",
            "DSC_5856_1_813.png\n",
            "DSC_5856_1_819.png\n",
            "DSC_5856_1_820.png\n",
            "DSC_5856_1_834.png\n",
            "DSC_5856_1_838.png\n",
            "DSC_5856_1_849.png\n",
            "DSC_5856_1_897.png\n",
            "DSC_5856_1_903.png\n",
            "DSC_5856_1_908.png\n",
            "DSC_5856_1_968.png\n",
            "DSC_5856_1_977.png\n",
            "DSC_5856_1_979.png\n",
            "DSC_5856_14_77.png\n",
            "DSC_5856_14_124.png\n",
            "DSC_5856_14_167.png\n",
            "DSC_5856_14_245.png\n",
            "DSC_5856_14_291.png\n",
            "DSC_5856_14_392.png\n",
            "DSC_5856_14_408.png\n",
            "DSC_5856_14_410.png\n",
            "DSC_5856_14_430.png\n",
            "DSC_5856_14_444.png\n",
            "DSC_5856_14_641.png\n",
            "DSC_5856_14_645.png\n",
            "DSC_5856_14_699.png\n",
            "DSC_5856_14_729.png\n",
            "DSC_5856_14_749.png\n",
            "DSC_5856_12_514.png\n",
            "DSC_5856_12_522.png\n",
            "DSC_5856_12_524.png\n",
            "DSC_5856_12_533.png\n",
            "DSC_5856_12_538.png\n",
            "DSC_5856_12_541.png\n",
            "DSC_5856_12_543.png\n",
            "DSC_5856_12_556.png\n",
            "DSC_5856_12_570.png\n",
            "DSC_5856_12_585.png\n",
            "DSC_5856_12_594.png\n",
            "DSC_5856_12_597.png\n",
            "DSC_5856_12_600.png\n",
            "DSC_5856_12_607.png\n",
            "DSC_5856_12_616.png\n",
            "DSC_5856_12_635.png\n",
            "DSC_5856_12_645.png\n",
            "DSC_5856_12_646.png\n",
            "DSC_5856_12_649.png\n",
            "DSC_5856_12_651.png\n",
            "DSC_5856_12_653.png\n",
            "DSC_5856_12_661.png\n",
            "DSC_5856_12_666.png\n",
            "DSC_5856_12_672.png\n",
            "DSC_5856_12_683.png\n",
            "DSC_5856_12_685.png\n",
            "DSC_5856_12_694.png\n",
            "DSC_5856_12_695.png\n",
            "DSC_5856_12_701.png\n",
            "DSC_5856_12_704.png\n",
            "DSC_5856_12_705.png\n",
            "DSC_5856_12_711.png\n",
            "DSC_5856_12_713.png\n",
            "DSC_5856_12_717.png\n",
            "DSC_5856_12_731.png\n",
            "DSC_5856_12_732.png\n",
            "DSC_5856_12_741.png\n",
            "DSC_5856_12_743.png\n",
            "DSC_5856_12_748.png\n",
            "DSC_5856_12_750.png\n",
            "DSC_5856_12_758.png\n",
            "DSC_5856_12_767.png\n",
            "DSC_5856_12_772.png\n",
            "DSC_5856_12_774.png\n",
            "DSC_5856_12_776.png\n",
            "DSC_5856_12_790.png\n",
            "DSC_5856_12_796.png\n",
            "DSC_5856_12_799.png\n",
            "DSC_5856_12_812.png\n",
            "DSC_5856_12_815.png\n",
            "DSC_5856_12_823.png\n",
            "DSC_5856_12_836.png\n",
            "DSC_5856_12_838.png\n",
            "DSC_5856_12_844.png\n",
            "DSC_5856_12_868.png\n",
            "DSC_5856_12_873.png\n",
            "DSC_5856_12_885.png\n",
            "DSC_5856_12_888.png\n",
            "DSC_5856_12_889.png\n",
            "DSC_5856_12_901.png\n",
            "DSC_5856_12_903.png\n",
            "DSC_5856_12_906.png\n",
            "DSC_5856_12_908.png\n",
            "DSC_5856_12_939.png\n",
            "DSC_5856_12_944.png\n",
            "DSC_5856_12_950.png\n",
            "DSC_5856_12_958.png\n",
            "DSC_5856_12_974.png\n",
            "DSC_5856_12_986.png\n",
            "DSC_5856_12_990.png\n",
            "DSC_5856_12_992.png\n",
            "DSC_5856_12_1009.png\n",
            "DSC_5856_12_1020.png\n",
            "DSC_5856_12_1021.png\n",
            "DSC_5856_12_1033.png\n",
            "DSC_5856_12_1035.png\n",
            "DSC_5856_12_1041.png\n",
            "DSC_5856_12_1043.png\n",
            "DSC_5856_12_1047.png\n",
            "DSC_5856_12_1054.png\n",
            "DSC_5856_12_1091.png\n",
            "DSC_5856_12_1097.png\n",
            "DSC_5856_12_1132.png\n",
            "DSC_5856_12_1134.png\n",
            "DSC_5856_12_1144.png\n",
            "DSC_5856_12_1150.png\n",
            "DSC_5856_12_1152.png\n",
            "DSC_5856_12_1158.png\n",
            "DSC_5856_12_1160.png\n",
            "DSC_5856_12_1170.png\n",
            "DSC_5856_12_1172.png\n",
            "DSC_5856_12_1174.png\n",
            "DSC_5856_12_1177.png\n",
            "DSC_5856_12_1180.png\n",
            "DSC_5856_12_1182.png\n",
            "DSC_5856_12_1187.png\n",
            "DSC_5856_12_1197.png\n",
            "DSC_5856_12_1216.png\n",
            "DSC_5856_12_1224.png\n",
            "DSC_5856_12_1227.png\n",
            "DSC_5856_12_1254.png\n",
            "DSC_5856_12_1256.png\n",
            "DSC_5856_12_1258.png\n",
            "DSC_5856_12_1261.png\n",
            "DSC_5856_12_1263.png\n",
            "DSC_5856_12_1273.png\n",
            "DSC_5856_12_1275.png\n",
            "DSC_5856_12_1278.png\n",
            "DSC_5856_12_1284.png\n",
            "DSC_5856_12_1290.png\n",
            "DSC_5856_12_1297.png\n",
            "DSC_5856_12_1318.png\n",
            "DSC_5856_12_1322.png\n",
            "DSC_5856_12_1323.png\n",
            "DSC_5856_12_1326.png\n",
            "DSC_5856_12_1329.png\n",
            "DSC_5856_12_1332.png\n",
            "DSC_5856_12_1338.png\n",
            "DSC_5856_12_1341.png\n",
            "DSC_5856_12_1345.png\n",
            "DSC_5856_12_1350.png\n",
            "DSC_5856_12_1360.png\n",
            "DSC_5856_12_1365.png\n",
            "DSC_5856_12_1372.png\n",
            "DSC_5856_12_1374.png\n",
            "DSC_5856_12_1384.png\n",
            "DSC_5856_12_1389.png\n",
            "DSC_5856_12_1395.png\n",
            "DSC_5856_12_1401.png\n",
            "DSC_5856_12_1407.png\n",
            "DSC_5856_12_1422.png\n",
            "DSC_5856_12_1426.png\n",
            "DSC_5856_12_1434.png\n",
            "DSC_5856_12_1442.png\n",
            "DSC_5856_12_1444.png\n",
            "DSC_5856_12_1453.png\n",
            "DSC_5856_12_1456.png\n",
            "DSC_5856_12_1464.png\n",
            "DSC_5856_12_1484.png\n",
            "DSC_5856_12_1486.png\n",
            "DSC_5856_12_1492.png\n",
            "DSC_5856_13_503.png\n",
            "DSC_5856_13_519.png\n",
            "DSC_5856_13_526.png\n",
            "DSC_5856_13_532.png\n",
            "DSC_5856_13_541.png\n",
            "DSC_5856_13_552.png\n",
            "DSC_5856_13_554.png\n",
            "DSC_5856_13_556.png\n",
            "DSC_5856_13_558.png\n",
            "DSC_5856_13_567.png\n",
            "DSC_5856_13_581.png\n",
            "DSC_5856_13_583.png\n",
            "DSC_5856_13_585.png\n",
            "DSC_5856_13_589.png\n",
            "DSC_5856_13_598.png\n",
            "DSC_5856_13_605.png\n",
            "DSC_5856_13_613.png\n",
            "DSC_5856_13_620.png\n",
            "DSC_5856_13_625.png\n",
            "DSC_5856_13_631.png\n",
            "DSC_5856_13_678.png\n",
            "DSC_5856_13_693.png\n",
            "DSC_5856_13_702.png\n",
            "DSC_5856_13_710.png\n",
            "DSC_5856_13_713.png\n",
            "DSC_5856_13_715.png\n",
            "DSC_5856_13_723.png\n",
            "DSC_5856_13_725.png\n",
            "DSC_5856_13_731.png\n",
            "DSC_5856_13_734.png\n",
            "DSC_5856_13_746.png\n",
            "DSC_5856_13_751.png\n",
            "DSC_5856_13_753.png\n",
            "DSC_5856_13_757.png\n",
            "DSC_5856_13_763.png\n",
            "DSC_5856_13_790.png\n",
            "DSC_5856_13_797.png\n",
            "DSC_5856_13_800.png\n",
            "DSC_5856_13_806.png\n",
            "DSC_5856_13_815.png\n",
            "DSC_5856_13_822.png\n",
            "DSC_5856_13_828.png\n",
            "DSC_5856_13_830.png\n",
            "DSC_5856_13_849.png\n",
            "DSC_5856_13_864.png\n",
            "DSC_5856_13_875.png\n",
            "DSC_5856_13_877.png\n",
            "DSC_5856_13_889.png\n",
            "DSC_5856_13_899.png\n",
            "DSC_5856_13_902.png\n",
            "DSC_5856_13_911.png\n",
            "DSC_5856_13_918.png\n",
            "DSC_5856_13_920.png\n",
            "DSC_5856_13_924.png\n",
            "DSC_5856_13_926.png\n",
            "DSC_5856_13_928.png\n",
            "DSC_5856_13_937.png\n",
            "DSC_5856_13_940.png\n",
            "DSC_5856_13_942.png\n",
            "DSC_5856_13_952.png\n",
            "DSC_5856_13_963.png\n",
            "DSC_5856_13_965.png\n",
            "DSC_5856_13_966.png\n",
            "DSC_5856_13_971.png\n",
            "DSC_5856_13_974.png\n",
            "DSC_5856_13_991.png\n",
            "DSC_5856_13_1002.png\n",
            "DSC_5856_13_1004.png\n",
            "DSC_5856_13_1012.png\n",
            "DSC_5856_13_1022.png\n",
            "DSC_5856_13_1029.png\n",
            "DSC_5856_13_1035.png\n",
            "DSC_5856_13_1077.png\n",
            "DSC_5856_13_1088.png\n",
            "DSC_5856_13_1090.png\n",
            "DSC_5856_13_1138.png\n",
            "DSC_5856_13_1140.png\n",
            "DSC_5856_13_1158.png\n",
            "DSC_5856_13_1190.png\n",
            "DSC_5856_13_1196.png\n",
            "DSC_5856_13_1211.png\n",
            "DSC_5856_13_1228.png\n",
            "DSC_5856_13_1236.png\n",
            "DSC_5856_13_1239.png\n",
            "DSC_5856_13_1242.png\n",
            "DSC_5856_13_1250.png\n",
            "DSC_5856_13_1254.png\n",
            "DSC_5856_13_1257.png\n",
            "DSC_5856_13_1263.png\n",
            "DSC_5856_13_1265.png\n",
            "DSC_5856_13_1267.png\n",
            "DSC_5856_13_1269.png\n",
            "DSC_5856_13_1281.png\n",
            "DSC_5856_13_1284.png\n",
            "DSC_5856_13_1295.png\n",
            "DSC_5856_13_1297.png\n",
            "DSC_5856_13_1299.png\n",
            "DSC_5856_13_1301.png\n",
            "DSC_5856_13_1309.png\n",
            "DSC_5856_13_1311.png\n",
            "DSC_5856_13_1314.png\n",
            "DSC_5856_13_1317.png\n",
            "DSC_5856_13_1338.png\n",
            "DSC_5856_13_1357.png\n",
            "DSC_5856_13_1360.png\n",
            "DSC_5856_13_1369.png\n",
            "DSC_5856_13_1376.png\n",
            "DSC_5856_13_1378.png\n",
            "DSC_5856_13_1379.png\n",
            "DSC_5856_13_1388.png\n",
            "DSC_5856_13_1409.png\n",
            "DSC_5856_13_1414.png\n",
            "DSC_5856_13_1418.png\n",
            "DSC_5856_13_1429.png\n",
            "DSC_5856_13_1431.png\n",
            "DSC_5856_13_1453.png\n",
            "DSC_5856_13_1470.png\n",
            "DSC_5856_13_1473.png\n",
            "DSC_5856_13_1483.png\n",
            "DSC_5856_13_1490.png\n",
            "DSC_5856_13_1492.png\n",
            "DSC_5856_21_521.png\n",
            "DSC_5856_21_531.png\n",
            "DSC_5856_21_540.png\n",
            "DSC_5856_21_543.png\n",
            "DSC_5856_21_556.png\n",
            "DSC_5856_21_563.png\n",
            "DSC_5856_21_565.png\n",
            "DSC_5856_21_580.png\n",
            "DSC_5856_21_595.png\n",
            "DSC_5856_21_597.png\n",
            "DSC_5856_21_599.png\n",
            "DSC_5856_21_614.png\n",
            "DSC_5856_21_617.png\n",
            "DSC_5856_21_618.png\n",
            "DSC_5856_21_621.png\n",
            "DSC_5856_21_628.png\n",
            "DSC_5856_21_640.png\n",
            "DSC_5856_21_642.png\n",
            "DSC_5856_21_650.png\n",
            "DSC_5856_21_654.png\n",
            "DSC_5856_21_656.png\n",
            "DSC_5856_21_660.png\n",
            "DSC_5856_21_667.png\n",
            "DSC_5856_21_671.png\n",
            "DSC_5856_21_673.png\n",
            "DSC_5856_21_675.png\n",
            "DSC_5856_21_684.png\n",
            "DSC_5856_21_686.png\n",
            "DSC_5856_21_687.png\n",
            "DSC_5856_21_694.png\n",
            "DSC_5856_21_709.png\n",
            "DSC_5856_21_727.png\n",
            "DSC_5856_21_730.png\n",
            "DSC_5856_21_734.png\n",
            "DSC_5856_21_737.png\n",
            "DSC_5856_21_740.png\n",
            "DSC_5856_21_760.png\n",
            "DSC_5856_21_766.png\n",
            "DSC_5856_21_771.png\n",
            "DSC_5856_21_782.png\n",
            "DSC_5856_21_783.png\n",
            "DSC_5856_21_786.png\n",
            "DSC_5856_21_789.png\n",
            "DSC_5856_21_791.png\n",
            "DSC_5856_21_792.png\n",
            "DSC_5856_21_793.png\n",
            "DSC_5856_21_804.png\n",
            "DSC_5856_21_824.png\n",
            "DSC_5856_21_831.png\n",
            "DSC_5856_21_838.png\n",
            "DSC_5856_21_840.png\n",
            "DSC_5856_21_843.png\n",
            "DSC_5856_21_845.png\n",
            "DSC_5856_21_848.png\n",
            "DSC_5856_21_850.png\n",
            "DSC_5856_21_853.png\n",
            "DSC_5856_21_863.png\n",
            "DSC_5856_21_872.png\n",
            "DSC_5856_21_875.png\n",
            "DSC_5856_21_898.png\n",
            "DSC_5856_21_900.png\n",
            "DSC_5856_21_902.png\n",
            "DSC_5856_21_907.png\n",
            "DSC_5856_21_909.png\n",
            "DSC_5856_21_919.png\n",
            "DSC_5856_21_929.png\n",
            "DSC_5856_21_931.png\n",
            "DSC_5856_21_939.png\n",
            "DSC_5856_21_941.png\n",
            "DSC_5856_21_946.png\n",
            "DSC_5856_21_953.png\n",
            "DSC_5856_21_963.png\n",
            "DSC_5856_21_970.png\n",
            "DSC_5856_21_993.png\n",
            "DSC_5856_21_999.png\n",
            "DSC_5856_21_1001.png\n",
            "DSC_5856_21_1003.png\n",
            "DSC_5856_21_1006.png\n",
            "DSC_5856_21_1011.png\n",
            "DSC_5856_21_1013.png\n",
            "DSC_5856_21_1025.png\n",
            "DSC_5856_21_1036.png\n",
            "DSC_5856_21_1042.png\n",
            "DSC_5856_21_1045.png\n",
            "DSC_5856_21_1048.png\n",
            "DSC_5856_21_1066.png\n",
            "DSC_5856_21_1070.png\n",
            "DSC_5856_21_1077.png\n",
            "DSC_5856_21_1079.png\n",
            "DSC_5856_21_1085.png\n",
            "DSC_5856_21_1112.png\n",
            "DSC_5856_21_1114.png\n",
            "DSC_5856_21_1118.png\n",
            "DSC_5856_21_1133.png\n",
            "DSC_5856_21_1138.png\n",
            "DSC_5856_21_1141.png\n",
            "DSC_5856_21_1147.png\n",
            "DSC_5856_21_1151.png\n",
            "DSC_5856_21_1155.png\n",
            "DSC_5856_21_1156.png\n",
            "DSC_5856_21_1161.png\n",
            "DSC_5856_21_1168.png\n",
            "DSC_5856_21_1171.png\n",
            "DSC_5856_21_1173.png\n",
            "DSC_5856_21_1176.png\n",
            "DSC_5856_21_1189.png\n",
            "DSC_5856_21_1191.png\n",
            "DSC_5856_21_1201.png\n",
            "DSC_5856_21_1217.png\n",
            "DSC_5856_21_1259.png\n",
            "DSC_5856_21_1264.png\n",
            "DSC_5856_21_1269.png\n",
            "DSC_5856_21_1272.png\n",
            "DSC_5856_21_1276.png\n",
            "DSC_5856_21_1298.png\n",
            "DSC_5856_21_1299.png\n",
            "DSC_5856_21_1303.png\n",
            "DSC_5856_21_1305.png\n",
            "DSC_5856_21_1306.png\n",
            "DSC_5856_21_1310.png\n",
            "DSC_5856_21_1321.png\n",
            "DSC_5856_21_1323.png\n",
            "DSC_5856_21_1333.png\n",
            "DSC_5856_21_1335.png\n",
            "DSC_5856_21_1337.png\n",
            "DSC_5856_21_1339.png\n",
            "DSC_5856_21_1344.png\n",
            "DSC_5856_21_1351.png\n",
            "DSC_5856_21_1361.png\n",
            "DSC_5856_21_1366.png\n",
            "DSC_5856_21_1368.png\n",
            "DSC_5856_21_1369.png\n",
            "DSC_5856_21_1370.png\n",
            "DSC_5856_21_1375.png\n",
            "DSC_5856_21_1377.png\n",
            "DSC_5856_21_1380.png\n",
            "DSC_5856_21_1382.png\n",
            "DSC_5856_21_1383.png\n",
            "DSC_5856_21_1388.png\n",
            "DSC_5856_21_1391.png\n",
            "DSC_5856_21_1396.png\n",
            "DSC_5856_21_1417.png\n",
            "DSC_5856_21_1419.png\n",
            "DSC_5856_21_1430.png\n",
            "DSC_5856_21_1431.png\n",
            "DSC_5856_21_1433.png\n",
            "DSC_5856_21_1435.png\n",
            "DSC_5856_21_1445.png\n",
            "DSC_5856_21_1446.png\n",
            "DSC_5856_21_1457.png\n",
            "DSC_5856_21_1471.png\n",
            "DSC_5856_21_1473.png\n",
            "DSC_5856_21_1481.png\n",
            "DSC_5856_21_1488.png\n",
            "DSC_5856_21_1490.png\n",
            "DSC_5856_21_1493.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        },
        "id": "JZx8TQJc1pZW",
        "outputId": "e9fdb130-8609-4dfc-aa16-cbb1e7809517"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow_hub as hub\r\n",
        "!pip install tensorflow-gpu==2.0.0-beta0\r\n",
        "!pip install tensorflow_hub\r\n",
        "\r\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Increase precision of presented data for better side-by-side comparison\r\n",
        "pd.set_option(\"display.precision\", 8)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/7e/87c4c94686cda7066f52cbca4c344248516490acdd6b258ec6b8a805d956/tensorflow_gpu-2.0.0b0-cp36-cp36m-manylinux1_x86_64.whl (348.8MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.32.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (1.1.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.8.1)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta0) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta0) (3.4.0)\n",
            "Installing collected packages: keras-applications, tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
            "Successfully installed keras-applications-1.0.8 tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b0 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.11.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (53.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFEPbGmO3L4C"
      },
      "source": [
        "data_root='/content/gdrive/MyDrive/colab/class/dataall/datanew'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvZjEQbG399r",
        "outputId": "420d571b-8658-4cc4-a0d3-02a403da3fc0"
      },
      "source": [
        "IMAGE_SHAPE = (224, 224)\r\n",
        "TRAINING_DATA_DIR = str(data_root)\r\n",
        "print(TRAINING_DATA_DIR);\r\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\r\n",
        "#datagen_kwargs_train = dict(rescale=1./255)\r\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\r\n",
        "valid_generator = valid_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"validation\", \r\n",
        "    shuffle=True,\r\n",
        "    target_size=IMAGE_SHAPE\r\n",
        ")\r\n",
        "\r\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"training\", \r\n",
        "    shuffle=True,\r\n",
        "    target_size=IMAGE_SHAPE)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/class/dataall/datanew\n",
            "Found 1289 images belonging to 6 classes.\n",
            "Found 5169 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1zLzKZE4ite",
        "outputId": "c4d69a3f-ea36-4064-b34b-eb9fd46f8215"
      },
      "source": [
        "image_batch_train, label_batch_train = next(iter(train_generator))\r\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\r\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\r\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\r\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\r\n",
        "print(dataset_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image batch shape:  (32, 224, 224, 3)\n",
            "Label batch shape:  (32, 6)\n",
            "['0' '1' '2' '3' '4' '5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jhuD-aw4-EJ",
        "outputId": "7bd7b6d4-471f-44e3-ba0e-1e9cca7c77fd"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \r\n",
        "                 output_shape=[1280],\r\n",
        "                 trainable=False),\r\n",
        "  tf.keras.layers.Dropout(0.4),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "])\r\n",
        "model.build([None, 224, 224, 3])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 7686      \n",
            "=================================================================\n",
            "Total params: 2,265,670\n",
            "Trainable params: 7,686\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qIKmaK25CW3"
      },
      "source": [
        "model.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SULI0wA5GX8",
        "outputId": "156e55ae-faba-4d08-df55-39cad3af4a6b"
      },
      "source": [
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist = model.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5890481510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5890481510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5890481510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 1.8076 - acc: 0.2897WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f583d6f2378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f583d6f2378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f583d6f2378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 25s 92ms/step - loss: 1.8065 - acc: 0.2900 - val_loss: 1.3574 - val_acc: 0.3957\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 1.3906 - acc: 0.4046 - val_loss: 1.2673 - val_acc: 0.4352\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 1.2756 - acc: 0.4521 - val_loss: 1.2850 - val_acc: 0.4383\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.2450 - acc: 0.4623 - val_loss: 1.2509 - val_acc: 0.4414\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1971 - acc: 0.4992 - val_loss: 1.2517 - val_acc: 0.4205\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1762 - acc: 0.4903 - val_loss: 1.2375 - val_acc: 0.4383\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1640 - acc: 0.5028 - val_loss: 1.2127 - val_acc: 0.4562\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1618 - acc: 0.5065 - val_loss: 1.2163 - val_acc: 0.4375\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.1270 - acc: 0.5076 - val_loss: 1.2319 - val_acc: 0.4360\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.1332 - acc: 0.5136 - val_loss: 1.2223 - val_acc: 0.4298\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1336 - acc: 0.5097 - val_loss: 1.2497 - val_acc: 0.4182\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.1086 - acc: 0.5317 - val_loss: 1.2147 - val_acc: 0.4375\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.1280 - acc: 0.5314 - val_loss: 1.2277 - val_acc: 0.4375\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.1046 - acc: 0.5356 - val_loss: 1.2155 - val_acc: 0.4422\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0926 - acc: 0.5480 - val_loss: 1.2005 - val_acc: 0.4756\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0652 - acc: 0.5584 - val_loss: 1.1932 - val_acc: 0.4701\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0707 - acc: 0.5419 - val_loss: 1.2165 - val_acc: 0.4531\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0892 - acc: 0.5380 - val_loss: 1.2454 - val_acc: 0.4337\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0955 - acc: 0.5335 - val_loss: 1.1974 - val_acc: 0.4562\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0674 - acc: 0.5510 - val_loss: 1.2235 - val_acc: 0.4430\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0646 - acc: 0.5540 - val_loss: 1.2332 - val_acc: 0.4492\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0658 - acc: 0.5396 - val_loss: 1.2244 - val_acc: 0.4600\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0867 - acc: 0.5437 - val_loss: 1.2262 - val_acc: 0.4476\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0561 - acc: 0.5610 - val_loss: 1.1855 - val_acc: 0.4655\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0774 - acc: 0.5345 - val_loss: 1.2230 - val_acc: 0.4461\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0743 - acc: 0.5403 - val_loss: 1.2208 - val_acc: 0.4220\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0348 - acc: 0.5522 - val_loss: 1.2382 - val_acc: 0.4158\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0509 - acc: 0.5580 - val_loss: 1.1836 - val_acc: 0.4686\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0602 - acc: 0.5528 - val_loss: 1.1980 - val_acc: 0.4492\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0333 - acc: 0.5599 - val_loss: 1.1921 - val_acc: 0.4531\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0241 - acc: 0.5763 - val_loss: 1.1921 - val_acc: 0.4717\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0373 - acc: 0.5638 - val_loss: 1.2020 - val_acc: 0.4507\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0274 - acc: 0.5607 - val_loss: 1.2082 - val_acc: 0.4492\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 1.0402 - acc: 0.5626 - val_loss: 1.2036 - val_acc: 0.4399\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 1.0343 - acc: 0.5606 - val_loss: 1.2134 - val_acc: 0.4399\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0311 - acc: 0.5614 - val_loss: 1.2077 - val_acc: 0.4399\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 1.0186 - acc: 0.5764 - val_loss: 1.1902 - val_acc: 0.4663\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0245 - acc: 0.5631 - val_loss: 1.1903 - val_acc: 0.4531\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 1.0440 - acc: 0.5628 - val_loss: 1.2362 - val_acc: 0.4158\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0132 - acc: 0.5835 - val_loss: 1.2102 - val_acc: 0.4686\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0160 - acc: 0.5699 - val_loss: 1.2040 - val_acc: 0.4593\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0337 - acc: 0.5675 - val_loss: 1.2367 - val_acc: 0.4407\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 1.0241 - acc: 0.5645 - val_loss: 1.2498 - val_acc: 0.4422\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0196 - acc: 0.5773 - val_loss: 1.2396 - val_acc: 0.4220\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0213 - acc: 0.5726 - val_loss: 1.1982 - val_acc: 0.4461\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0066 - acc: 0.5717 - val_loss: 1.2294 - val_acc: 0.4407\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0291 - acc: 0.5659 - val_loss: 1.2590 - val_acc: 0.4391\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0291 - acc: 0.5728 - val_loss: 1.2085 - val_acc: 0.4554\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9994 - acc: 0.5884 - val_loss: 1.2220 - val_acc: 0.4484\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0042 - acc: 0.5732 - val_loss: 1.2243 - val_acc: 0.4484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcJEKvNN5J4n",
        "outputId": "04ee9bd2-2306-4f17-a554-534191fbc43d"
      },
      "source": [
        "hist1 = model.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\r  1/162 [..............................] - ETA: 17s - loss: 0.9974 - acc: 0.5312"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0203 - acc: 0.5734 - val_loss: 1.2229 - val_acc: 0.4717\n",
            "Epoch 2/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0184 - acc: 0.5775 - val_loss: 1.2146 - val_acc: 0.4577\n",
            "Epoch 3/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0168 - acc: 0.5761 - val_loss: 1.1978 - val_acc: 0.4771\n",
            "Epoch 4/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0112 - acc: 0.5736 - val_loss: 1.2094 - val_acc: 0.4523\n",
            "Epoch 5/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 1.0028 - acc: 0.5779 - val_loss: 1.2173 - val_acc: 0.4531\n",
            "Epoch 6/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0100 - acc: 0.5748 - val_loss: 1.2337 - val_acc: 0.4422\n",
            "Epoch 7/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0039 - acc: 0.5841 - val_loss: 1.2010 - val_acc: 0.4670\n",
            "Epoch 8/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 1.0051 - acc: 0.5794 - val_loss: 1.2359 - val_acc: 0.4407\n",
            "Epoch 9/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0139 - acc: 0.5695 - val_loss: 1.1905 - val_acc: 0.4740\n",
            "Epoch 10/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0032 - acc: 0.5771 - val_loss: 1.2492 - val_acc: 0.4244\n",
            "Epoch 11/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0038 - acc: 0.5771 - val_loss: 1.2040 - val_acc: 0.4461\n",
            "Epoch 12/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0034 - acc: 0.5752 - val_loss: 1.2365 - val_acc: 0.4585\n",
            "Epoch 13/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0028 - acc: 0.5829 - val_loss: 1.2127 - val_acc: 0.4593\n",
            "Epoch 14/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 1.0124 - acc: 0.5771 - val_loss: 1.2145 - val_acc: 0.4515\n",
            "Epoch 15/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0028 - acc: 0.5692 - val_loss: 1.2415 - val_acc: 0.4546\n",
            "Epoch 16/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9907 - acc: 0.5883 - val_loss: 1.2211 - val_acc: 0.4407\n",
            "Epoch 17/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 1.0133 - acc: 0.5723 - val_loss: 1.2188 - val_acc: 0.4414\n",
            "Epoch 18/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 1.0094 - acc: 0.5692 - val_loss: 1.2223 - val_acc: 0.4492\n",
            "Epoch 19/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.9994 - acc: 0.5817 - val_loss: 1.2261 - val_acc: 0.4492\n",
            "Epoch 20/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.9958 - acc: 0.5769 - val_loss: 1.2394 - val_acc: 0.4569\n",
            "Epoch 21/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9946 - acc: 0.5825 - val_loss: 1.2229 - val_acc: 0.4500\n",
            "Epoch 22/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0050 - acc: 0.5676 - val_loss: 1.2117 - val_acc: 0.4631\n",
            "Epoch 23/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9912 - acc: 0.5846 - val_loss: 1.2325 - val_acc: 0.4554\n",
            "Epoch 24/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 1.0067 - acc: 0.5844 - val_loss: 1.2437 - val_acc: 0.4531\n",
            "Epoch 25/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0070 - acc: 0.5860 - val_loss: 1.2326 - val_acc: 0.4748\n",
            "Epoch 26/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9922 - acc: 0.5904 - val_loss: 1.2396 - val_acc: 0.4546\n",
            "Epoch 27/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9901 - acc: 0.5786 - val_loss: 1.2460 - val_acc: 0.4267\n",
            "Epoch 28/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.9904 - acc: 0.5889 - val_loss: 1.2432 - val_acc: 0.4399\n",
            "Epoch 29/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.9957 - acc: 0.5839 - val_loss: 1.2823 - val_acc: 0.4298\n",
            "Epoch 30/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9799 - acc: 0.5906 - val_loss: 1.2155 - val_acc: 0.4500\n",
            "Epoch 31/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9910 - acc: 0.5914 - val_loss: 1.2086 - val_acc: 0.4585\n",
            "Epoch 32/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9847 - acc: 0.5883 - val_loss: 1.1935 - val_acc: 0.4593\n",
            "Epoch 33/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9860 - acc: 0.5786 - val_loss: 1.2401 - val_acc: 0.4321\n",
            "Epoch 34/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9844 - acc: 0.5854 - val_loss: 1.1893 - val_acc: 0.4663\n",
            "Epoch 35/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 1.0014 - acc: 0.5790 - val_loss: 1.2357 - val_acc: 0.4461\n",
            "Epoch 36/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9964 - acc: 0.5817 - val_loss: 1.2513 - val_acc: 0.4507\n",
            "Epoch 37/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9874 - acc: 0.5841 - val_loss: 1.2000 - val_acc: 0.4787\n",
            "Epoch 38/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9949 - acc: 0.5748 - val_loss: 1.2429 - val_acc: 0.4608\n",
            "Epoch 39/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9880 - acc: 0.5848 - val_loss: 1.2417 - val_acc: 0.4461\n",
            "Epoch 40/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9905 - acc: 0.5887 - val_loss: 1.2273 - val_acc: 0.4562\n",
            "Epoch 41/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9767 - acc: 0.5953 - val_loss: 1.2249 - val_acc: 0.4577\n",
            "Epoch 42/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9919 - acc: 0.5881 - val_loss: 1.2577 - val_acc: 0.4438\n",
            "Epoch 43/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9923 - acc: 0.5827 - val_loss: 1.2069 - val_acc: 0.4538\n",
            "Epoch 44/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9819 - acc: 0.5914 - val_loss: 1.2148 - val_acc: 0.4740\n",
            "Epoch 45/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9741 - acc: 0.5904 - val_loss: 1.2424 - val_acc: 0.4492\n",
            "Epoch 46/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9771 - acc: 0.5843 - val_loss: 1.2385 - val_acc: 0.4438\n",
            "Epoch 47/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9988 - acc: 0.5846 - val_loss: 1.2256 - val_acc: 0.4554\n",
            "Epoch 48/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9802 - acc: 0.5844 - val_loss: 1.2090 - val_acc: 0.4577\n",
            "Epoch 49/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9824 - acc: 0.5825 - val_loss: 1.2382 - val_acc: 0.4445\n",
            "Epoch 50/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9759 - acc: 0.5823 - val_loss: 1.2405 - val_acc: 0.4686\n",
            "Epoch 51/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9815 - acc: 0.5881 - val_loss: 1.2686 - val_acc: 0.4407\n",
            "Epoch 52/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9839 - acc: 0.5841 - val_loss: 1.2579 - val_acc: 0.4259\n",
            "Epoch 53/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9763 - acc: 0.5910 - val_loss: 1.2540 - val_acc: 0.4306\n",
            "Epoch 54/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9923 - acc: 0.5868 - val_loss: 1.2415 - val_acc: 0.4352\n",
            "Epoch 55/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9891 - acc: 0.5875 - val_loss: 1.2507 - val_acc: 0.4383\n",
            "Epoch 56/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9883 - acc: 0.5848 - val_loss: 1.2499 - val_acc: 0.4445\n",
            "Epoch 57/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9840 - acc: 0.5862 - val_loss: 1.2255 - val_acc: 0.4585\n",
            "Epoch 58/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9802 - acc: 0.5856 - val_loss: 1.2921 - val_acc: 0.4383\n",
            "Epoch 59/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9853 - acc: 0.5817 - val_loss: 1.3070 - val_acc: 0.4306\n",
            "Epoch 60/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9713 - acc: 0.5920 - val_loss: 1.2125 - val_acc: 0.4484\n",
            "Epoch 61/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 1.0015 - acc: 0.5765 - val_loss: 1.2255 - val_acc: 0.4546\n",
            "Epoch 62/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9877 - acc: 0.5897 - val_loss: 1.2825 - val_acc: 0.4352\n",
            "Epoch 63/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9777 - acc: 0.5904 - val_loss: 1.2314 - val_acc: 0.4476\n",
            "Epoch 64/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9819 - acc: 0.5866 - val_loss: 1.2490 - val_acc: 0.4492\n",
            "Epoch 65/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9763 - acc: 0.5897 - val_loss: 1.2352 - val_acc: 0.4631\n",
            "Epoch 66/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9818 - acc: 0.5935 - val_loss: 1.2639 - val_acc: 0.4593\n",
            "Epoch 67/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9857 - acc: 0.5924 - val_loss: 1.2069 - val_acc: 0.4647\n",
            "Epoch 68/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9807 - acc: 0.5906 - val_loss: 1.2053 - val_acc: 0.4709\n",
            "Epoch 69/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9708 - acc: 0.5908 - val_loss: 1.2881 - val_acc: 0.4422\n",
            "Epoch 70/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9848 - acc: 0.5854 - val_loss: 1.2557 - val_acc: 0.4445\n",
            "Epoch 71/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9800 - acc: 0.5932 - val_loss: 1.2387 - val_acc: 0.4554\n",
            "Epoch 72/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9750 - acc: 0.5860 - val_loss: 1.2885 - val_acc: 0.4329\n",
            "Epoch 73/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9768 - acc: 0.5926 - val_loss: 1.3101 - val_acc: 0.4298\n",
            "Epoch 74/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.9784 - acc: 0.5910 - val_loss: 1.2413 - val_acc: 0.4616\n",
            "Epoch 75/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9645 - acc: 0.6005 - val_loss: 1.2826 - val_acc: 0.4476\n",
            "Epoch 76/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9681 - acc: 0.5966 - val_loss: 1.2584 - val_acc: 0.4383\n",
            "Epoch 77/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.9745 - acc: 0.5959 - val_loss: 1.2267 - val_acc: 0.4593\n",
            "Epoch 78/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9818 - acc: 0.5862 - val_loss: 1.2386 - val_acc: 0.4461\n",
            "Epoch 79/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9903 - acc: 0.5881 - val_loss: 1.2696 - val_acc: 0.4368\n",
            "Epoch 80/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9812 - acc: 0.5872 - val_loss: 1.2641 - val_acc: 0.4461\n",
            "Epoch 81/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9676 - acc: 0.5922 - val_loss: 1.2550 - val_acc: 0.4663\n",
            "Epoch 82/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9653 - acc: 0.6051 - val_loss: 1.2500 - val_acc: 0.4546\n",
            "Epoch 83/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9846 - acc: 0.5850 - val_loss: 1.2590 - val_acc: 0.4383\n",
            "Epoch 84/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9726 - acc: 0.5844 - val_loss: 1.2352 - val_acc: 0.4531\n",
            "Epoch 85/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9807 - acc: 0.5916 - val_loss: 1.2616 - val_acc: 0.4438\n",
            "Epoch 86/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9639 - acc: 0.5980 - val_loss: 1.2297 - val_acc: 0.4484\n",
            "Epoch 87/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9707 - acc: 0.5999 - val_loss: 1.2276 - val_acc: 0.4500\n",
            "Epoch 88/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9590 - acc: 0.6021 - val_loss: 1.2624 - val_acc: 0.4352\n",
            "Epoch 89/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9737 - acc: 0.5988 - val_loss: 1.2341 - val_acc: 0.4399\n",
            "Epoch 90/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9703 - acc: 0.5976 - val_loss: 1.2242 - val_acc: 0.4569\n",
            "Epoch 91/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9727 - acc: 0.5862 - val_loss: 1.2458 - val_acc: 0.4585\n",
            "Epoch 92/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9612 - acc: 0.6013 - val_loss: 1.2372 - val_acc: 0.4500\n",
            "Epoch 93/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9907 - acc: 0.5873 - val_loss: 1.2341 - val_acc: 0.4554\n",
            "Epoch 94/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9702 - acc: 0.5930 - val_loss: 1.2357 - val_acc: 0.4515\n",
            "Epoch 95/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9592 - acc: 0.5959 - val_loss: 1.2469 - val_acc: 0.4422\n",
            "Epoch 96/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9703 - acc: 0.5904 - val_loss: 1.2309 - val_acc: 0.4538\n",
            "Epoch 97/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.9638 - acc: 0.5895 - val_loss: 1.2392 - val_acc: 0.4484\n",
            "Epoch 98/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9679 - acc: 0.5926 - val_loss: 1.2220 - val_acc: 0.4608\n",
            "Epoch 99/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9749 - acc: 0.5951 - val_loss: 1.2437 - val_acc: 0.4631\n",
            "Epoch 100/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9804 - acc: 0.5850 - val_loss: 1.2410 - val_acc: 0.4484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8IqB7hm9Tap",
        "outputId": "16c3441e-2212-420f-bc09-571f0b586089"
      },
      "source": [
        "hist2 = model.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9668 - acc: 0.5982 - val_loss: 1.2765 - val_acc: 0.4127\n",
            "Epoch 2/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9650 - acc: 0.6079 - val_loss: 1.2462 - val_acc: 0.4492\n",
            "Epoch 3/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9655 - acc: 0.5926 - val_loss: 1.2377 - val_acc: 0.4507\n",
            "Epoch 4/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9656 - acc: 0.5916 - val_loss: 1.2595 - val_acc: 0.4391\n",
            "Epoch 5/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9543 - acc: 0.5986 - val_loss: 1.2383 - val_acc: 0.4647\n",
            "Epoch 6/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9679 - acc: 0.5868 - val_loss: 1.2426 - val_acc: 0.4593\n",
            "Epoch 7/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9588 - acc: 0.5988 - val_loss: 1.2633 - val_acc: 0.4344\n",
            "Epoch 8/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9686 - acc: 0.5995 - val_loss: 1.2365 - val_acc: 0.4484\n",
            "Epoch 9/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9626 - acc: 0.5928 - val_loss: 1.2813 - val_acc: 0.4337\n",
            "Epoch 10/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9786 - acc: 0.5908 - val_loss: 1.2393 - val_acc: 0.4538\n",
            "Epoch 11/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9710 - acc: 0.5843 - val_loss: 1.2359 - val_acc: 0.4531\n",
            "Epoch 12/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9624 - acc: 0.5982 - val_loss: 1.2687 - val_acc: 0.4569\n",
            "Epoch 13/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9760 - acc: 0.5889 - val_loss: 1.2437 - val_acc: 0.4500\n",
            "Epoch 14/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9616 - acc: 0.5949 - val_loss: 1.2477 - val_acc: 0.4430\n",
            "Epoch 15/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9607 - acc: 0.5951 - val_loss: 1.2375 - val_acc: 0.4554\n",
            "Epoch 16/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9710 - acc: 0.5945 - val_loss: 1.2422 - val_acc: 0.4562\n",
            "Epoch 17/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9681 - acc: 0.5961 - val_loss: 1.2872 - val_acc: 0.4337\n",
            "Epoch 18/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9605 - acc: 0.6001 - val_loss: 1.2408 - val_acc: 0.4515\n",
            "Epoch 19/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9732 - acc: 0.5941 - val_loss: 1.2420 - val_acc: 0.4624\n",
            "Epoch 20/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9721 - acc: 0.5868 - val_loss: 1.2379 - val_acc: 0.4639\n",
            "Epoch 21/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9636 - acc: 0.6001 - val_loss: 1.2367 - val_acc: 0.4585\n",
            "Epoch 22/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9644 - acc: 0.5990 - val_loss: 1.2134 - val_acc: 0.4694\n",
            "Epoch 23/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9661 - acc: 0.5889 - val_loss: 1.2346 - val_acc: 0.4538\n",
            "Epoch 24/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9674 - acc: 0.5999 - val_loss: 1.2714 - val_acc: 0.4569\n",
            "Epoch 25/100\n",
            "162/162 [==============================] - 13s 79ms/step - loss: 0.9800 - acc: 0.5858 - val_loss: 1.2400 - val_acc: 0.4414\n",
            "Epoch 26/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9526 - acc: 0.5984 - val_loss: 1.2327 - val_acc: 0.4484\n",
            "Epoch 27/100\n",
            "162/162 [==============================] - 13s 79ms/step - loss: 0.9611 - acc: 0.5988 - val_loss: 1.2277 - val_acc: 0.4554\n",
            "Epoch 28/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9631 - acc: 0.6019 - val_loss: 1.2177 - val_acc: 0.4647\n",
            "Epoch 29/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9394 - acc: 0.6129 - val_loss: 1.2674 - val_acc: 0.4321\n",
            "Epoch 30/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9657 - acc: 0.5862 - val_loss: 1.2464 - val_acc: 0.4453\n",
            "Epoch 31/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9631 - acc: 0.6032 - val_loss: 1.2693 - val_acc: 0.4267\n",
            "Epoch 32/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9717 - acc: 0.5914 - val_loss: 1.2116 - val_acc: 0.4678\n",
            "Epoch 33/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9585 - acc: 0.6026 - val_loss: 1.2410 - val_acc: 0.4600\n",
            "Epoch 34/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9417 - acc: 0.6082 - val_loss: 1.2583 - val_acc: 0.4538\n",
            "Epoch 35/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9706 - acc: 0.5893 - val_loss: 1.2445 - val_acc: 0.4554\n",
            "Epoch 36/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9725 - acc: 0.5930 - val_loss: 1.2448 - val_acc: 0.4430\n",
            "Epoch 37/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9571 - acc: 0.6082 - val_loss: 1.2184 - val_acc: 0.4616\n",
            "Epoch 38/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9537 - acc: 0.5997 - val_loss: 1.2277 - val_acc: 0.4531\n",
            "Epoch 39/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9545 - acc: 0.6005 - val_loss: 1.2192 - val_acc: 0.4763\n",
            "Epoch 40/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9638 - acc: 0.5959 - val_loss: 1.2359 - val_acc: 0.4507\n",
            "Epoch 41/100\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.9689 - acc: 0.5924 - val_loss: 1.2532 - val_acc: 0.4484\n",
            "Epoch 42/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9518 - acc: 0.6007 - val_loss: 1.2627 - val_acc: 0.4453\n",
            "Epoch 43/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9569 - acc: 0.6017 - val_loss: 1.2371 - val_acc: 0.4507\n",
            "Epoch 44/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9603 - acc: 0.5901 - val_loss: 1.2455 - val_acc: 0.4438\n",
            "Epoch 45/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9646 - acc: 0.5964 - val_loss: 1.2343 - val_acc: 0.4523\n",
            "Epoch 46/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9637 - acc: 0.6011 - val_loss: 1.2320 - val_acc: 0.4523\n",
            "Epoch 47/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9646 - acc: 0.5910 - val_loss: 1.3292 - val_acc: 0.4577\n",
            "Epoch 48/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9569 - acc: 0.5939 - val_loss: 1.2377 - val_acc: 0.4569\n",
            "Epoch 49/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9719 - acc: 0.5885 - val_loss: 1.2495 - val_acc: 0.4461\n",
            "Epoch 50/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9693 - acc: 0.5920 - val_loss: 1.2471 - val_acc: 0.4593\n",
            "Epoch 51/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9633 - acc: 0.5947 - val_loss: 1.2535 - val_acc: 0.4438\n",
            "Epoch 52/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9755 - acc: 0.5910 - val_loss: 1.2367 - val_acc: 0.4562\n",
            "Epoch 53/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9601 - acc: 0.5928 - val_loss: 1.2608 - val_acc: 0.4445\n",
            "Epoch 54/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9634 - acc: 0.6001 - val_loss: 1.2918 - val_acc: 0.4282\n",
            "Epoch 55/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9551 - acc: 0.6026 - val_loss: 1.2606 - val_acc: 0.4445\n",
            "Epoch 56/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9648 - acc: 0.5912 - val_loss: 1.2974 - val_acc: 0.4321\n",
            "Epoch 57/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9507 - acc: 0.6038 - val_loss: 1.2386 - val_acc: 0.4616\n",
            "Epoch 58/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9640 - acc: 0.6069 - val_loss: 1.2718 - val_acc: 0.4531\n",
            "Epoch 59/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9615 - acc: 0.5953 - val_loss: 1.2644 - val_acc: 0.4453\n",
            "Epoch 60/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9534 - acc: 0.5953 - val_loss: 1.2746 - val_acc: 0.4500\n",
            "Epoch 61/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9626 - acc: 0.5968 - val_loss: 1.2643 - val_acc: 0.4321\n",
            "Epoch 62/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9571 - acc: 0.6011 - val_loss: 1.2767 - val_acc: 0.4313\n",
            "Epoch 63/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.9586 - acc: 0.5947 - val_loss: 1.2735 - val_acc: 0.4375\n",
            "Epoch 64/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9518 - acc: 0.5982 - val_loss: 1.2729 - val_acc: 0.4414\n",
            "Epoch 65/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9489 - acc: 0.6067 - val_loss: 1.2446 - val_acc: 0.4515\n",
            "Epoch 66/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9609 - acc: 0.5980 - val_loss: 1.2815 - val_acc: 0.4329\n",
            "Epoch 67/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9627 - acc: 0.5990 - val_loss: 1.2316 - val_acc: 0.4507\n",
            "Epoch 68/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9545 - acc: 0.5957 - val_loss: 1.3101 - val_acc: 0.4461\n",
            "Epoch 69/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9563 - acc: 0.6050 - val_loss: 1.2946 - val_acc: 0.4546\n",
            "Epoch 70/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9627 - acc: 0.5904 - val_loss: 1.2881 - val_acc: 0.4298\n",
            "Epoch 71/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9541 - acc: 0.6127 - val_loss: 1.3092 - val_acc: 0.4461\n",
            "Epoch 72/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9519 - acc: 0.6036 - val_loss: 1.2663 - val_acc: 0.4515\n",
            "Epoch 73/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9639 - acc: 0.5957 - val_loss: 1.2689 - val_acc: 0.4647\n",
            "Epoch 74/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9615 - acc: 0.5928 - val_loss: 1.2916 - val_acc: 0.4174\n",
            "Epoch 75/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9508 - acc: 0.6067 - val_loss: 1.2851 - val_acc: 0.4453\n",
            "Epoch 76/100\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.9626 - acc: 0.5932 - val_loss: 1.2834 - val_acc: 0.4476\n",
            "Epoch 77/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9462 - acc: 0.6046 - val_loss: 1.2483 - val_acc: 0.4663\n",
            "Epoch 78/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9564 - acc: 0.6051 - val_loss: 1.2511 - val_acc: 0.4531\n",
            "Epoch 79/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9537 - acc: 0.5970 - val_loss: 1.2504 - val_acc: 0.4538\n",
            "Epoch 80/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9616 - acc: 0.6017 - val_loss: 1.2701 - val_acc: 0.4329\n",
            "Epoch 81/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9555 - acc: 0.5993 - val_loss: 1.2794 - val_acc: 0.4337\n",
            "Epoch 82/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9636 - acc: 0.5924 - val_loss: 1.2677 - val_acc: 0.4600\n",
            "Epoch 83/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9549 - acc: 0.5959 - val_loss: 1.2596 - val_acc: 0.4515\n",
            "Epoch 84/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9708 - acc: 0.5904 - val_loss: 1.2892 - val_acc: 0.4476\n",
            "Epoch 85/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9472 - acc: 0.6051 - val_loss: 1.2797 - val_acc: 0.4337\n",
            "Epoch 86/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9634 - acc: 0.5924 - val_loss: 1.2566 - val_acc: 0.4515\n",
            "Epoch 87/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9576 - acc: 0.5984 - val_loss: 1.2538 - val_acc: 0.4414\n",
            "Epoch 88/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9555 - acc: 0.6084 - val_loss: 1.2461 - val_acc: 0.4577\n",
            "Epoch 89/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9471 - acc: 0.6113 - val_loss: 1.2516 - val_acc: 0.4500\n",
            "Epoch 90/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9486 - acc: 0.6026 - val_loss: 1.2382 - val_acc: 0.4616\n",
            "Epoch 91/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9577 - acc: 0.6007 - val_loss: 1.2683 - val_acc: 0.4422\n",
            "Epoch 92/100\n",
            "162/162 [==============================] - 13s 82ms/step - loss: 0.9670 - acc: 0.5924 - val_loss: 1.2676 - val_acc: 0.4500\n",
            "Epoch 93/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9381 - acc: 0.6108 - val_loss: 1.2609 - val_acc: 0.4414\n",
            "Epoch 94/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9593 - acc: 0.5910 - val_loss: 1.2517 - val_acc: 0.4546\n",
            "Epoch 95/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9324 - acc: 0.6096 - val_loss: 1.2676 - val_acc: 0.4484\n",
            "Epoch 96/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9483 - acc: 0.6024 - val_loss: 1.2707 - val_acc: 0.4407\n",
            "Epoch 97/100\n",
            "162/162 [==============================] - 13s 81ms/step - loss: 0.9525 - acc: 0.5935 - val_loss: 1.2478 - val_acc: 0.4538\n",
            "Epoch 98/100\n",
            "162/162 [==============================] - 13s 79ms/step - loss: 0.9489 - acc: 0.6017 - val_loss: 1.2802 - val_acc: 0.4453\n",
            "Epoch 99/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9503 - acc: 0.5999 - val_loss: 1.2585 - val_acc: 0.4321\n",
            "Epoch 100/100\n",
            "162/162 [==============================] - 13s 80ms/step - loss: 0.9636 - acc: 0.5951 - val_loss: 1.2448 - val_acc: 0.4430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWQmKoMVCqom",
        "outputId": "de4b3816-da6e-4cc9-b676-c8575215b1c8"
      },
      "source": [
        "model_3 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/deepmind/ganeval-cifar10-convnet/1\", \r\n",
        "                 output_shape=[1280],\r\n",
        "                 trainable=False),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "])\r\n",
        "model_3.build([None,32,32,3])\r\n",
        "\r\n",
        "model_3.summary()\r\n",
        "model_3.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_3 = model_3.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 10)                7796426   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 66        \n",
            "=================================================================\n",
            "Total params: 7,796,492\n",
            "Trainable params: 66\n",
            "Non-trainable params: 7,796,426\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f583c1ed488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f583c1ed488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f583c1ed488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 3.8079 - acc: 0.2348WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f58323176a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f58323176a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f58323176a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 46s 274ms/step - loss: 3.8035 - acc: 0.2350 - val_loss: 2.0254 - val_acc: 0.2723\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.7343 - acc: 0.3339 - val_loss: 1.6272 - val_acc: 0.2863\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.5586 - acc: 0.3411 - val_loss: 1.5433 - val_acc: 0.3212\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.4867 - acc: 0.3674 - val_loss: 1.5103 - val_acc: 0.3344\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.4598 - acc: 0.3713 - val_loss: 1.4859 - val_acc: 0.3429\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.4342 - acc: 0.3750 - val_loss: 1.4686 - val_acc: 0.3421\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.4224 - acc: 0.3793 - val_loss: 1.4600 - val_acc: 0.3530\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.4104 - acc: 0.3794 - val_loss: 1.4521 - val_acc: 0.3623\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3934 - acc: 0.3971 - val_loss: 1.4533 - val_acc: 0.3561\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3820 - acc: 0.3902 - val_loss: 1.4287 - val_acc: 0.3693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Itj58rHu2d",
        "outputId": "eb32afb2-83cd-484b-d400-0abbd03b8ac1"
      },
      "source": [
        "\r\n",
        "hist_3_1 = model_3.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 43s 266ms/step - loss: 1.3767 - acc: 0.3954 - val_loss: 1.4208 - val_acc: 0.3693\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3670 - acc: 0.4039 - val_loss: 1.4274 - val_acc: 0.3638\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3600 - acc: 0.4092 - val_loss: 1.4089 - val_acc: 0.3732\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3521 - acc: 0.4159 - val_loss: 1.3980 - val_acc: 0.3879\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3451 - acc: 0.4111 - val_loss: 1.4001 - val_acc: 0.3848\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.3394 - acc: 0.4179 - val_loss: 1.4010 - val_acc: 0.3786\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.3336 - acc: 0.4243 - val_loss: 1.3945 - val_acc: 0.3918\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3274 - acc: 0.4295 - val_loss: 1.3829 - val_acc: 0.3926\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3225 - acc: 0.4268 - val_loss: 1.3820 - val_acc: 0.3980\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3176 - acc: 0.4275 - val_loss: 1.3798 - val_acc: 0.3949\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3143 - acc: 0.4279 - val_loss: 1.3743 - val_acc: 0.4011\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.3081 - acc: 0.4322 - val_loss: 1.3836 - val_acc: 0.3980\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.3053 - acc: 0.4366 - val_loss: 1.3653 - val_acc: 0.4073\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.3000 - acc: 0.4370 - val_loss: 1.3712 - val_acc: 0.3964\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2975 - acc: 0.4417 - val_loss: 1.3589 - val_acc: 0.4003\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.2937 - acc: 0.4415 - val_loss: 1.3640 - val_acc: 0.3988\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2907 - acc: 0.4498 - val_loss: 1.3559 - val_acc: 0.3995\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2885 - acc: 0.4403 - val_loss: 1.3413 - val_acc: 0.4065\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2849 - acc: 0.4419 - val_loss: 1.3524 - val_acc: 0.4042\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2825 - acc: 0.4452 - val_loss: 1.3368 - val_acc: 0.4065\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2796 - acc: 0.4438 - val_loss: 1.3449 - val_acc: 0.3988\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2768 - acc: 0.4494 - val_loss: 1.3414 - val_acc: 0.3980\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2743 - acc: 0.4525 - val_loss: 1.3385 - val_acc: 0.4003\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2734 - acc: 0.4513 - val_loss: 1.3386 - val_acc: 0.4011\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2710 - acc: 0.4527 - val_loss: 1.3321 - val_acc: 0.4073\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2682 - acc: 0.4529 - val_loss: 1.3385 - val_acc: 0.4011\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2666 - acc: 0.4498 - val_loss: 1.3280 - val_acc: 0.4034\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2642 - acc: 0.4591 - val_loss: 1.3295 - val_acc: 0.4081\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2621 - acc: 0.4479 - val_loss: 1.3235 - val_acc: 0.4050\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2610 - acc: 0.4556 - val_loss: 1.3215 - val_acc: 0.4088\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2599 - acc: 0.4583 - val_loss: 1.3259 - val_acc: 0.4057\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2575 - acc: 0.4602 - val_loss: 1.3212 - val_acc: 0.4057\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2545 - acc: 0.4597 - val_loss: 1.3152 - val_acc: 0.4104\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2543 - acc: 0.4558 - val_loss: 1.3226 - val_acc: 0.3980\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2536 - acc: 0.4554 - val_loss: 1.3152 - val_acc: 0.4081\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2513 - acc: 0.4571 - val_loss: 1.3074 - val_acc: 0.4104\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2497 - acc: 0.4581 - val_loss: 1.3104 - val_acc: 0.4096\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2490 - acc: 0.4597 - val_loss: 1.3170 - val_acc: 0.4026\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2477 - acc: 0.4564 - val_loss: 1.3095 - val_acc: 0.4112\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2468 - acc: 0.4639 - val_loss: 1.3092 - val_acc: 0.4081\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2462 - acc: 0.4570 - val_loss: 1.3107 - val_acc: 0.4112\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2443 - acc: 0.4620 - val_loss: 1.3060 - val_acc: 0.4127\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2432 - acc: 0.4556 - val_loss: 1.3043 - val_acc: 0.4143\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2427 - acc: 0.4630 - val_loss: 1.3089 - val_acc: 0.4081\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.2415 - acc: 0.4639 - val_loss: 1.3023 - val_acc: 0.4127\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2408 - acc: 0.4614 - val_loss: 1.3092 - val_acc: 0.4158\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2395 - acc: 0.4616 - val_loss: 1.2954 - val_acc: 0.4104\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2384 - acc: 0.4674 - val_loss: 1.3012 - val_acc: 0.4158\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2378 - acc: 0.4620 - val_loss: 1.2896 - val_acc: 0.4166\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2361 - acc: 0.4695 - val_loss: 1.3005 - val_acc: 0.4228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZduxMSGFJZgE",
        "outputId": "c513b3da-2935-4808-dbcf-d28cd762a2f2"
      },
      "source": [
        "\r\n",
        "hist_3_2 = model_3.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 43s 265ms/step - loss: 1.2350 - acc: 0.4655 - val_loss: 1.2908 - val_acc: 0.4251\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 43s 262ms/step - loss: 1.2350 - acc: 0.4662 - val_loss: 1.2920 - val_acc: 0.4197\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2342 - acc: 0.4647 - val_loss: 1.2908 - val_acc: 0.4213\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2334 - acc: 0.4691 - val_loss: 1.2903 - val_acc: 0.4220\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2325 - acc: 0.4653 - val_loss: 1.2869 - val_acc: 0.4119\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2314 - acc: 0.4664 - val_loss: 1.2989 - val_acc: 0.4127\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2317 - acc: 0.4688 - val_loss: 1.2866 - val_acc: 0.4205\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2302 - acc: 0.4688 - val_loss: 1.2883 - val_acc: 0.4182\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2301 - acc: 0.4668 - val_loss: 1.2877 - val_acc: 0.4228\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2290 - acc: 0.4651 - val_loss: 1.2949 - val_acc: 0.4158\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2283 - acc: 0.4662 - val_loss: 1.2858 - val_acc: 0.4151\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2275 - acc: 0.4678 - val_loss: 1.2881 - val_acc: 0.4282\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2268 - acc: 0.4722 - val_loss: 1.2818 - val_acc: 0.4213\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2263 - acc: 0.4748 - val_loss: 1.2813 - val_acc: 0.4182\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2265 - acc: 0.4717 - val_loss: 1.2804 - val_acc: 0.4182\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2257 - acc: 0.4684 - val_loss: 1.2837 - val_acc: 0.4213\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2250 - acc: 0.4728 - val_loss: 1.2905 - val_acc: 0.4112\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2249 - acc: 0.4736 - val_loss: 1.2819 - val_acc: 0.4275\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2238 - acc: 0.4713 - val_loss: 1.2837 - val_acc: 0.4275\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2237 - acc: 0.4709 - val_loss: 1.2735 - val_acc: 0.4220\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2227 - acc: 0.4715 - val_loss: 1.2856 - val_acc: 0.4197\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2233 - acc: 0.4699 - val_loss: 1.2759 - val_acc: 0.4298\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2223 - acc: 0.4724 - val_loss: 1.2762 - val_acc: 0.4259\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2202 - acc: 0.4746 - val_loss: 1.2838 - val_acc: 0.4259\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2215 - acc: 0.4717 - val_loss: 1.2766 - val_acc: 0.4236\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2199 - acc: 0.4707 - val_loss: 1.2812 - val_acc: 0.4244\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2215 - acc: 0.4684 - val_loss: 1.2731 - val_acc: 0.4259\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2191 - acc: 0.4730 - val_loss: 1.2749 - val_acc: 0.4244\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2188 - acc: 0.4778 - val_loss: 1.2776 - val_acc: 0.4275\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2185 - acc: 0.4759 - val_loss: 1.2685 - val_acc: 0.4259\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2180 - acc: 0.4738 - val_loss: 1.2693 - val_acc: 0.4321\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2179 - acc: 0.4753 - val_loss: 1.2697 - val_acc: 0.4282\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2185 - acc: 0.4707 - val_loss: 1.2739 - val_acc: 0.4290\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2177 - acc: 0.4688 - val_loss: 1.2714 - val_acc: 0.4267\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2161 - acc: 0.4755 - val_loss: 1.2727 - val_acc: 0.4267\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2163 - acc: 0.4788 - val_loss: 1.2645 - val_acc: 0.4298\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2165 - acc: 0.4777 - val_loss: 1.2758 - val_acc: 0.4259\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2157 - acc: 0.4728 - val_loss: 1.2675 - val_acc: 0.4259\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2155 - acc: 0.4711 - val_loss: 1.2697 - val_acc: 0.4220\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2145 - acc: 0.4778 - val_loss: 1.2704 - val_acc: 0.4220\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2157 - acc: 0.4771 - val_loss: 1.2645 - val_acc: 0.4337\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2142 - acc: 0.4748 - val_loss: 1.2629 - val_acc: 0.4282\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2139 - acc: 0.4778 - val_loss: 1.2678 - val_acc: 0.4228\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 43s 263ms/step - loss: 1.2139 - acc: 0.4736 - val_loss: 1.2739 - val_acc: 0.4267\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2132 - acc: 0.4736 - val_loss: 1.2657 - val_acc: 0.4321\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2127 - acc: 0.4713 - val_loss: 1.2647 - val_acc: 0.4337\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2127 - acc: 0.4778 - val_loss: 1.2652 - val_acc: 0.4298\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2122 - acc: 0.4730 - val_loss: 1.2626 - val_acc: 0.4375\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2118 - acc: 0.4703 - val_loss: 1.2607 - val_acc: 0.4313\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 43s 264ms/step - loss: 1.2120 - acc: 0.4765 - val_loss: 1.2612 - val_acc: 0.4360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfDhhgyRvo8",
        "outputId": "8c37109e-d189-4ffa-bc1e-0a774579fa9d"
      },
      "source": [
        "IMAGE_SHAPE = (299, 299)\r\n",
        "TRAINING_DATA_DIR = str(data_root)\r\n",
        "print(TRAINING_DATA_DIR);\r\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.30)\r\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\r\n",
        "valid_generator = valid_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"validation\", \r\n",
        "    shuffle=True,\r\n",
        "    target_size=IMAGE_SHAPE\r\n",
        ")\r\n",
        "\r\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"training\", \r\n",
        "    shuffle=True,\r\n",
        "    target_size=IMAGE_SHAPE)\r\n",
        "\r\n",
        "\r\n",
        "model_4 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/inception_v3/classification/4\", \r\n",
        "                 output_shape=[1001],\r\n",
        "                 trainable=False),\r\n",
        "  tf.keras.layers.Dropout(0.4),               \r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "])\r\n",
        "model_4.build([None,299,299,3])\r\n",
        "\r\n",
        "model_4.summary()\r\n",
        "model_4.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_4 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=20,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/class/dataall/datanew\n",
            "Found 1934 images belonging to 6 classes.\n",
            "Found 4524 images belonging to 6 classes.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_2 (KerasLayer)   (None, 1001)              23853833  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1001)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 6012      \n",
            "=================================================================\n",
            "Total params: 23,859,845\n",
            "Trainable params: 6,012\n",
            "Non-trainable params: 23,853,833\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f582e3f5488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f582e3f5488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f582e3f5488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - ETA: 0s - loss: 1.8830 - acc: 0.4087WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f560e125620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f560e125620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f560e125620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - 33s 200ms/step - loss: 1.8809 - acc: 0.4091 - val_loss: 1.0517 - val_acc: 0.5491\n",
            "Epoch 2/20\n",
            "142/142 [==============================] - 27s 187ms/step - loss: 1.2074 - acc: 0.5561 - val_loss: 1.0052 - val_acc: 0.5641\n",
            "Epoch 3/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 1.1127 - acc: 0.5724 - val_loss: 0.9977 - val_acc: 0.5688\n",
            "Epoch 4/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.9987 - acc: 0.6039 - val_loss: 0.9862 - val_acc: 0.5796\n",
            "Epoch 5/20\n",
            "142/142 [==============================] - 27s 187ms/step - loss: 0.9423 - acc: 0.6241 - val_loss: 0.9945 - val_acc: 0.5708\n",
            "Epoch 6/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.9328 - acc: 0.6220 - val_loss: 1.0091 - val_acc: 0.5750\n",
            "Epoch 7/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.9092 - acc: 0.6293 - val_loss: 0.9888 - val_acc: 0.5812\n",
            "Epoch 8/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8994 - acc: 0.6163 - val_loss: 0.9268 - val_acc: 0.5967\n",
            "Epoch 9/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8259 - acc: 0.6507 - val_loss: 0.9815 - val_acc: 0.5822\n",
            "Epoch 10/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8522 - acc: 0.6491 - val_loss: 0.9467 - val_acc: 0.5977\n",
            "Epoch 11/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8303 - acc: 0.6566 - val_loss: 0.9585 - val_acc: 0.5832\n",
            "Epoch 12/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8351 - acc: 0.6393 - val_loss: 0.9683 - val_acc: 0.5895\n",
            "Epoch 13/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8080 - acc: 0.6599 - val_loss: 0.9567 - val_acc: 0.5869\n",
            "Epoch 14/20\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8186 - acc: 0.6571 - val_loss: 0.9799 - val_acc: 0.5967\n",
            "Epoch 15/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7932 - acc: 0.6616 - val_loss: 0.9264 - val_acc: 0.5946\n",
            "Epoch 16/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8158 - acc: 0.6502 - val_loss: 1.0007 - val_acc: 0.5853\n",
            "Epoch 17/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7847 - acc: 0.6611 - val_loss: 1.0147 - val_acc: 0.5683\n",
            "Epoch 18/20\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8030 - acc: 0.6586 - val_loss: 0.9565 - val_acc: 0.6034\n",
            "Epoch 19/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8100 - acc: 0.6606 - val_loss: 0.9511 - val_acc: 0.5982\n",
            "Epoch 20/20\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8032 - acc: 0.6568 - val_loss: 0.9979 - val_acc: 0.5832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbV0UmD9aCiG",
        "outputId": "f168122f-5b0a-4bab-80d2-ec5c21016fd9"
      },
      "source": [
        "hist_4_1 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8205 - acc: 0.6534 - val_loss: 0.9916 - val_acc: 0.5905\n",
            "Epoch 2/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8136 - acc: 0.6589 - val_loss: 0.9559 - val_acc: 0.5931\n",
            "Epoch 3/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8057 - acc: 0.6636 - val_loss: 1.0655 - val_acc: 0.5631\n",
            "Epoch 4/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8258 - acc: 0.6554 - val_loss: 0.9998 - val_acc: 0.5822\n",
            "Epoch 5/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8209 - acc: 0.6556 - val_loss: 1.0266 - val_acc: 0.5765\n",
            "Epoch 6/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8089 - acc: 0.6592 - val_loss: 0.9659 - val_acc: 0.5936\n",
            "Epoch 7/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8016 - acc: 0.6649 - val_loss: 0.9887 - val_acc: 0.5812\n",
            "Epoch 8/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8270 - acc: 0.6594 - val_loss: 1.0445 - val_acc: 0.5698\n",
            "Epoch 9/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8137 - acc: 0.6532 - val_loss: 1.0407 - val_acc: 0.5719\n",
            "Epoch 10/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8099 - acc: 0.6642 - val_loss: 1.0142 - val_acc: 0.5750\n",
            "Epoch 11/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8211 - acc: 0.6488 - val_loss: 0.9744 - val_acc: 0.5807\n",
            "Epoch 12/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8131 - acc: 0.6536 - val_loss: 0.9607 - val_acc: 0.5967\n",
            "Epoch 13/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8172 - acc: 0.6521 - val_loss: 0.9454 - val_acc: 0.6050\n",
            "Epoch 14/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7925 - acc: 0.6616 - val_loss: 0.9931 - val_acc: 0.6013\n",
            "Epoch 15/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7999 - acc: 0.6569 - val_loss: 1.1027 - val_acc: 0.5667\n",
            "Epoch 16/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8174 - acc: 0.6538 - val_loss: 1.1469 - val_acc: 0.5657\n",
            "Epoch 17/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7965 - acc: 0.6662 - val_loss: 0.9615 - val_acc: 0.6060\n",
            "Epoch 18/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7800 - acc: 0.6755 - val_loss: 0.9438 - val_acc: 0.6148\n",
            "Epoch 19/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8217 - acc: 0.6521 - val_loss: 0.9743 - val_acc: 0.5967\n",
            "Epoch 20/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8160 - acc: 0.6589 - val_loss: 0.9859 - val_acc: 0.5863\n",
            "Epoch 21/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8128 - acc: 0.6556 - val_loss: 0.9970 - val_acc: 0.5900\n",
            "Epoch 22/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8085 - acc: 0.6530 - val_loss: 0.9736 - val_acc: 0.5936\n",
            "Epoch 23/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8176 - acc: 0.6585 - val_loss: 0.9800 - val_acc: 0.5962\n",
            "Epoch 24/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7958 - acc: 0.6746 - val_loss: 1.0193 - val_acc: 0.5822\n",
            "Epoch 25/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8050 - acc: 0.6574 - val_loss: 0.9933 - val_acc: 0.5962\n",
            "Epoch 26/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7973 - acc: 0.6592 - val_loss: 0.9976 - val_acc: 0.5895\n",
            "Epoch 27/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8160 - acc: 0.6567 - val_loss: 0.9765 - val_acc: 0.6044\n",
            "Epoch 28/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8190 - acc: 0.6485 - val_loss: 0.9556 - val_acc: 0.6075\n",
            "Epoch 29/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8105 - acc: 0.6611 - val_loss: 1.0718 - val_acc: 0.5739\n",
            "Epoch 30/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8005 - acc: 0.6572 - val_loss: 1.0250 - val_acc: 0.5915\n",
            "Epoch 31/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8108 - acc: 0.6576 - val_loss: 0.9466 - val_acc: 0.6122\n",
            "Epoch 32/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8060 - acc: 0.6614 - val_loss: 1.0177 - val_acc: 0.5967\n",
            "Epoch 33/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8101 - acc: 0.6580 - val_loss: 1.0226 - val_acc: 0.5786\n",
            "Epoch 34/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7901 - acc: 0.6629 - val_loss: 0.9953 - val_acc: 0.6003\n",
            "Epoch 35/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8089 - acc: 0.6589 - val_loss: 1.0162 - val_acc: 0.5853\n",
            "Epoch 36/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7904 - acc: 0.6695 - val_loss: 0.9540 - val_acc: 0.6019\n",
            "Epoch 37/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8025 - acc: 0.6616 - val_loss: 1.0518 - val_acc: 0.5796\n",
            "Epoch 38/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8065 - acc: 0.6636 - val_loss: 0.9673 - val_acc: 0.5920\n",
            "Epoch 39/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8237 - acc: 0.6578 - val_loss: 1.0217 - val_acc: 0.5832\n",
            "Epoch 40/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8051 - acc: 0.6676 - val_loss: 0.9466 - val_acc: 0.6008\n",
            "Epoch 41/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8010 - acc: 0.6616 - val_loss: 1.0008 - val_acc: 0.5843\n",
            "Epoch 42/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7975 - acc: 0.6625 - val_loss: 1.0049 - val_acc: 0.5879\n",
            "Epoch 43/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8171 - acc: 0.6547 - val_loss: 1.0610 - val_acc: 0.5770\n",
            "Epoch 44/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8005 - acc: 0.6629 - val_loss: 1.0578 - val_acc: 0.5776\n",
            "Epoch 45/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8117 - acc: 0.6554 - val_loss: 1.0865 - val_acc: 0.5791\n",
            "Epoch 46/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8242 - acc: 0.6490 - val_loss: 0.9932 - val_acc: 0.5941\n",
            "Epoch 47/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7995 - acc: 0.6620 - val_loss: 1.0083 - val_acc: 0.5910\n",
            "Epoch 48/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8000 - acc: 0.6682 - val_loss: 1.0345 - val_acc: 0.5889\n",
            "Epoch 49/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8099 - acc: 0.6594 - val_loss: 1.0114 - val_acc: 0.5822\n",
            "Epoch 50/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8062 - acc: 0.6585 - val_loss: 1.1114 - val_acc: 0.5724\n",
            "Epoch 51/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8079 - acc: 0.6585 - val_loss: 0.9581 - val_acc: 0.6019\n",
            "Epoch 52/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8199 - acc: 0.6530 - val_loss: 0.9868 - val_acc: 0.5874\n",
            "Epoch 53/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8000 - acc: 0.6689 - val_loss: 1.0102 - val_acc: 0.5951\n",
            "Epoch 54/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7980 - acc: 0.6658 - val_loss: 1.1203 - val_acc: 0.5641\n",
            "Epoch 55/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8074 - acc: 0.6618 - val_loss: 1.0057 - val_acc: 0.5879\n",
            "Epoch 56/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7870 - acc: 0.6642 - val_loss: 0.9909 - val_acc: 0.5936\n",
            "Epoch 57/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8171 - acc: 0.6583 - val_loss: 0.9676 - val_acc: 0.6029\n",
            "Epoch 58/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8003 - acc: 0.6580 - val_loss: 0.9794 - val_acc: 0.6029\n",
            "Epoch 59/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8057 - acc: 0.6536 - val_loss: 0.9489 - val_acc: 0.6189\n",
            "Epoch 60/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8140 - acc: 0.6605 - val_loss: 1.0262 - val_acc: 0.5869\n",
            "Epoch 61/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8052 - acc: 0.6572 - val_loss: 1.0072 - val_acc: 0.5920\n",
            "Epoch 62/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7895 - acc: 0.6660 - val_loss: 0.9705 - val_acc: 0.5967\n",
            "Epoch 63/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8176 - acc: 0.6538 - val_loss: 0.9713 - val_acc: 0.5962\n",
            "Epoch 64/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8144 - acc: 0.6651 - val_loss: 0.9401 - val_acc: 0.5977\n",
            "Epoch 65/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8019 - acc: 0.6638 - val_loss: 0.9745 - val_acc: 0.5962\n",
            "Epoch 66/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8052 - acc: 0.6605 - val_loss: 1.0220 - val_acc: 0.5905\n",
            "Epoch 67/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7940 - acc: 0.6585 - val_loss: 0.9833 - val_acc: 0.5869\n",
            "Epoch 68/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8053 - acc: 0.6607 - val_loss: 1.0316 - val_acc: 0.5863\n",
            "Epoch 69/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8020 - acc: 0.6585 - val_loss: 0.9686 - val_acc: 0.5972\n",
            "Epoch 70/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8063 - acc: 0.6589 - val_loss: 0.9885 - val_acc: 0.5951\n",
            "Epoch 71/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8008 - acc: 0.6569 - val_loss: 1.0376 - val_acc: 0.5822\n",
            "Epoch 72/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8314 - acc: 0.6505 - val_loss: 0.9788 - val_acc: 0.5946\n",
            "Epoch 73/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.7990 - acc: 0.6629 - val_loss: 1.0217 - val_acc: 0.5858\n",
            "Epoch 74/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8166 - acc: 0.6609 - val_loss: 0.9982 - val_acc: 0.6050\n",
            "Epoch 75/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8033 - acc: 0.6627 - val_loss: 1.0054 - val_acc: 0.5982\n",
            "Epoch 76/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8262 - acc: 0.6565 - val_loss: 0.9780 - val_acc: 0.6060\n",
            "Epoch 77/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8099 - acc: 0.6647 - val_loss: 0.9908 - val_acc: 0.6024\n",
            "Epoch 78/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8145 - acc: 0.6598 - val_loss: 0.9681 - val_acc: 0.6086\n",
            "Epoch 79/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8095 - acc: 0.6634 - val_loss: 0.9507 - val_acc: 0.6024\n",
            "Epoch 80/100\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8171 - acc: 0.6541 - val_loss: 0.9877 - val_acc: 0.5905\n",
            "Epoch 81/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8183 - acc: 0.6603 - val_loss: 1.0164 - val_acc: 0.5895\n",
            "Epoch 82/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8055 - acc: 0.6596 - val_loss: 1.0130 - val_acc: 0.5926\n",
            "Epoch 83/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8176 - acc: 0.6603 - val_loss: 0.9809 - val_acc: 0.6070\n",
            "Epoch 84/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8140 - acc: 0.6578 - val_loss: 1.0039 - val_acc: 0.5993\n",
            "Epoch 85/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8128 - acc: 0.6574 - val_loss: 1.0443 - val_acc: 0.5677\n",
            "Epoch 86/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8102 - acc: 0.6561 - val_loss: 1.0090 - val_acc: 0.5791\n",
            "Epoch 87/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8258 - acc: 0.6523 - val_loss: 1.0097 - val_acc: 0.5936\n",
            "Epoch 88/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8010 - acc: 0.6631 - val_loss: 0.9866 - val_acc: 0.5884\n",
            "Epoch 89/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8085 - acc: 0.6660 - val_loss: 1.0140 - val_acc: 0.5817\n",
            "Epoch 90/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.7947 - acc: 0.6607 - val_loss: 0.9743 - val_acc: 0.6024\n",
            "Epoch 91/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7889 - acc: 0.6653 - val_loss: 1.0436 - val_acc: 0.5750\n",
            "Epoch 92/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8185 - acc: 0.6664 - val_loss: 1.0255 - val_acc: 0.5853\n",
            "Epoch 93/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8095 - acc: 0.6640 - val_loss: 0.9611 - val_acc: 0.6075\n",
            "Epoch 94/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8206 - acc: 0.6578 - val_loss: 1.0533 - val_acc: 0.5770\n",
            "Epoch 95/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8096 - acc: 0.6620 - val_loss: 0.9913 - val_acc: 0.6044\n",
            "Epoch 96/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7975 - acc: 0.6625 - val_loss: 0.9622 - val_acc: 0.6081\n",
            "Epoch 97/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7940 - acc: 0.6616 - val_loss: 1.0172 - val_acc: 0.5776\n",
            "Epoch 98/100\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.7954 - acc: 0.6629 - val_loss: 1.0205 - val_acc: 0.5765\n",
            "Epoch 99/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7910 - acc: 0.6660 - val_loss: 1.0131 - val_acc: 0.5838\n",
            "Epoch 100/100\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8097 - acc: 0.6583 - val_loss: 1.0518 - val_acc: 0.5724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5lZr5zScR0r",
        "outputId": "7c84878e-537c-471e-94cb-38657025002a"
      },
      "source": [
        "hist_4_2 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 27s 186ms/step - loss: 0.8054 - acc: 0.6614 - val_loss: 1.0261 - val_acc: 0.5812\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7987 - acc: 0.6594 - val_loss: 1.0308 - val_acc: 0.5853\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8143 - acc: 0.6605 - val_loss: 1.0397 - val_acc: 0.5801\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8086 - acc: 0.6585 - val_loss: 1.0043 - val_acc: 0.5832\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8017 - acc: 0.6622 - val_loss: 0.9834 - val_acc: 0.5931\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8037 - acc: 0.6605 - val_loss: 1.0246 - val_acc: 0.5801\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7929 - acc: 0.6596 - val_loss: 1.0038 - val_acc: 0.5905\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8176 - acc: 0.6574 - val_loss: 0.9842 - val_acc: 0.5977\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8129 - acc: 0.6567 - val_loss: 0.9827 - val_acc: 0.5879\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8097 - acc: 0.6554 - val_loss: 1.0091 - val_acc: 0.5843\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7911 - acc: 0.6724 - val_loss: 0.9934 - val_acc: 0.5972\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8123 - acc: 0.6603 - val_loss: 1.0868 - val_acc: 0.5698\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8204 - acc: 0.6494 - val_loss: 1.0131 - val_acc: 0.5796\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7879 - acc: 0.6609 - val_loss: 0.9783 - val_acc: 0.5957\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8064 - acc: 0.6647 - val_loss: 1.1120 - val_acc: 0.5491\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8191 - acc: 0.6649 - val_loss: 1.0058 - val_acc: 0.5863\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7922 - acc: 0.6645 - val_loss: 0.9849 - val_acc: 0.6003\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8141 - acc: 0.6532 - val_loss: 0.9889 - val_acc: 0.5863\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7960 - acc: 0.6647 - val_loss: 1.0582 - val_acc: 0.5812\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8035 - acc: 0.6625 - val_loss: 0.9721 - val_acc: 0.5941\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8020 - acc: 0.6662 - val_loss: 1.0144 - val_acc: 0.5848\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8107 - acc: 0.6569 - val_loss: 0.9580 - val_acc: 0.6039\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8071 - acc: 0.6616 - val_loss: 1.0170 - val_acc: 0.5900\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8028 - acc: 0.6634 - val_loss: 0.9949 - val_acc: 0.5858\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7953 - acc: 0.6667 - val_loss: 0.9829 - val_acc: 0.5957\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8012 - acc: 0.6682 - val_loss: 1.0173 - val_acc: 0.5905\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8244 - acc: 0.6578 - val_loss: 0.9919 - val_acc: 0.5905\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8130 - acc: 0.6576 - val_loss: 1.0536 - val_acc: 0.5770\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8383 - acc: 0.6541 - val_loss: 0.9587 - val_acc: 0.5931\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8040 - acc: 0.6711 - val_loss: 0.9647 - val_acc: 0.5946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omiwb1PEmUQm",
        "outputId": "ecb62351-163b-4492-a319-87b901ede7bf"
      },
      "source": [
        "hist_4_3 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8110 - acc: 0.6589 - val_loss: 0.9632 - val_acc: 0.6086\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8040 - acc: 0.6622 - val_loss: 1.0003 - val_acc: 0.5920\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 26s 183ms/step - loss: 0.8046 - acc: 0.6563 - val_loss: 1.0443 - val_acc: 0.5895\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7998 - acc: 0.6715 - val_loss: 1.0123 - val_acc: 0.5889\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7925 - acc: 0.6684 - val_loss: 0.9925 - val_acc: 0.5869\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8070 - acc: 0.6616 - val_loss: 1.0677 - val_acc: 0.5776\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8153 - acc: 0.6552 - val_loss: 1.0218 - val_acc: 0.5905\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8147 - acc: 0.6572 - val_loss: 0.9964 - val_acc: 0.5951\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8125 - acc: 0.6636 - val_loss: 1.0819 - val_acc: 0.5755\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8380 - acc: 0.6541 - val_loss: 0.9846 - val_acc: 0.6013\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8095 - acc: 0.6552 - val_loss: 0.9763 - val_acc: 0.6044\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8420 - acc: 0.6561 - val_loss: 0.9837 - val_acc: 0.5988\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8168 - acc: 0.6554 - val_loss: 1.0150 - val_acc: 0.5817\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8037 - acc: 0.6698 - val_loss: 1.0055 - val_acc: 0.5905\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8147 - acc: 0.6516 - val_loss: 0.9842 - val_acc: 0.6003\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7986 - acc: 0.6600 - val_loss: 1.0107 - val_acc: 0.5962\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7984 - acc: 0.6651 - val_loss: 1.1059 - val_acc: 0.5677\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8045 - acc: 0.6709 - val_loss: 1.0503 - val_acc: 0.5760\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8163 - acc: 0.6576 - val_loss: 1.0019 - val_acc: 0.5879\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7961 - acc: 0.6720 - val_loss: 1.0344 - val_acc: 0.5739\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8348 - acc: 0.6554 - val_loss: 1.0186 - val_acc: 0.5957\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7886 - acc: 0.6748 - val_loss: 1.0001 - val_acc: 0.6013\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 27s 187ms/step - loss: 0.7909 - acc: 0.6689 - val_loss: 1.0104 - val_acc: 0.5915\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8136 - acc: 0.6574 - val_loss: 1.0641 - val_acc: 0.5796\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8183 - acc: 0.6625 - val_loss: 1.0547 - val_acc: 0.5832\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8198 - acc: 0.6505 - val_loss: 1.0588 - val_acc: 0.5915\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8023 - acc: 0.6684 - val_loss: 0.9678 - val_acc: 0.6024\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8137 - acc: 0.6605 - val_loss: 0.9691 - val_acc: 0.6024\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8045 - acc: 0.6622 - val_loss: 0.9697 - val_acc: 0.6065\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8015 - acc: 0.6605 - val_loss: 1.0036 - val_acc: 0.5812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6HWWT7RpdAS",
        "outputId": "facf5f9d-1668-4255-d2c5-d225a0d58534"
      },
      "source": [
        "hist_4_4 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.7786 - acc: 0.6729 - val_loss: 1.0138 - val_acc: 0.5931\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7884 - acc: 0.6704 - val_loss: 1.0063 - val_acc: 0.5915\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8064 - acc: 0.6583 - val_loss: 1.0186 - val_acc: 0.5853\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8094 - acc: 0.6532 - val_loss: 1.0744 - val_acc: 0.5745\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8177 - acc: 0.6552 - val_loss: 0.9412 - val_acc: 0.6034\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7956 - acc: 0.6598 - val_loss: 0.9850 - val_acc: 0.5998\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8297 - acc: 0.6501 - val_loss: 0.9912 - val_acc: 0.5982\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8180 - acc: 0.6640 - val_loss: 1.0219 - val_acc: 0.5926\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8224 - acc: 0.6561 - val_loss: 0.9535 - val_acc: 0.6070\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7953 - acc: 0.6698 - val_loss: 1.0126 - val_acc: 0.5863\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7917 - acc: 0.6620 - val_loss: 1.0476 - val_acc: 0.5832\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8184 - acc: 0.6567 - val_loss: 1.0315 - val_acc: 0.5755\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8139 - acc: 0.6620 - val_loss: 0.9679 - val_acc: 0.5967\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8009 - acc: 0.6700 - val_loss: 0.9941 - val_acc: 0.5936\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8114 - acc: 0.6569 - val_loss: 1.0816 - val_acc: 0.5714\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7903 - acc: 0.6569 - val_loss: 1.0406 - val_acc: 0.5874\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8072 - acc: 0.6631 - val_loss: 1.0290 - val_acc: 0.5910\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7984 - acc: 0.6627 - val_loss: 0.9752 - val_acc: 0.5962\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8222 - acc: 0.6580 - val_loss: 1.0584 - val_acc: 0.5776\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8224 - acc: 0.6505 - val_loss: 1.0585 - val_acc: 0.5817\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8174 - acc: 0.6587 - val_loss: 0.9435 - val_acc: 0.6055\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8117 - acc: 0.6638 - val_loss: 1.0437 - val_acc: 0.5879\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7964 - acc: 0.6678 - val_loss: 1.1632 - val_acc: 0.5651\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8103 - acc: 0.6585 - val_loss: 0.9819 - val_acc: 0.5951\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8157 - acc: 0.6627 - val_loss: 1.0051 - val_acc: 0.5889\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8063 - acc: 0.6620 - val_loss: 0.9800 - val_acc: 0.5951\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8075 - acc: 0.6614 - val_loss: 1.0513 - val_acc: 0.5822\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8043 - acc: 0.6689 - val_loss: 1.0118 - val_acc: 0.5900\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8049 - acc: 0.6618 - val_loss: 1.0433 - val_acc: 0.5915\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8082 - acc: 0.6614 - val_loss: 1.0472 - val_acc: 0.5786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAtoGP8VscYB",
        "outputId": "713eb419-3777-4558-8304-020c7af867b8"
      },
      "source": [
        "hist_4_5 = model_4.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8408 - acc: 0.6424 - val_loss: 1.0781 - val_acc: 0.5827\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 26s 186ms/step - loss: 0.8160 - acc: 0.6614 - val_loss: 0.9992 - val_acc: 0.5869\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7947 - acc: 0.6620 - val_loss: 1.0233 - val_acc: 0.5853\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8286 - acc: 0.6435 - val_loss: 0.9877 - val_acc: 0.5946\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8026 - acc: 0.6605 - val_loss: 0.9733 - val_acc: 0.5915\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8316 - acc: 0.6622 - val_loss: 1.0664 - val_acc: 0.5734\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8142 - acc: 0.6592 - val_loss: 0.9891 - val_acc: 0.5920\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8079 - acc: 0.6627 - val_loss: 1.0101 - val_acc: 0.5817\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7929 - acc: 0.6689 - val_loss: 1.0518 - val_acc: 0.5750\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8096 - acc: 0.6519 - val_loss: 1.0711 - val_acc: 0.5750\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7988 - acc: 0.6667 - val_loss: 0.9827 - val_acc: 0.5998\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.7923 - acc: 0.6556 - val_loss: 0.9858 - val_acc: 0.5863\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7940 - acc: 0.6627 - val_loss: 1.0142 - val_acc: 0.5812\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7909 - acc: 0.6636 - val_loss: 1.0843 - val_acc: 0.5688\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8158 - acc: 0.6523 - val_loss: 0.9663 - val_acc: 0.6122\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8115 - acc: 0.6625 - val_loss: 1.0325 - val_acc: 0.5853\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8303 - acc: 0.6587 - val_loss: 0.9921 - val_acc: 0.5900\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8138 - acc: 0.6554 - val_loss: 0.9674 - val_acc: 0.5962\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8028 - acc: 0.6616 - val_loss: 0.9606 - val_acc: 0.5988\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.7869 - acc: 0.6678 - val_loss: 1.0315 - val_acc: 0.5817\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 26s 184ms/step - loss: 0.8363 - acc: 0.6580 - val_loss: 1.0228 - val_acc: 0.5905\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8172 - acc: 0.6485 - val_loss: 1.0307 - val_acc: 0.5858\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8145 - acc: 0.6680 - val_loss: 0.9820 - val_acc: 0.6029\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8290 - acc: 0.6516 - val_loss: 0.9896 - val_acc: 0.5946\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8135 - acc: 0.6587 - val_loss: 1.0831 - val_acc: 0.5719\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8022 - acc: 0.6673 - val_loss: 0.9875 - val_acc: 0.5998\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8360 - acc: 0.6523 - val_loss: 1.0347 - val_acc: 0.5770\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8122 - acc: 0.6651 - val_loss: 1.0505 - val_acc: 0.5812\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8022 - acc: 0.6585 - val_loss: 1.0201 - val_acc: 0.5905\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 26s 185ms/step - loss: 0.8029 - acc: 0.6726 - val_loss: 1.0550 - val_acc: 0.5745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea5Dy5-lvgSj",
        "outputId": "2180005d-c518-4b1c-c732-a5e3cef1d148"
      },
      "source": [
        "model_5 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\"),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "])\r\n",
        "model_5.build([None,224,224,3])\r\n",
        "\r\n",
        "model_5.summary()\r\n",
        "model_5.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_5 = model_5.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_3 (KerasLayer)   (None, 1001)              5327773   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 6012      \n",
            "=================================================================\n",
            "Total params: 5,333,785\n",
            "Trainable params: 6,012\n",
            "Non-trainable params: 5,327,773\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f560ccc19d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f560ccc19d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f560ccc19d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - ETA: 0s - loss: 1.5601 - acc: 0.3659WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56076b4730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56076b4730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56076b4730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - 39s 180ms/step - loss: 1.5584 - acc: 0.3664 - val_loss: 1.2047 - val_acc: 0.4933\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 23s 164ms/step - loss: 1.0857 - acc: 0.5353 - val_loss: 1.0825 - val_acc: 0.5103\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 1.0383 - acc: 0.5512 - val_loss: 1.1121 - val_acc: 0.4902\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.9855 - acc: 0.5673 - val_loss: 1.0692 - val_acc: 0.5171\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.9611 - acc: 0.5980 - val_loss: 1.0509 - val_acc: 0.5321\n",
            "Epoch 6/10\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.9077 - acc: 0.6128 - val_loss: 1.0297 - val_acc: 0.5388\n",
            "Epoch 7/10\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.9150 - acc: 0.6050 - val_loss: 1.1457 - val_acc: 0.5155\n",
            "Epoch 8/10\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.8730 - acc: 0.6300 - val_loss: 1.0692 - val_acc: 0.5222\n",
            "Epoch 9/10\n",
            "142/142 [==============================] - 23s 161ms/step - loss: 0.8939 - acc: 0.6113 - val_loss: 1.0546 - val_acc: 0.5290\n",
            "Epoch 10/10\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8733 - acc: 0.6332 - val_loss: 1.0670 - val_acc: 0.5129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW5GEeceyiF5",
        "outputId": "35bd71a5-c62d-4bfa-ca5f-386470f84000"
      },
      "source": [
        "hist_5_1 = model_5.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 24s 166ms/step - loss: 0.8648 - acc: 0.6324 - val_loss: 1.1261 - val_acc: 0.5010\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8680 - acc: 0.6291 - val_loss: 1.1083 - val_acc: 0.4845\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.8624 - acc: 0.6333 - val_loss: 1.1175 - val_acc: 0.5377\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8523 - acc: 0.6375 - val_loss: 1.0485 - val_acc: 0.5346\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 23s 164ms/step - loss: 0.8473 - acc: 0.6399 - val_loss: 1.0921 - val_acc: 0.5186\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8418 - acc: 0.6466 - val_loss: 1.0587 - val_acc: 0.5357\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8209 - acc: 0.6600 - val_loss: 1.0627 - val_acc: 0.5341\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8360 - acc: 0.6454 - val_loss: 1.0806 - val_acc: 0.5274\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8349 - acc: 0.6443 - val_loss: 1.0861 - val_acc: 0.5134\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8116 - acc: 0.6587 - val_loss: 1.0681 - val_acc: 0.5424\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7991 - acc: 0.6660 - val_loss: 1.0827 - val_acc: 0.5274\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8110 - acc: 0.6572 - val_loss: 1.0372 - val_acc: 0.5357\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7981 - acc: 0.6664 - val_loss: 1.0596 - val_acc: 0.5377\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.8063 - acc: 0.6620 - val_loss: 1.1068 - val_acc: 0.5145\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.8009 - acc: 0.6614 - val_loss: 1.1181 - val_acc: 0.5207\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7900 - acc: 0.6649 - val_loss: 1.0848 - val_acc: 0.5150\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7947 - acc: 0.6678 - val_loss: 1.0819 - val_acc: 0.5207\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7782 - acc: 0.6700 - val_loss: 1.0692 - val_acc: 0.5315\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7899 - acc: 0.6704 - val_loss: 1.1543 - val_acc: 0.5010\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7813 - acc: 0.6733 - val_loss: 1.0946 - val_acc: 0.5248\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7860 - acc: 0.6662 - val_loss: 1.1540 - val_acc: 0.4938\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7881 - acc: 0.6691 - val_loss: 1.1380 - val_acc: 0.4969\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7744 - acc: 0.6768 - val_loss: 1.0976 - val_acc: 0.5346\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7695 - acc: 0.6742 - val_loss: 1.0782 - val_acc: 0.5357\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7653 - acc: 0.6852 - val_loss: 1.0923 - val_acc: 0.5238\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7629 - acc: 0.6866 - val_loss: 1.0776 - val_acc: 0.5300\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7825 - acc: 0.6642 - val_loss: 1.1017 - val_acc: 0.5352\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7675 - acc: 0.6755 - val_loss: 1.0580 - val_acc: 0.5300\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7563 - acc: 0.6857 - val_loss: 1.1718 - val_acc: 0.4979\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7638 - acc: 0.6832 - val_loss: 1.0588 - val_acc: 0.5408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTorBMHjznXH",
        "outputId": "ef2f432c-2910-4692-cdaa-c09fb669df62"
      },
      "source": [
        "hist_5_2 = model_5.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "142/142 [==============================] - 23s 164ms/step - loss: 0.7751 - acc: 0.6753 - val_loss: 1.1729 - val_acc: 0.5217\n",
            "Epoch 2/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7629 - acc: 0.6810 - val_loss: 1.1942 - val_acc: 0.5145\n",
            "Epoch 3/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7508 - acc: 0.6802 - val_loss: 1.0813 - val_acc: 0.5491\n",
            "Epoch 4/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7468 - acc: 0.6916 - val_loss: 1.0740 - val_acc: 0.5403\n",
            "Epoch 5/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7577 - acc: 0.6788 - val_loss: 1.0958 - val_acc: 0.5357\n",
            "Epoch 6/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7467 - acc: 0.6936 - val_loss: 1.1153 - val_acc: 0.5062\n",
            "Epoch 7/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7559 - acc: 0.6773 - val_loss: 1.0759 - val_acc: 0.5388\n",
            "Epoch 8/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7462 - acc: 0.6939 - val_loss: 1.0914 - val_acc: 0.5377\n",
            "Epoch 9/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7430 - acc: 0.6947 - val_loss: 1.1506 - val_acc: 0.5083\n",
            "Epoch 10/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7520 - acc: 0.6852 - val_loss: 1.1507 - val_acc: 0.5233\n",
            "Epoch 11/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7545 - acc: 0.6828 - val_loss: 1.0744 - val_acc: 0.5243\n",
            "Epoch 12/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7526 - acc: 0.6819 - val_loss: 1.1356 - val_acc: 0.5326\n",
            "Epoch 13/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7331 - acc: 0.6919 - val_loss: 1.0909 - val_acc: 0.5357\n",
            "Epoch 14/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7405 - acc: 0.6950 - val_loss: 1.1332 - val_acc: 0.5057\n",
            "Epoch 15/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7361 - acc: 0.6936 - val_loss: 1.1395 - val_acc: 0.5300\n",
            "Epoch 16/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7432 - acc: 0.6888 - val_loss: 1.1128 - val_acc: 0.5321\n",
            "Epoch 17/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7249 - acc: 0.7038 - val_loss: 1.0828 - val_acc: 0.5383\n",
            "Epoch 18/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7270 - acc: 0.7034 - val_loss: 1.0949 - val_acc: 0.5284\n",
            "Epoch 19/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7310 - acc: 0.6985 - val_loss: 1.1345 - val_acc: 0.5212\n",
            "Epoch 20/30\n",
            "142/142 [==============================] - 23s 164ms/step - loss: 0.7237 - acc: 0.7056 - val_loss: 1.1942 - val_acc: 0.5088\n",
            "Epoch 21/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7250 - acc: 0.7003 - val_loss: 1.0827 - val_acc: 0.5403\n",
            "Epoch 22/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7338 - acc: 0.6947 - val_loss: 1.1533 - val_acc: 0.5181\n",
            "Epoch 23/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7350 - acc: 0.6958 - val_loss: 1.0913 - val_acc: 0.5217\n",
            "Epoch 24/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7266 - acc: 0.7000 - val_loss: 1.0687 - val_acc: 0.5367\n",
            "Epoch 25/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7388 - acc: 0.6969 - val_loss: 1.1257 - val_acc: 0.5150\n",
            "Epoch 26/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7195 - acc: 0.7067 - val_loss: 1.1420 - val_acc: 0.5140\n",
            "Epoch 27/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7211 - acc: 0.7009 - val_loss: 1.0959 - val_acc: 0.5284\n",
            "Epoch 28/30\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7176 - acc: 0.7045 - val_loss: 1.2459 - val_acc: 0.4835\n",
            "Epoch 29/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7190 - acc: 0.6983 - val_loss: 1.1532 - val_acc: 0.5222\n",
            "Epoch 30/30\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7289 - acc: 0.6947 - val_loss: 1.0935 - val_acc: 0.5321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLQqIF3p2TLK",
        "outputId": "59e25317-e044-4f04-b5f7-dfe8b42648e6"
      },
      "source": [
        "import yaml\r\n",
        "import json\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_5.to_yaml()\r\n",
        "open('model_architecture_choosed.yaml', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_5.save_weights('model_weights_choosed.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMxOJamj5SrM"
      },
      "source": [
        "import yaml\r\n",
        "import json\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import model_from_yaml\r\n",
        "\r\n",
        "# 加载模型结构\r\n",
        "model00 = model_from_yaml(open('model_architecture_choosed.yaml').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "# 加载模型参数\r\n",
        "model00.load_weights('model_weights_choosed.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR8jJ85s5e8z",
        "outputId": "805df67f-004c-412b-8356-d091ec3f3c26"
      },
      "source": [
        "model00.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=1,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5609ccd400> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5609ccd400> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f5609ccd400> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - ETA: 0s - loss: 0.7348 - acc: 0.6906WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56043c6730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56043c6730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f56043c6730> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "142/142 [==============================] - 37s 177ms/step - loss: 0.7347 - acc: 0.6906 - val_loss: 1.1259 - val_acc: 0.5243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhPi8Ojd5k47",
        "outputId": "25c4d788-a703-4f25-88a8-5d3e7b03a1ad"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=3,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "142/142 [==============================] - 23s 164ms/step - loss: 0.7234 - acc: 0.6978 - val_loss: 1.0903 - val_acc: 0.5465\n",
            "Epoch 2/3\n",
            "142/142 [==============================] - 23s 163ms/step - loss: 0.7269 - acc: 0.7011 - val_loss: 1.1315 - val_acc: 0.5331\n",
            "Epoch 3/3\n",
            "142/142 [==============================] - 23s 162ms/step - loss: 0.7166 - acc: 0.6963 - val_loss: 1.1187 - val_acc: 0.5171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urEKs7rP5uwU",
        "outputId": "66db1dd8-ff50-47ce-e0da-c5b92a94966e"
      },
      "source": [
        "import yaml\r\n",
        "import json\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import model_from_yaml\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "# 加载模型结构\r\n",
        "model00 = model_from_yaml(open('model_architecture_choosed.yaml').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "# 加载模型参数\r\n",
        "model00.load_weights('model_weights_choosed.h5')\r\n",
        "model00.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=1,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39abeb69d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39abeb69d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39abeb69d8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 1.4284 - acc: 0.5338WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39b5734378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39b5734378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39b5734378> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 1811s 11s/step - loss: 1.4276 - acc: 0.5340 - val_loss: 1.4530 - val_acc: 0.5128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoKoI3pEU3EE",
        "outputId": "9f95e830-dc8c-43fb-9c05-116bcbc85f33"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 1.1073 - acc: 0.6005 - val_loss: 1.2979 - val_acc: 0.5198\n",
            "Epoch 2/100\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.0178 - acc: 0.6164 - val_loss: 1.5132 - val_acc: 0.4926\n",
            "Epoch 3/100\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9799 - acc: 0.6195 - val_loss: 1.2451 - val_acc: 0.5415\n",
            "Epoch 4/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.9400 - acc: 0.6274 - val_loss: 1.3585 - val_acc: 0.5159\n",
            "Epoch 5/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.9084 - acc: 0.6311 - val_loss: 1.2141 - val_acc: 0.5462\n",
            "Epoch 6/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.8832 - acc: 0.6460 - val_loss: 1.1807 - val_acc: 0.5539\n",
            "Epoch 7/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.8584 - acc: 0.6527 - val_loss: 1.1748 - val_acc: 0.5454\n",
            "Epoch 8/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.8358 - acc: 0.6605 - val_loss: 1.1666 - val_acc: 0.5462\n",
            "Epoch 9/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.8326 - acc: 0.6597 - val_loss: 1.1995 - val_acc: 0.5485\n",
            "Epoch 10/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.8257 - acc: 0.6624 - val_loss: 1.1868 - val_acc: 0.5500\n",
            "Epoch 11/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.8148 - acc: 0.6636 - val_loss: 1.1941 - val_acc: 0.5454\n",
            "Epoch 12/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7992 - acc: 0.6715 - val_loss: 1.1010 - val_acc: 0.5632\n",
            "Epoch 13/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7945 - acc: 0.6680 - val_loss: 1.1039 - val_acc: 0.5749\n",
            "Epoch 14/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7749 - acc: 0.6790 - val_loss: 1.1164 - val_acc: 0.5656\n",
            "Epoch 15/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7674 - acc: 0.6763 - val_loss: 1.1179 - val_acc: 0.5578\n",
            "Epoch 16/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.7566 - acc: 0.6823 - val_loss: 1.1284 - val_acc: 0.5617\n",
            "Epoch 17/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7568 - acc: 0.6870 - val_loss: 1.1510 - val_acc: 0.5454\n",
            "Epoch 18/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7498 - acc: 0.6860 - val_loss: 1.0798 - val_acc: 0.5640\n",
            "Epoch 19/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.7618 - acc: 0.6806 - val_loss: 1.1405 - val_acc: 0.5508\n",
            "Epoch 20/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7382 - acc: 0.6879 - val_loss: 1.1920 - val_acc: 0.5454\n",
            "Epoch 21/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7467 - acc: 0.6845 - val_loss: 1.2532 - val_acc: 0.5446\n",
            "Epoch 22/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7372 - acc: 0.6887 - val_loss: 1.1291 - val_acc: 0.5508\n",
            "Epoch 23/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7405 - acc: 0.6845 - val_loss: 1.2391 - val_acc: 0.5454\n",
            "Epoch 24/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7333 - acc: 0.6957 - val_loss: 1.0802 - val_acc: 0.5570\n",
            "Epoch 25/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7165 - acc: 0.6961 - val_loss: 1.0888 - val_acc: 0.5555\n",
            "Epoch 26/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7324 - acc: 0.6928 - val_loss: 1.0955 - val_acc: 0.5555\n",
            "Epoch 27/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7197 - acc: 0.6932 - val_loss: 1.1393 - val_acc: 0.5593\n",
            "Epoch 28/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7249 - acc: 0.6893 - val_loss: 1.1163 - val_acc: 0.5586\n",
            "Epoch 29/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6963 - acc: 0.7077 - val_loss: 1.0720 - val_acc: 0.5772\n",
            "Epoch 30/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7267 - acc: 0.6916 - val_loss: 1.1630 - val_acc: 0.5516\n",
            "Epoch 31/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.7044 - acc: 0.7059 - val_loss: 1.0625 - val_acc: 0.5795\n",
            "Epoch 32/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6985 - acc: 0.7038 - val_loss: 1.1161 - val_acc: 0.5640\n",
            "Epoch 33/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.7027 - acc: 0.7017 - val_loss: 1.1504 - val_acc: 0.5508\n",
            "Epoch 34/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6941 - acc: 0.7085 - val_loss: 1.0959 - val_acc: 0.5772\n",
            "Epoch 35/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6899 - acc: 0.7137 - val_loss: 1.1639 - val_acc: 0.5625\n",
            "Epoch 36/100\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.7114 - acc: 0.6978 - val_loss: 1.1522 - val_acc: 0.5656\n",
            "Epoch 37/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.7069 - acc: 0.7019 - val_loss: 1.1741 - val_acc: 0.5446\n",
            "Epoch 38/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6969 - acc: 0.7121 - val_loss: 1.1417 - val_acc: 0.5671\n",
            "Epoch 39/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6842 - acc: 0.7137 - val_loss: 1.0948 - val_acc: 0.5625\n",
            "Epoch 40/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6903 - acc: 0.7090 - val_loss: 1.0818 - val_acc: 0.5780\n",
            "Epoch 41/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6893 - acc: 0.7050 - val_loss: 1.1315 - val_acc: 0.5539\n",
            "Epoch 42/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6870 - acc: 0.7081 - val_loss: 1.0793 - val_acc: 0.5656\n",
            "Epoch 43/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6765 - acc: 0.7129 - val_loss: 1.0874 - val_acc: 0.5687\n",
            "Epoch 44/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6748 - acc: 0.7133 - val_loss: 1.0448 - val_acc: 0.5702\n",
            "Epoch 45/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6777 - acc: 0.7114 - val_loss: 1.0589 - val_acc: 0.5663\n",
            "Epoch 46/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6729 - acc: 0.7129 - val_loss: 1.0932 - val_acc: 0.5749\n",
            "Epoch 47/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6692 - acc: 0.7156 - val_loss: 1.1317 - val_acc: 0.5663\n",
            "Epoch 48/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6907 - acc: 0.7032 - val_loss: 1.1716 - val_acc: 0.5609\n",
            "Epoch 49/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6741 - acc: 0.7143 - val_loss: 1.1171 - val_acc: 0.5780\n",
            "Epoch 50/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6685 - acc: 0.7185 - val_loss: 1.0896 - val_acc: 0.5632\n",
            "Epoch 51/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6657 - acc: 0.7094 - val_loss: 1.1204 - val_acc: 0.5687\n",
            "Epoch 52/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6729 - acc: 0.7224 - val_loss: 1.1149 - val_acc: 0.5648\n",
            "Epoch 53/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6736 - acc: 0.7133 - val_loss: 1.0688 - val_acc: 0.5679\n",
            "Epoch 54/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6682 - acc: 0.7216 - val_loss: 1.0818 - val_acc: 0.5702\n",
            "Epoch 55/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6610 - acc: 0.7189 - val_loss: 1.0642 - val_acc: 0.5764\n",
            "Epoch 56/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.6658 - acc: 0.7212 - val_loss: 1.0793 - val_acc: 0.5710\n",
            "Epoch 57/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6586 - acc: 0.7251 - val_loss: 1.1411 - val_acc: 0.5609\n",
            "Epoch 58/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6559 - acc: 0.7210 - val_loss: 1.0895 - val_acc: 0.5625\n",
            "Epoch 59/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6513 - acc: 0.7286 - val_loss: 1.1325 - val_acc: 0.5555\n",
            "Epoch 60/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.6591 - acc: 0.7234 - val_loss: 1.0837 - val_acc: 0.5811\n",
            "Epoch 61/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.6552 - acc: 0.7243 - val_loss: 1.1008 - val_acc: 0.5679\n",
            "Epoch 62/100\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.6584 - acc: 0.7270 - val_loss: 1.1638 - val_acc: 0.5524\n",
            "Epoch 63/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6444 - acc: 0.7301 - val_loss: 1.1141 - val_acc: 0.5663\n",
            "Epoch 64/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6374 - acc: 0.7332 - val_loss: 1.1083 - val_acc: 0.5687\n",
            "Epoch 65/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6549 - acc: 0.7272 - val_loss: 1.1308 - val_acc: 0.5725\n",
            "Epoch 66/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6540 - acc: 0.7216 - val_loss: 1.1355 - val_acc: 0.5562\n",
            "Epoch 67/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6533 - acc: 0.7230 - val_loss: 1.1337 - val_acc: 0.5749\n",
            "Epoch 68/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6482 - acc: 0.7309 - val_loss: 1.1219 - val_acc: 0.5625\n",
            "Epoch 69/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6467 - acc: 0.7321 - val_loss: 1.1680 - val_acc: 0.5562\n",
            "Epoch 70/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6395 - acc: 0.7293 - val_loss: 1.1571 - val_acc: 0.5477\n",
            "Epoch 71/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6412 - acc: 0.7292 - val_loss: 1.0987 - val_acc: 0.5671\n",
            "Epoch 72/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6377 - acc: 0.7355 - val_loss: 1.1619 - val_acc: 0.5570\n",
            "Epoch 73/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6306 - acc: 0.7415 - val_loss: 1.0867 - val_acc: 0.5679\n",
            "Epoch 74/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6466 - acc: 0.7264 - val_loss: 1.1022 - val_acc: 0.5609\n",
            "Epoch 75/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6365 - acc: 0.7309 - val_loss: 1.0796 - val_acc: 0.5640\n",
            "Epoch 76/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6383 - acc: 0.7268 - val_loss: 1.0458 - val_acc: 0.5904\n",
            "Epoch 77/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6470 - acc: 0.7301 - val_loss: 1.1821 - val_acc: 0.5539\n",
            "Epoch 78/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6342 - acc: 0.7344 - val_loss: 1.0696 - val_acc: 0.5679\n",
            "Epoch 79/100\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6318 - acc: 0.7350 - val_loss: 1.0870 - val_acc: 0.5702\n",
            "Epoch 80/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6327 - acc: 0.7301 - val_loss: 1.1315 - val_acc: 0.5539\n",
            "Epoch 81/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6513 - acc: 0.7199 - val_loss: 1.1330 - val_acc: 0.5656\n",
            "Epoch 82/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6361 - acc: 0.7352 - val_loss: 1.1328 - val_acc: 0.5702\n",
            "Epoch 83/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6374 - acc: 0.7357 - val_loss: 1.1774 - val_acc: 0.5547\n",
            "Epoch 84/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6397 - acc: 0.7363 - val_loss: 1.1062 - val_acc: 0.5648\n",
            "Epoch 85/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6335 - acc: 0.7313 - val_loss: 1.1094 - val_acc: 0.5764\n",
            "Epoch 86/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6365 - acc: 0.7373 - val_loss: 1.1153 - val_acc: 0.5702\n",
            "Epoch 87/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6342 - acc: 0.7350 - val_loss: 1.0902 - val_acc: 0.5702\n",
            "Epoch 88/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6308 - acc: 0.7390 - val_loss: 1.1041 - val_acc: 0.5741\n",
            "Epoch 89/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6350 - acc: 0.7319 - val_loss: 1.1239 - val_acc: 0.5593\n",
            "Epoch 90/100\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.6298 - acc: 0.7338 - val_loss: 1.1918 - val_acc: 0.5578\n",
            "Epoch 91/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6348 - acc: 0.7415 - val_loss: 1.1537 - val_acc: 0.5632\n",
            "Epoch 92/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6307 - acc: 0.7309 - val_loss: 1.1483 - val_acc: 0.5531\n",
            "Epoch 93/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6352 - acc: 0.7361 - val_loss: 1.1100 - val_acc: 0.5787\n",
            "Epoch 94/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6352 - acc: 0.7371 - val_loss: 1.1847 - val_acc: 0.5632\n",
            "Epoch 95/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6234 - acc: 0.7411 - val_loss: 1.1830 - val_acc: 0.5586\n",
            "Epoch 96/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6490 - acc: 0.7288 - val_loss: 1.1163 - val_acc: 0.5671\n",
            "Epoch 97/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6247 - acc: 0.7396 - val_loss: 1.2380 - val_acc: 0.5469\n",
            "Epoch 98/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6175 - acc: 0.7433 - val_loss: 1.1345 - val_acc: 0.5671\n",
            "Epoch 99/100\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6189 - acc: 0.7377 - val_loss: 1.2655 - val_acc: 0.5283\n",
            "Epoch 100/100\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6283 - acc: 0.7369 - val_loss: 1.1745 - val_acc: 0.5570\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy23KSlddU4B",
        "outputId": "19b4fe36-22e0-461d-c2ae-bf5fc90f86be"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6095 - acc: 0.7439 - val_loss: 1.1744 - val_acc: 0.5516\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6205 - acc: 0.7359 - val_loss: 1.1435 - val_acc: 0.5640\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6140 - acc: 0.7448 - val_loss: 1.1264 - val_acc: 0.5749\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6216 - acc: 0.7425 - val_loss: 1.1443 - val_acc: 0.5632\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6386 - acc: 0.7332 - val_loss: 1.1668 - val_acc: 0.5415\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6163 - acc: 0.7456 - val_loss: 1.1650 - val_acc: 0.5609\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6229 - acc: 0.7419 - val_loss: 1.1554 - val_acc: 0.5718\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6104 - acc: 0.7483 - val_loss: 1.2081 - val_acc: 0.5469\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6364 - acc: 0.7350 - val_loss: 1.1914 - val_acc: 0.5485\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6162 - acc: 0.7398 - val_loss: 1.1838 - val_acc: 0.5531\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6059 - acc: 0.7456 - val_loss: 1.1646 - val_acc: 0.5640\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6260 - acc: 0.7363 - val_loss: 1.1562 - val_acc: 0.5687\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6184 - acc: 0.7425 - val_loss: 1.1672 - val_acc: 0.5593\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6078 - acc: 0.7487 - val_loss: 1.1647 - val_acc: 0.5539\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6154 - acc: 0.7450 - val_loss: 1.1147 - val_acc: 0.5764\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6136 - acc: 0.7485 - val_loss: 1.0778 - val_acc: 0.5803\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6075 - acc: 0.7415 - val_loss: 1.1054 - val_acc: 0.5803\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6072 - acc: 0.7410 - val_loss: 1.1123 - val_acc: 0.5741\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5941 - acc: 0.7514 - val_loss: 1.0986 - val_acc: 0.5795\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6031 - acc: 0.7502 - val_loss: 1.1297 - val_acc: 0.5656\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6187 - acc: 0.7411 - val_loss: 1.1209 - val_acc: 0.5849\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5932 - acc: 0.7570 - val_loss: 1.1415 - val_acc: 0.5725\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6258 - acc: 0.7439 - val_loss: 1.1187 - val_acc: 0.5795\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6095 - acc: 0.7425 - val_loss: 1.1091 - val_acc: 0.5803\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6153 - acc: 0.7439 - val_loss: 1.1551 - val_acc: 0.5663\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.6102 - acc: 0.7386 - val_loss: 1.2298 - val_acc: 0.5361\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6145 - acc: 0.7425 - val_loss: 1.0943 - val_acc: 0.5834\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5970 - acc: 0.7537 - val_loss: 1.2035 - val_acc: 0.5524\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6064 - acc: 0.7551 - val_loss: 1.1390 - val_acc: 0.5578\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6059 - acc: 0.7444 - val_loss: 1.1131 - val_acc: 0.5919\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5953 - acc: 0.7570 - val_loss: 1.1620 - val_acc: 0.5710\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6079 - acc: 0.7448 - val_loss: 1.1624 - val_acc: 0.5663\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5981 - acc: 0.7460 - val_loss: 1.1390 - val_acc: 0.5625\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5954 - acc: 0.7526 - val_loss: 1.1006 - val_acc: 0.5733\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6062 - acc: 0.7471 - val_loss: 1.1704 - val_acc: 0.5640\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6004 - acc: 0.7471 - val_loss: 1.1541 - val_acc: 0.5663\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5979 - acc: 0.7593 - val_loss: 1.1171 - val_acc: 0.5803\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5991 - acc: 0.7454 - val_loss: 1.1478 - val_acc: 0.5795\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5967 - acc: 0.7504 - val_loss: 1.1671 - val_acc: 0.5601\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6039 - acc: 0.7526 - val_loss: 1.1748 - val_acc: 0.5570\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5971 - acc: 0.7526 - val_loss: 1.1433 - val_acc: 0.5826\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.6133 - acc: 0.7379 - val_loss: 1.1339 - val_acc: 0.5656\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5908 - acc: 0.7516 - val_loss: 1.1320 - val_acc: 0.5656\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5988 - acc: 0.7475 - val_loss: 1.2493 - val_acc: 0.5446\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5956 - acc: 0.7518 - val_loss: 1.0966 - val_acc: 0.5756\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5920 - acc: 0.7543 - val_loss: 1.1582 - val_acc: 0.5547\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5910 - acc: 0.7514 - val_loss: 1.1928 - val_acc: 0.5562\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.6030 - acc: 0.7450 - val_loss: 1.1756 - val_acc: 0.5687\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6007 - acc: 0.7452 - val_loss: 1.1486 - val_acc: 0.5803\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5878 - acc: 0.7508 - val_loss: 1.1633 - val_acc: 0.5609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB6nqB6viwBc",
        "outputId": "c26299cd-32f7-49d7-ceeb-1bf85d07bfd2"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5939 - acc: 0.7549 - val_loss: 1.1885 - val_acc: 0.5531\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6022 - acc: 0.7425 - val_loss: 1.1319 - val_acc: 0.5632\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5855 - acc: 0.7551 - val_loss: 1.1894 - val_acc: 0.5617\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5915 - acc: 0.7531 - val_loss: 1.2069 - val_acc: 0.5555\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.6035 - acc: 0.7417 - val_loss: 1.2068 - val_acc: 0.5531\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5888 - acc: 0.7557 - val_loss: 1.1231 - val_acc: 0.5725\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5785 - acc: 0.7613 - val_loss: 1.2107 - val_acc: 0.5570\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5906 - acc: 0.7545 - val_loss: 1.1308 - val_acc: 0.5733\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5955 - acc: 0.7531 - val_loss: 1.1567 - val_acc: 0.5718\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5861 - acc: 0.7557 - val_loss: 1.1832 - val_acc: 0.5632\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5855 - acc: 0.7531 - val_loss: 1.2160 - val_acc: 0.5648\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5853 - acc: 0.7528 - val_loss: 1.1887 - val_acc: 0.5694\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5919 - acc: 0.7574 - val_loss: 1.1618 - val_acc: 0.5632\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5854 - acc: 0.7568 - val_loss: 1.1553 - val_acc: 0.5648\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5904 - acc: 0.7564 - val_loss: 1.1649 - val_acc: 0.5671\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5944 - acc: 0.7468 - val_loss: 1.1326 - val_acc: 0.5787\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5861 - acc: 0.7539 - val_loss: 1.1753 - val_acc: 0.5648\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5847 - acc: 0.7576 - val_loss: 1.1409 - val_acc: 0.5795\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5840 - acc: 0.7568 - val_loss: 1.1631 - val_acc: 0.5702\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5898 - acc: 0.7500 - val_loss: 1.1486 - val_acc: 0.5764\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5890 - acc: 0.7560 - val_loss: 1.1118 - val_acc: 0.5881\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5943 - acc: 0.7518 - val_loss: 1.1690 - val_acc: 0.5648\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5832 - acc: 0.7559 - val_loss: 1.1787 - val_acc: 0.5632\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5791 - acc: 0.7595 - val_loss: 1.1683 - val_acc: 0.5617\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5806 - acc: 0.7580 - val_loss: 1.2192 - val_acc: 0.5555\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5770 - acc: 0.7595 - val_loss: 1.1961 - val_acc: 0.5477\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5892 - acc: 0.7566 - val_loss: 1.2293 - val_acc: 0.5477\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5920 - acc: 0.7570 - val_loss: 1.1438 - val_acc: 0.5679\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5781 - acc: 0.7584 - val_loss: 1.1408 - val_acc: 0.5780\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5741 - acc: 0.7636 - val_loss: 1.1790 - val_acc: 0.5562\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5884 - acc: 0.7541 - val_loss: 1.1607 - val_acc: 0.5733\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5798 - acc: 0.7566 - val_loss: 1.1786 - val_acc: 0.5694\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5826 - acc: 0.7603 - val_loss: 1.1999 - val_acc: 0.5524\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5663 - acc: 0.7717 - val_loss: 1.2592 - val_acc: 0.5446\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5880 - acc: 0.7568 - val_loss: 1.1338 - val_acc: 0.5710\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5768 - acc: 0.7613 - val_loss: 1.1867 - val_acc: 0.5586\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5673 - acc: 0.7717 - val_loss: 1.1963 - val_acc: 0.5578\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5752 - acc: 0.7618 - val_loss: 1.1786 - val_acc: 0.5710\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5659 - acc: 0.7620 - val_loss: 1.1886 - val_acc: 0.5648\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5781 - acc: 0.7618 - val_loss: 1.2636 - val_acc: 0.5431\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5744 - acc: 0.7690 - val_loss: 1.2285 - val_acc: 0.5493\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5859 - acc: 0.7588 - val_loss: 1.1462 - val_acc: 0.5656\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5678 - acc: 0.7700 - val_loss: 1.1557 - val_acc: 0.5702\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5886 - acc: 0.7530 - val_loss: 1.1242 - val_acc: 0.5865\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5833 - acc: 0.7609 - val_loss: 1.1656 - val_acc: 0.5578\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5674 - acc: 0.7702 - val_loss: 1.1924 - val_acc: 0.5663\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5827 - acc: 0.7599 - val_loss: 1.2013 - val_acc: 0.5640\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5669 - acc: 0.7630 - val_loss: 1.1276 - val_acc: 0.5772\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5892 - acc: 0.7576 - val_loss: 1.1980 - val_acc: 0.5570\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5779 - acc: 0.7570 - val_loss: 1.1985 - val_acc: 0.5632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB5ZDpZLlh9D",
        "outputId": "b5803d01-7a4b-43a4-8453-daa192143d07"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5814 - acc: 0.7620 - val_loss: 1.2965 - val_acc: 0.5539\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5823 - acc: 0.7605 - val_loss: 1.2201 - val_acc: 0.5648\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5714 - acc: 0.7649 - val_loss: 1.1904 - val_acc: 0.5671\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5806 - acc: 0.7617 - val_loss: 1.1634 - val_acc: 0.5787\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5890 - acc: 0.7526 - val_loss: 1.2087 - val_acc: 0.5593\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5644 - acc: 0.7686 - val_loss: 1.2151 - val_acc: 0.5431\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5829 - acc: 0.7562 - val_loss: 1.2073 - val_acc: 0.5578\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5776 - acc: 0.7634 - val_loss: 1.1739 - val_acc: 0.5694\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5723 - acc: 0.7665 - val_loss: 1.2738 - val_acc: 0.5392\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5697 - acc: 0.7684 - val_loss: 1.2958 - val_acc: 0.5477\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5871 - acc: 0.7539 - val_loss: 1.2249 - val_acc: 0.5547\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5720 - acc: 0.7638 - val_loss: 1.2207 - val_acc: 0.5656\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5622 - acc: 0.7667 - val_loss: 1.1894 - val_acc: 0.5679\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5647 - acc: 0.7653 - val_loss: 1.1967 - val_acc: 0.5694\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5686 - acc: 0.7682 - val_loss: 1.1777 - val_acc: 0.5663\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5689 - acc: 0.7661 - val_loss: 1.1390 - val_acc: 0.5818\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5850 - acc: 0.7568 - val_loss: 1.1782 - val_acc: 0.5609\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5666 - acc: 0.7684 - val_loss: 1.2275 - val_acc: 0.5524\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5723 - acc: 0.7622 - val_loss: 1.1761 - val_acc: 0.5648\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5690 - acc: 0.7690 - val_loss: 1.1801 - val_acc: 0.5656\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5571 - acc: 0.7698 - val_loss: 1.1904 - val_acc: 0.5694\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5616 - acc: 0.7636 - val_loss: 1.2191 - val_acc: 0.5725\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5564 - acc: 0.7678 - val_loss: 1.2204 - val_acc: 0.5485\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5594 - acc: 0.7719 - val_loss: 1.2556 - val_acc: 0.5586\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5638 - acc: 0.7611 - val_loss: 1.1475 - val_acc: 0.5764\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5546 - acc: 0.7740 - val_loss: 1.1527 - val_acc: 0.5741\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5632 - acc: 0.7690 - val_loss: 1.1875 - val_acc: 0.5663\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5742 - acc: 0.7576 - val_loss: 1.2830 - val_acc: 0.5431\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5714 - acc: 0.7601 - val_loss: 1.2012 - val_acc: 0.5710\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5634 - acc: 0.7682 - val_loss: 1.1963 - val_acc: 0.5679\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5570 - acc: 0.7700 - val_loss: 1.2197 - val_acc: 0.5586\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5702 - acc: 0.7607 - val_loss: 1.1845 - val_acc: 0.5756\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5670 - acc: 0.7663 - val_loss: 1.2471 - val_acc: 0.5609\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5667 - acc: 0.7671 - val_loss: 1.1975 - val_acc: 0.5656\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5703 - acc: 0.7646 - val_loss: 1.2900 - val_acc: 0.5555\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5662 - acc: 0.7642 - val_loss: 1.2513 - val_acc: 0.5516\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5608 - acc: 0.7725 - val_loss: 1.3903 - val_acc: 0.5384\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5573 - acc: 0.7715 - val_loss: 1.2162 - val_acc: 0.5578\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5682 - acc: 0.7682 - val_loss: 1.2445 - val_acc: 0.5500\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5631 - acc: 0.7694 - val_loss: 1.2887 - val_acc: 0.5477\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5559 - acc: 0.7698 - val_loss: 1.2245 - val_acc: 0.5694\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5632 - acc: 0.7731 - val_loss: 1.2054 - val_acc: 0.5671\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5589 - acc: 0.7748 - val_loss: 1.2246 - val_acc: 0.5578\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5620 - acc: 0.7665 - val_loss: 1.2100 - val_acc: 0.5632\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5734 - acc: 0.7613 - val_loss: 1.2377 - val_acc: 0.5570\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5630 - acc: 0.7663 - val_loss: 1.2049 - val_acc: 0.5718\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5664 - acc: 0.7638 - val_loss: 1.2642 - val_acc: 0.5531\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5590 - acc: 0.7700 - val_loss: 1.2231 - val_acc: 0.5663\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5601 - acc: 0.7696 - val_loss: 1.2048 - val_acc: 0.5741\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5608 - acc: 0.7646 - val_loss: 1.2479 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LhtOC_LoOmu",
        "outputId": "92a2bada-cb65-4adb-9a7e-5d77f927dee8"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5633 - acc: 0.7651 - val_loss: 1.1981 - val_acc: 0.5648\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5487 - acc: 0.7766 - val_loss: 1.2798 - val_acc: 0.5516\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5598 - acc: 0.7684 - val_loss: 1.1441 - val_acc: 0.5780\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5598 - acc: 0.7706 - val_loss: 1.2973 - val_acc: 0.5516\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5541 - acc: 0.7733 - val_loss: 1.2071 - val_acc: 0.5617\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5561 - acc: 0.7733 - val_loss: 1.2089 - val_acc: 0.5593\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5605 - acc: 0.7673 - val_loss: 1.2324 - val_acc: 0.5562\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5518 - acc: 0.7775 - val_loss: 1.1825 - val_acc: 0.5632\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5613 - acc: 0.7723 - val_loss: 1.1740 - val_acc: 0.5648\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5670 - acc: 0.7671 - val_loss: 1.1878 - val_acc: 0.5780\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5507 - acc: 0.7725 - val_loss: 1.2496 - val_acc: 0.5485\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5463 - acc: 0.7717 - val_loss: 1.1923 - val_acc: 0.5718\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5582 - acc: 0.7651 - val_loss: 1.1771 - val_acc: 0.5671\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5510 - acc: 0.7756 - val_loss: 1.3193 - val_acc: 0.5469\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5599 - acc: 0.7680 - val_loss: 1.1672 - val_acc: 0.5749\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5533 - acc: 0.7760 - val_loss: 1.2573 - val_acc: 0.5601\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5608 - acc: 0.7665 - val_loss: 1.1772 - val_acc: 0.5756\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5443 - acc: 0.7816 - val_loss: 1.2446 - val_acc: 0.5539\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5517 - acc: 0.7686 - val_loss: 1.2133 - val_acc: 0.5671\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5512 - acc: 0.7733 - val_loss: 1.2895 - val_acc: 0.5547\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5706 - acc: 0.7624 - val_loss: 1.1971 - val_acc: 0.5648\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5737 - acc: 0.7624 - val_loss: 1.2212 - val_acc: 0.5578\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5515 - acc: 0.7767 - val_loss: 1.1930 - val_acc: 0.5795\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5394 - acc: 0.7808 - val_loss: 1.2536 - val_acc: 0.5648\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5587 - acc: 0.7671 - val_loss: 1.2654 - val_acc: 0.5462\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5496 - acc: 0.7655 - val_loss: 1.2218 - val_acc: 0.5679\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5450 - acc: 0.7750 - val_loss: 1.2700 - val_acc: 0.5291\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5534 - acc: 0.7684 - val_loss: 1.2661 - val_acc: 0.5485\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5655 - acc: 0.7620 - val_loss: 1.1702 - val_acc: 0.5834\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5514 - acc: 0.7764 - val_loss: 1.2724 - val_acc: 0.5500\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5520 - acc: 0.7746 - val_loss: 1.2679 - val_acc: 0.5547\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5557 - acc: 0.7665 - val_loss: 1.2451 - val_acc: 0.5586\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5593 - acc: 0.7696 - val_loss: 1.2693 - val_acc: 0.5477\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5588 - acc: 0.7709 - val_loss: 1.2144 - val_acc: 0.5702\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5438 - acc: 0.7800 - val_loss: 1.2455 - val_acc: 0.5648\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5432 - acc: 0.7773 - val_loss: 1.2209 - val_acc: 0.5671\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5612 - acc: 0.7642 - val_loss: 1.2077 - val_acc: 0.5756\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5511 - acc: 0.7707 - val_loss: 1.2270 - val_acc: 0.5772\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5494 - acc: 0.7754 - val_loss: 1.2837 - val_acc: 0.5586\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5403 - acc: 0.7818 - val_loss: 1.2601 - val_acc: 0.5617\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5462 - acc: 0.7795 - val_loss: 1.2195 - val_acc: 0.5640\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5523 - acc: 0.7729 - val_loss: 1.2469 - val_acc: 0.5524\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5544 - acc: 0.7731 - val_loss: 1.2363 - val_acc: 0.5586\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5532 - acc: 0.7733 - val_loss: 1.3068 - val_acc: 0.5508\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5497 - acc: 0.7706 - val_loss: 1.2151 - val_acc: 0.5609\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5397 - acc: 0.7767 - val_loss: 1.2482 - val_acc: 0.5485\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5507 - acc: 0.7731 - val_loss: 1.1994 - val_acc: 0.5656\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5530 - acc: 0.7754 - val_loss: 1.2616 - val_acc: 0.5562\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5662 - acc: 0.7580 - val_loss: 1.2487 - val_acc: 0.5593\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5439 - acc: 0.7771 - val_loss: 1.1907 - val_acc: 0.5764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0f7e37xrGDE",
        "outputId": "54ff86b4-73fe-4e95-ab4c-ec465395445e"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5367 - acc: 0.7798 - val_loss: 1.2380 - val_acc: 0.5586\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5538 - acc: 0.7713 - val_loss: 1.2988 - val_acc: 0.5462\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5484 - acc: 0.7766 - val_loss: 1.2477 - val_acc: 0.5663\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5536 - acc: 0.7769 - val_loss: 1.2666 - val_acc: 0.5500\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5346 - acc: 0.7796 - val_loss: 1.2160 - val_acc: 0.5780\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5378 - acc: 0.7835 - val_loss: 1.1855 - val_acc: 0.5741\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5377 - acc: 0.7862 - val_loss: 1.2004 - val_acc: 0.5687\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5555 - acc: 0.7709 - val_loss: 1.2467 - val_acc: 0.5679\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5467 - acc: 0.7750 - val_loss: 1.2307 - val_acc: 0.5648\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5486 - acc: 0.7737 - val_loss: 1.2855 - val_acc: 0.5454\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5430 - acc: 0.7816 - val_loss: 1.2586 - val_acc: 0.5508\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5393 - acc: 0.7847 - val_loss: 1.1782 - val_acc: 0.5787\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5348 - acc: 0.7831 - val_loss: 1.2589 - val_acc: 0.5656\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5604 - acc: 0.7678 - val_loss: 1.1897 - val_acc: 0.5710\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5494 - acc: 0.7775 - val_loss: 1.2715 - val_acc: 0.5578\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5427 - acc: 0.7735 - val_loss: 1.2480 - val_acc: 0.5609\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5453 - acc: 0.7766 - val_loss: 1.2166 - val_acc: 0.5749\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5415 - acc: 0.7802 - val_loss: 1.2272 - val_acc: 0.5679\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5496 - acc: 0.7719 - val_loss: 1.2163 - val_acc: 0.5656\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5455 - acc: 0.7731 - val_loss: 1.1626 - val_acc: 0.5764\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5397 - acc: 0.7775 - val_loss: 1.2999 - val_acc: 0.5547\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5406 - acc: 0.7779 - val_loss: 1.2568 - val_acc: 0.5454\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5444 - acc: 0.7789 - val_loss: 1.2104 - val_acc: 0.5803\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5540 - acc: 0.7737 - val_loss: 1.2458 - val_acc: 0.5586\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5372 - acc: 0.7831 - val_loss: 1.2657 - val_acc: 0.5485\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5375 - acc: 0.7800 - val_loss: 1.3060 - val_acc: 0.5570\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5356 - acc: 0.7849 - val_loss: 1.3561 - val_acc: 0.5454\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5583 - acc: 0.7729 - val_loss: 1.2729 - val_acc: 0.5547\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5424 - acc: 0.7764 - val_loss: 1.2592 - val_acc: 0.5718\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5525 - acc: 0.7713 - val_loss: 1.2243 - val_acc: 0.5593\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5523 - acc: 0.7752 - val_loss: 1.2492 - val_acc: 0.5648\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5457 - acc: 0.7766 - val_loss: 1.2899 - val_acc: 0.5500\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5355 - acc: 0.7816 - val_loss: 1.2818 - val_acc: 0.5508\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5437 - acc: 0.7806 - val_loss: 1.2226 - val_acc: 0.5687\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5341 - acc: 0.7839 - val_loss: 1.2844 - val_acc: 0.5531\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5397 - acc: 0.7779 - val_loss: 1.2366 - val_acc: 0.5702\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5481 - acc: 0.7787 - val_loss: 1.2408 - val_acc: 0.5795\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5274 - acc: 0.7897 - val_loss: 1.2293 - val_acc: 0.5725\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5480 - acc: 0.7760 - val_loss: 1.2545 - val_acc: 0.5648\n",
            "Epoch 40/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5300 - acc: 0.7829 - val_loss: 1.2702 - val_acc: 0.5547\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5379 - acc: 0.7816 - val_loss: 1.2416 - val_acc: 0.5663\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5303 - acc: 0.7827 - val_loss: 1.2562 - val_acc: 0.5632\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5378 - acc: 0.7812 - val_loss: 1.2227 - val_acc: 0.5764\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5488 - acc: 0.7738 - val_loss: 1.2077 - val_acc: 0.5764\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5488 - acc: 0.7750 - val_loss: 1.2374 - val_acc: 0.5632\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5282 - acc: 0.7818 - val_loss: 1.2420 - val_acc: 0.5756\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5336 - acc: 0.7824 - val_loss: 1.2113 - val_acc: 0.5694\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5477 - acc: 0.7721 - val_loss: 1.2634 - val_acc: 0.5632\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5447 - acc: 0.7764 - val_loss: 1.2183 - val_acc: 0.5718\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5338 - acc: 0.7878 - val_loss: 1.2844 - val_acc: 0.5539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IyLo4hRt047",
        "outputId": "af4b8296-78ea-46b7-f3c7-4e5928e555b2"
      },
      "source": [
        "hist00 = model00.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5295 - acc: 0.7856 - val_loss: 1.2465 - val_acc: 0.5640\n",
            "Epoch 2/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5429 - acc: 0.7748 - val_loss: 1.2619 - val_acc: 0.5578\n",
            "Epoch 3/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5352 - acc: 0.7798 - val_loss: 1.2587 - val_acc: 0.5640\n",
            "Epoch 4/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5350 - acc: 0.7814 - val_loss: 1.2647 - val_acc: 0.5640\n",
            "Epoch 5/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5390 - acc: 0.7767 - val_loss: 1.2508 - val_acc: 0.5694\n",
            "Epoch 6/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5335 - acc: 0.7795 - val_loss: 1.2360 - val_acc: 0.5663\n",
            "Epoch 7/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5386 - acc: 0.7806 - val_loss: 1.2602 - val_acc: 0.5632\n",
            "Epoch 8/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5366 - acc: 0.7858 - val_loss: 1.2598 - val_acc: 0.5687\n",
            "Epoch 9/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5344 - acc: 0.7822 - val_loss: 1.2324 - val_acc: 0.5632\n",
            "Epoch 10/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5335 - acc: 0.7831 - val_loss: 1.2529 - val_acc: 0.5570\n",
            "Epoch 11/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5367 - acc: 0.7787 - val_loss: 1.2330 - val_acc: 0.5756\n",
            "Epoch 12/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5351 - acc: 0.7839 - val_loss: 1.2232 - val_acc: 0.5733\n",
            "Epoch 13/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5207 - acc: 0.7899 - val_loss: 1.3074 - val_acc: 0.5500\n",
            "Epoch 14/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5479 - acc: 0.7746 - val_loss: 1.3347 - val_acc: 0.5555\n",
            "Epoch 15/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5402 - acc: 0.7789 - val_loss: 1.3043 - val_acc: 0.5570\n",
            "Epoch 16/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5352 - acc: 0.7771 - val_loss: 1.2601 - val_acc: 0.5671\n",
            "Epoch 17/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5446 - acc: 0.7800 - val_loss: 1.2779 - val_acc: 0.5656\n",
            "Epoch 18/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5400 - acc: 0.7812 - val_loss: 1.2933 - val_acc: 0.5570\n",
            "Epoch 19/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5352 - acc: 0.7829 - val_loss: 1.2470 - val_acc: 0.5694\n",
            "Epoch 20/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5315 - acc: 0.7837 - val_loss: 1.2753 - val_acc: 0.5516\n",
            "Epoch 21/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5359 - acc: 0.7793 - val_loss: 1.2221 - val_acc: 0.5787\n",
            "Epoch 22/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5298 - acc: 0.7868 - val_loss: 1.2340 - val_acc: 0.5764\n",
            "Epoch 23/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5498 - acc: 0.7723 - val_loss: 1.2058 - val_acc: 0.5842\n",
            "Epoch 24/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5236 - acc: 0.7822 - val_loss: 1.2789 - val_acc: 0.5586\n",
            "Epoch 25/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5326 - acc: 0.7796 - val_loss: 1.3472 - val_acc: 0.5469\n",
            "Epoch 26/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5400 - acc: 0.7793 - val_loss: 1.2640 - val_acc: 0.5640\n",
            "Epoch 27/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5292 - acc: 0.7827 - val_loss: 1.2572 - val_acc: 0.5648\n",
            "Epoch 28/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5277 - acc: 0.7853 - val_loss: 1.2332 - val_acc: 0.5593\n",
            "Epoch 29/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5378 - acc: 0.7808 - val_loss: 1.2984 - val_acc: 0.5516\n",
            "Epoch 30/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5277 - acc: 0.7858 - val_loss: 1.2749 - val_acc: 0.5570\n",
            "Epoch 31/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5300 - acc: 0.7862 - val_loss: 1.2564 - val_acc: 0.5547\n",
            "Epoch 32/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5354 - acc: 0.7744 - val_loss: 1.3066 - val_acc: 0.5617\n",
            "Epoch 33/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5277 - acc: 0.7800 - val_loss: 1.3047 - val_acc: 0.5493\n",
            "Epoch 34/50\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5319 - acc: 0.7824 - val_loss: 1.3124 - val_acc: 0.5500\n",
            "Epoch 35/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5326 - acc: 0.7787 - val_loss: 1.3006 - val_acc: 0.5586\n",
            "Epoch 36/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5336 - acc: 0.7779 - val_loss: 1.2261 - val_acc: 0.5687\n",
            "Epoch 37/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5214 - acc: 0.7891 - val_loss: 1.3073 - val_acc: 0.5570\n",
            "Epoch 38/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5234 - acc: 0.7843 - val_loss: 1.2578 - val_acc: 0.5625\n",
            "Epoch 39/50\n",
            "162/162 [==============================] - ETA: 0s - loss: 0.5301 - acc: 0.7810Epoch 40/50\n",
            "162/162 [==============================] - 13s 83ms/step - loss: 0.5333 - acc: 0.7791 - val_loss: 1.3021 - val_acc: 0.5524\n",
            "Epoch 41/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5241 - acc: 0.7825 - val_loss: 1.2691 - val_acc: 0.5601\n",
            "Epoch 42/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5487 - acc: 0.7725 - val_loss: 1.3380 - val_acc: 0.5508\n",
            "Epoch 43/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5299 - acc: 0.7833 - val_loss: 1.2705 - val_acc: 0.5710\n",
            "Epoch 44/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5211 - acc: 0.7866 - val_loss: 1.2796 - val_acc: 0.5710\n",
            "Epoch 45/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5307 - acc: 0.7862 - val_loss: 1.2734 - val_acc: 0.5462\n",
            "Epoch 46/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5536 - acc: 0.7750 - val_loss: 1.2388 - val_acc: 0.5756\n",
            "Epoch 47/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5292 - acc: 0.7876 - val_loss: 1.2647 - val_acc: 0.5562\n",
            "Epoch 48/50\n",
            "162/162 [==============================] - 14s 83ms/step - loss: 0.5161 - acc: 0.7911 - val_loss: 1.2750 - val_acc: 0.5617\n",
            "Epoch 49/50\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5232 - acc: 0.7839 - val_loss: 1.2684 - val_acc: 0.5687\n",
            "Epoch 50/50\n",
            "162/162 [==============================] - 14s 84ms/step - loss: 0.5300 - acc: 0.7767 - val_loss: 1.2930 - val_acc: 0.5725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnPPnHodwjlz",
        "outputId": "20272c74-3383-4f37-fe5b-b6d2b2b30c61"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model00.to_yaml()\r\n",
        "open('model_architecture_choosed.yaml_1', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model00.save_weights('model_weights_choosed_1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOD7OI53zrJV"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class_names = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\r\n",
        "class_names = np.array([key.title() for key, value in class_names])\r\n",
        "\r\n",
        "image_batch_test, label_batch_test = next(iter(train_generator))\r\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\r\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\r\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\r\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\r\n",
        "print(dataset_labels)\r\n",
        "\r\n",
        "\r\n",
        "predicted_batch = model.predict(image_batch_train)\r\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\r\n",
        "predicted_label_batch = class_names[predicted_id]\r\n",
        "label_id = np.argmax(label_batch_train, axis=-1)\r\n",
        "\r\n",
        "plt.figure(figsize=(10,9))\r\n",
        "plt.subplots_adjust(hspace=0.5)\r\n",
        "for n in range(30):\r\n",
        "  plt.subplot(6,5,n+1)\r\n",
        "  plt.imshow(image_batch_train[n])\r\n",
        "  color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\r\n",
        "  plt.title(predicted_label_batch[n].title(), color=color)\r\n",
        "  plt.axis('off')\r\n",
        "_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LPF8fEM0Tym",
        "outputId": "259632cc-fd30-4abf-e3b8-32b457d0f98b"
      },
      "source": [
        "predicted_batch = model00.predict(valid_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f39b42d78c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f39b42d78c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f39b42d78c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yNqNGx0buD"
      },
      "source": [
        "\r\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VEfTeEx0gCd",
        "outputId": "436d8cae-6049-46e9-e3bb-f7b5f7d59515"
      },
      "source": [
        "predicted_id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 5, 1, ..., 5, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKbOGNj90hua"
      },
      "source": [
        "\r\n",
        "import math\r\n",
        "number_of_examples = len(valid_generator.filenames)\r\n",
        "number_of_generator_calls = math.ceil(number_of_examples / (1.0 * 32)) \r\n",
        "# 1.0 above is to skip integer division\r\n",
        "\r\n",
        "test_labels = []\r\n",
        "\r\n",
        "for i in range(0,int(number_of_generator_calls)):\r\n",
        "    #train_labels.extend(np.array(train_generator[i][1]))\r\n",
        "    _,l=next(iter(valid_generator))\r\n",
        "    label = np.argmax(l, axis=-1)\r\n",
        "    list_l=label.tolist()\r\n",
        "    test_labels.extend(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXVL4LwO0sTj",
        "outputId": "df56dc35-9cd0-483b-a369-96661287fb39"
      },
      "source": [
        "type(valid_generator.filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wODe6M5G0vm1"
      },
      "source": [
        "test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFAxma3-1JtP",
        "outputId": "0f0206dd-0303-4d14-fd0b-78ee61c8e6ec"
      },
      "source": [
        "valid_generator.filepaths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWs7psPO1UE_"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "a=valid_generator.filenames\r\n",
        "shape = len(a)\r\n",
        "name = [a[i][a[i].rindex('/')+1:len(a[i])] for i in range(shape)]\r\n",
        "#strr[strr.rindex( '\\\\' ) + 1 : len(strr)] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pf0NbOFQ16eA",
        "outputId": "3ddb0d64-ad5d-4e92-ca8f-b05cd7130ddc"
      },
      "source": [
        "valid_generator.filenames[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0/DSC_5856_11_101.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq6nfXhh18Ep",
        "outputId": "6deff5a6-dc50-4fcf-fc00-4bb515949828"
      },
      "source": [
        "type(name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0limG0e12KVE",
        "outputId": "147a0b9a-00b8-4ec9-d6d3-d314c444e298"
      },
      "source": [
        "type(test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPfS5gUA2jtv",
        "outputId": "05a32c6f-5f87-4f4c-eecd-9f6ccf17b998"
      },
      "source": [
        "type(predicted_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Iy8MTN2oob"
      },
      "source": [
        "a1=pd.Series(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7doUuua27Fi"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ajm_CXcx27tp"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(test_labels)\r\n",
        "a3=pd.Series(predicted_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waMkFHdW3C9s"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(test_labels)\r\n",
        "a3=pd.Series(predicted_id)\r\n",
        "aa=pd.concat([a1,a2,a3],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNCuSEDV3T32"
      },
      "source": [
        "aaa=pd.DataFrame(aa,columns=['name','label','predict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8og7qGP3ZtY",
        "outputId": "0428c545-a18f-4a12-b77d-4b278af5ea1a"
      },
      "source": [
        "type(aa)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDeWdVzv3bNz"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(test_labels)\r\n",
        "a3=pd.Series(predicted_id)\r\n",
        "aa=pd.concat([a1,a2,a3],axis=1)\r\n",
        "aa.columns=['name','label','predict_label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "jMgKk-i13cfJ",
        "outputId": "030d991d-305c-4beb-c29f-846123057eae"
      },
      "source": [
        "aa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>label</th>\n",
              "      <th>predict_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DSC_5856_11_1.png</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DSC_5856_11_101.png</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>DSC_5856_11_103.png</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DSC_5856_11_115.png</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DSC_5856_11_116.png</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>DSC_5856_12_270.png</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1285</th>\n",
              "      <td>DSC_5856_12_272.png</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1286</th>\n",
              "      <td>DSC_5856_12_274.png</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>DSC_5856_12_275.png</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1288</th>\n",
              "      <td>DSC_5856_12_276.png</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1289 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     name  label  predict_label\n",
              "0       DSC_5856_11_1.png      3              4\n",
              "1     DSC_5856_11_101.png      5              5\n",
              "2     DSC_5856_11_103.png      3              1\n",
              "3     DSC_5856_11_115.png      5              1\n",
              "4     DSC_5856_11_116.png      3              2\n",
              "...                   ...    ...            ...\n",
              "1284  DSC_5856_12_270.png      3              5\n",
              "1285  DSC_5856_12_272.png      3              4\n",
              "1286  DSC_5856_12_274.png      0              5\n",
              "1287  DSC_5856_12_275.png      3              5\n",
              "1288  DSC_5856_12_276.png      5              5\n",
              "\n",
              "[1289 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgFvithL5RP1"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(test_labels)\r\n",
        "a3=pd.Series(predicted_id)\r\n",
        "aa=pd.concat([a1,a2,a3],axis=1)\r\n",
        "aa.columns=['name','label','predict_label']\r\n",
        "aa.to_csv('aa.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK2p2tNy5imU",
        "outputId": "9bbf79f8-2c75-4671-ecb1-752d3ee49b3b"
      },
      "source": [
        "# -*- coding: utf-8 -*-\r\n",
        "'''\r\n",
        "将一张图片填充为正方形后切为9张图\r\n",
        "Author:\r\n",
        "'''\r\n",
        "from PIL import Image\r\n",
        "import sys\r\n",
        "\r\n",
        "#将图片填充为正方形\r\n",
        "def fill_image(image):\r\n",
        "    width, height = image.size\r\n",
        "    #选取长和宽中较大值作为新图片的\r\n",
        "    new_image_length = width if width > height else height\r\n",
        "    #生成新图片[白底]\r\n",
        "    new_image = Image.new(image.mode, (new_image_length, new_image_length), color='white')\r\n",
        "    #将之前的图粘贴在新图上，居中\r\n",
        "    if width > height:#原图宽大于高，则填充图片的竖直维度\r\n",
        "        #(x,y)二元组表示粘贴上图相对下图的起始位置\r\n",
        "        new_image.paste(image, (0, int((new_image_length - height) / 2)))\r\n",
        "    else:\r\n",
        "        new_image.paste(image,(int((new_image_length - width) / 2),0))\r\n",
        "\r\n",
        "    return new_image\r\n",
        "\r\n",
        "#切图\r\n",
        "def cut_image(image):\r\n",
        "    width, height = image.size\r\n",
        "    item_width = int(width / 30)\r\n",
        "    box_list = []\r\n",
        "    # (left, upper, right, lower)\r\n",
        "    for i in range(0,30):#两重循环，生成9张图片基于原图的位置\r\n",
        "        for j in range(0,30):\r\n",
        "            #print((i*item_width,j*item_width,(i+1)*item_width,(j+1)*item_width))\r\n",
        "            box = (j*item_width,i*item_width,(j+1)*item_width,(i+1)*item_width)\r\n",
        "            box_list.append(box)\r\n",
        "\r\n",
        "    image_list = [image.crop(box) for box in box_list]\r\n",
        "    return image_list\r\n",
        "\r\n",
        "#保存\r\n",
        "def save_images(image_list):\r\n",
        "    index = 1\r\n",
        "    for image in image_list:\r\n",
        "        image.save('img/'+str(index) + '.png', 'PNG')\r\n",
        "        index += 1\r\n",
        "\r\n",
        "%cd /content/gdrive/MyDrive/colab/class/dataall/data\r\n",
        "if __name__ == '__main__':\r\n",
        "    file_path = \"DSC_5856_11_1.png\"\r\n",
        "    image = Image.open(file_path)\r\n",
        "    image.show()\r\n",
        "    image = fill_image(image)\r\n",
        "    image_list = cut_image(image)\r\n",
        "    save_images(image_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/class/dataall/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRdD89rg6qM3",
        "outputId": "71b6923e-eeee-4977-ec5f-e5024d72f720"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "# 加载模型结构\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models/\r\n",
        "model001 = model_from_yaml(open('model_architecture_choosed.yaml_1').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "# 加载模型参数\r\n",
        "model001.load_weights('model_weights_choosed_1.h5')\r\n",
        "model001.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist00 = model001.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f394181d950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f394181d950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f394181d950> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 0.5438 - acc: 0.7805WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39aa7d1510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39aa7d1510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39aa7d1510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 29s 106ms/step - loss: 0.5437 - acc: 0.7805 - val_loss: 1.3287 - val_acc: 0.5493\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 15s 93ms/step - loss: 0.5116 - acc: 0.7959 - val_loss: 1.2560 - val_acc: 0.5617\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 92ms/step - loss: 0.5079 - acc: 0.7977 - val_loss: 1.3295 - val_acc: 0.5601\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.5552 - acc: 0.7643 - val_loss: 1.2150 - val_acc: 0.5865\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5255 - acc: 0.7881 - val_loss: 1.2837 - val_acc: 0.5578\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5194 - acc: 0.7905 - val_loss: 1.2875 - val_acc: 0.5656\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5213 - acc: 0.7842 - val_loss: 1.2809 - val_acc: 0.5687\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5107 - acc: 0.8000 - val_loss: 1.2761 - val_acc: 0.5601\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5146 - acc: 0.7882 - val_loss: 1.2825 - val_acc: 0.5555\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5178 - acc: 0.7973 - val_loss: 1.2220 - val_acc: 0.5733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfid7aU3_ojF",
        "outputId": "4765c067-f7fb-4a06-fc49-406f904b1ca4"
      },
      "source": [
        "hist00 = model001.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=200,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5153 - acc: 0.7868 - val_loss: 1.2450 - val_acc: 0.5687\n",
            "Epoch 2/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5267 - acc: 0.7824 - val_loss: 1.3059 - val_acc: 0.5539\n",
            "Epoch 3/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5260 - acc: 0.7829 - val_loss: 1.2435 - val_acc: 0.5625\n",
            "Epoch 4/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5254 - acc: 0.7825 - val_loss: 1.2921 - val_acc: 0.5586\n",
            "Epoch 5/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5107 - acc: 0.7895 - val_loss: 1.2274 - val_acc: 0.5756\n",
            "Epoch 6/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5240 - acc: 0.7837 - val_loss: 1.3292 - val_acc: 0.5454\n",
            "Epoch 7/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5235 - acc: 0.7866 - val_loss: 1.2437 - val_acc: 0.5756\n",
            "Epoch 8/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5325 - acc: 0.7771 - val_loss: 1.2909 - val_acc: 0.5578\n",
            "Epoch 9/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5173 - acc: 0.7945 - val_loss: 1.2881 - val_acc: 0.5710\n",
            "Epoch 10/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5251 - acc: 0.7855 - val_loss: 1.3296 - val_acc: 0.5516\n",
            "Epoch 11/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5223 - acc: 0.7816 - val_loss: 1.2630 - val_acc: 0.5640\n",
            "Epoch 12/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5178 - acc: 0.7897 - val_loss: 1.3456 - val_acc: 0.5438\n",
            "Epoch 13/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5236 - acc: 0.7849 - val_loss: 1.2641 - val_acc: 0.5632\n",
            "Epoch 14/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5250 - acc: 0.7833 - val_loss: 1.2764 - val_acc: 0.5593\n",
            "Epoch 15/200\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5250 - acc: 0.7851 - val_loss: 1.3372 - val_acc: 0.5562\n",
            "Epoch 16/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5284 - acc: 0.7839 - val_loss: 1.3044 - val_acc: 0.5555\n",
            "Epoch 17/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5163 - acc: 0.7947 - val_loss: 1.2943 - val_acc: 0.5593\n",
            "Epoch 18/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5244 - acc: 0.7862 - val_loss: 1.2712 - val_acc: 0.5625\n",
            "Epoch 19/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5136 - acc: 0.7924 - val_loss: 1.2242 - val_acc: 0.5710\n",
            "Epoch 20/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5249 - acc: 0.7820 - val_loss: 1.2694 - val_acc: 0.5601\n",
            "Epoch 21/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5257 - acc: 0.7839 - val_loss: 1.2641 - val_acc: 0.5609\n",
            "Epoch 22/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5258 - acc: 0.7855 - val_loss: 1.2966 - val_acc: 0.5562\n",
            "Epoch 23/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5269 - acc: 0.7808 - val_loss: 1.3117 - val_acc: 0.5586\n",
            "Epoch 24/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5117 - acc: 0.7905 - val_loss: 1.3053 - val_acc: 0.5555\n",
            "Epoch 25/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5303 - acc: 0.7855 - val_loss: 1.3640 - val_acc: 0.5400\n",
            "Epoch 26/200\n",
            "162/162 [==============================] - 14s 85ms/step - loss: 0.5244 - acc: 0.7885 - val_loss: 1.2566 - val_acc: 0.5710\n",
            "Epoch 27/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5293 - acc: 0.7806 - val_loss: 1.2778 - val_acc: 0.5462\n",
            "Epoch 28/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5346 - acc: 0.7858 - val_loss: 1.2600 - val_acc: 0.5656\n",
            "Epoch 29/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5172 - acc: 0.7856 - val_loss: 1.4546 - val_acc: 0.5376\n",
            "Epoch 30/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5386 - acc: 0.7785 - val_loss: 1.3093 - val_acc: 0.5609\n",
            "Epoch 31/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5195 - acc: 0.7858 - val_loss: 1.2641 - val_acc: 0.5593\n",
            "Epoch 32/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5203 - acc: 0.7874 - val_loss: 1.2616 - val_acc: 0.5593\n",
            "Epoch 33/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5268 - acc: 0.7870 - val_loss: 1.2614 - val_acc: 0.5656\n",
            "Epoch 34/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5176 - acc: 0.7907 - val_loss: 1.3097 - val_acc: 0.5640\n",
            "Epoch 35/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5114 - acc: 0.7911 - val_loss: 1.3193 - val_acc: 0.5609\n",
            "Epoch 36/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5178 - acc: 0.7897 - val_loss: 1.2857 - val_acc: 0.5687\n",
            "Epoch 37/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5277 - acc: 0.7829 - val_loss: 1.3237 - val_acc: 0.5694\n",
            "Epoch 38/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5192 - acc: 0.7868 - val_loss: 1.2983 - val_acc: 0.5656\n",
            "Epoch 39/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5074 - acc: 0.7994 - val_loss: 1.3291 - val_acc: 0.5555\n",
            "Epoch 40/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5240 - acc: 0.7849 - val_loss: 1.3521 - val_acc: 0.5392\n",
            "Epoch 41/200\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.5264 - acc: 0.7787 - val_loss: 1.3173 - val_acc: 0.5508\n",
            "Epoch 42/200\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.5283 - acc: 0.7798 - val_loss: 1.2978 - val_acc: 0.5679\n",
            "Epoch 43/200\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5178 - acc: 0.7930 - val_loss: 1.2658 - val_acc: 0.5780\n",
            "Epoch 44/200\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5245 - acc: 0.7885 - val_loss: 1.3818 - val_acc: 0.5454\n",
            "Epoch 45/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5152 - acc: 0.7882 - val_loss: 1.2694 - val_acc: 0.5648\n",
            "Epoch 46/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5200 - acc: 0.7849 - val_loss: 1.2814 - val_acc: 0.5593\n",
            "Epoch 47/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5226 - acc: 0.7824 - val_loss: 1.3029 - val_acc: 0.5710\n",
            "Epoch 48/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5160 - acc: 0.7907 - val_loss: 1.3545 - val_acc: 0.5508\n",
            "Epoch 49/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5349 - acc: 0.7855 - val_loss: 1.3174 - val_acc: 0.5663\n",
            "Epoch 50/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5248 - acc: 0.7874 - val_loss: 1.3505 - val_acc: 0.5547\n",
            "Epoch 51/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5180 - acc: 0.7858 - val_loss: 1.3775 - val_acc: 0.5500\n",
            "Epoch 52/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5213 - acc: 0.7798 - val_loss: 1.2365 - val_acc: 0.5733\n",
            "Epoch 53/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5211 - acc: 0.7882 - val_loss: 1.2497 - val_acc: 0.5803\n",
            "Epoch 54/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5322 - acc: 0.7825 - val_loss: 1.3449 - val_acc: 0.5578\n",
            "Epoch 55/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5107 - acc: 0.7942 - val_loss: 1.3443 - val_acc: 0.5493\n",
            "Epoch 56/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5174 - acc: 0.7870 - val_loss: 1.3066 - val_acc: 0.5570\n",
            "Epoch 57/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5108 - acc: 0.7922 - val_loss: 1.3007 - val_acc: 0.5531\n",
            "Epoch 58/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5139 - acc: 0.7880 - val_loss: 1.3388 - val_acc: 0.5462\n",
            "Epoch 59/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5302 - acc: 0.7769 - val_loss: 1.3420 - val_acc: 0.5593\n",
            "Epoch 60/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5140 - acc: 0.7918 - val_loss: 1.2854 - val_acc: 0.5687\n",
            "Epoch 61/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5144 - acc: 0.7899 - val_loss: 1.2796 - val_acc: 0.5656\n",
            "Epoch 62/200\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5262 - acc: 0.7825 - val_loss: 1.3471 - val_acc: 0.5648\n",
            "Epoch 63/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5149 - acc: 0.7891 - val_loss: 1.3507 - val_acc: 0.5500\n",
            "Epoch 64/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5070 - acc: 0.7932 - val_loss: 1.2781 - val_acc: 0.5741\n",
            "Epoch 65/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5220 - acc: 0.7876 - val_loss: 1.2906 - val_acc: 0.5593\n",
            "Epoch 66/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5143 - acc: 0.7893 - val_loss: 1.3124 - val_acc: 0.5601\n",
            "Epoch 67/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5265 - acc: 0.7802 - val_loss: 1.2856 - val_acc: 0.5632\n",
            "Epoch 68/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5118 - acc: 0.7955 - val_loss: 1.3955 - val_acc: 0.5493\n",
            "Epoch 69/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5178 - acc: 0.7891 - val_loss: 1.3233 - val_acc: 0.5687\n",
            "Epoch 70/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5146 - acc: 0.7928 - val_loss: 1.3181 - val_acc: 0.5547\n",
            "Epoch 71/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5075 - acc: 0.7893 - val_loss: 1.2934 - val_acc: 0.5617\n",
            "Epoch 72/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5285 - acc: 0.7810 - val_loss: 1.3467 - val_acc: 0.5485\n",
            "Epoch 73/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5160 - acc: 0.7872 - val_loss: 1.3479 - val_acc: 0.5570\n",
            "Epoch 74/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5150 - acc: 0.7944 - val_loss: 1.3605 - val_acc: 0.5485\n",
            "Epoch 75/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5082 - acc: 0.7980 - val_loss: 1.2985 - val_acc: 0.5640\n",
            "Epoch 76/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5083 - acc: 0.7855 - val_loss: 1.3132 - val_acc: 0.5524\n",
            "Epoch 77/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5144 - acc: 0.7947 - val_loss: 1.3247 - val_acc: 0.5601\n",
            "Epoch 78/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5110 - acc: 0.7932 - val_loss: 1.3679 - val_acc: 0.5493\n",
            "Epoch 79/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5217 - acc: 0.7880 - val_loss: 1.2899 - val_acc: 0.5625\n",
            "Epoch 80/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5064 - acc: 0.7951 - val_loss: 1.2967 - val_acc: 0.5648\n",
            "Epoch 81/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5224 - acc: 0.7868 - val_loss: 1.2739 - val_acc: 0.5648\n",
            "Epoch 82/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5161 - acc: 0.7855 - val_loss: 1.3317 - val_acc: 0.5578\n",
            "Epoch 83/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5123 - acc: 0.7899 - val_loss: 1.2660 - val_acc: 0.5632\n",
            "Epoch 84/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5099 - acc: 0.7916 - val_loss: 1.2515 - val_acc: 0.5772\n",
            "Epoch 85/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5099 - acc: 0.7893 - val_loss: 1.2751 - val_acc: 0.5656\n",
            "Epoch 86/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5275 - acc: 0.7860 - val_loss: 1.3377 - val_acc: 0.5656\n",
            "Epoch 87/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5112 - acc: 0.7876 - val_loss: 1.2491 - val_acc: 0.5718\n",
            "Epoch 88/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5129 - acc: 0.7944 - val_loss: 1.2619 - val_acc: 0.5609\n",
            "Epoch 89/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5143 - acc: 0.7914 - val_loss: 1.2805 - val_acc: 0.5725\n",
            "Epoch 90/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5122 - acc: 0.7866 - val_loss: 1.2880 - val_acc: 0.5663\n",
            "Epoch 91/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5048 - acc: 0.7959 - val_loss: 1.3226 - val_acc: 0.5570\n",
            "Epoch 92/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5117 - acc: 0.7901 - val_loss: 1.3034 - val_acc: 0.5593\n",
            "Epoch 93/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5153 - acc: 0.7884 - val_loss: 1.3349 - val_acc: 0.5524\n",
            "Epoch 94/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5199 - acc: 0.7837 - val_loss: 1.2443 - val_acc: 0.5795\n",
            "Epoch 95/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5145 - acc: 0.7884 - val_loss: 1.3107 - val_acc: 0.5648\n",
            "Epoch 96/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5135 - acc: 0.7914 - val_loss: 1.2879 - val_acc: 0.5632\n",
            "Epoch 97/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5097 - acc: 0.7934 - val_loss: 1.3103 - val_acc: 0.5469\n",
            "Epoch 98/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5138 - acc: 0.7928 - val_loss: 1.2740 - val_acc: 0.5694\n",
            "Epoch 99/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5124 - acc: 0.7922 - val_loss: 1.2707 - val_acc: 0.5772\n",
            "Epoch 100/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5033 - acc: 0.7967 - val_loss: 1.3313 - val_acc: 0.5524\n",
            "Epoch 101/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5056 - acc: 0.7932 - val_loss: 1.2981 - val_acc: 0.5524\n",
            "Epoch 102/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5115 - acc: 0.7914 - val_loss: 1.2992 - val_acc: 0.5547\n",
            "Epoch 103/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5197 - acc: 0.7924 - val_loss: 1.3976 - val_acc: 0.5524\n",
            "Epoch 104/200\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.5152 - acc: 0.7864 - val_loss: 1.3152 - val_acc: 0.5609\n",
            "Epoch 105/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5097 - acc: 0.7862 - val_loss: 1.2958 - val_acc: 0.5656\n",
            "Epoch 106/200\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.5144 - acc: 0.7872 - val_loss: 1.3923 - val_acc: 0.5252\n",
            "Epoch 107/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5109 - acc: 0.7893 - val_loss: 1.2919 - val_acc: 0.5625\n",
            "Epoch 108/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5117 - acc: 0.7951 - val_loss: 1.3908 - val_acc: 0.5407\n",
            "Epoch 109/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5215 - acc: 0.7878 - val_loss: 1.3210 - val_acc: 0.5656\n",
            "Epoch 110/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5080 - acc: 0.7961 - val_loss: 1.2939 - val_acc: 0.5656\n",
            "Epoch 111/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5164 - acc: 0.7856 - val_loss: 1.2705 - val_acc: 0.5741\n",
            "Epoch 112/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5072 - acc: 0.7899 - val_loss: 1.3004 - val_acc: 0.5656\n",
            "Epoch 113/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5182 - acc: 0.7882 - val_loss: 1.3431 - val_acc: 0.5570\n",
            "Epoch 114/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5101 - acc: 0.7872 - val_loss: 1.3431 - val_acc: 0.5531\n",
            "Epoch 115/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5012 - acc: 0.7976 - val_loss: 1.3655 - val_acc: 0.5462\n",
            "Epoch 116/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5059 - acc: 0.7990 - val_loss: 1.3070 - val_acc: 0.5648\n",
            "Epoch 117/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5079 - acc: 0.7926 - val_loss: 1.3151 - val_acc: 0.5656\n",
            "Epoch 118/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5034 - acc: 0.7942 - val_loss: 1.3078 - val_acc: 0.5625\n",
            "Epoch 119/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5114 - acc: 0.7882 - val_loss: 1.3294 - val_acc: 0.5601\n",
            "Epoch 120/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5187 - acc: 0.7864 - val_loss: 1.3037 - val_acc: 0.5632\n",
            "Epoch 121/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4997 - acc: 0.7959 - val_loss: 1.3299 - val_acc: 0.5601\n",
            "Epoch 122/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5063 - acc: 0.7965 - val_loss: 1.3008 - val_acc: 0.5516\n",
            "Epoch 123/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5079 - acc: 0.7955 - val_loss: 1.3391 - val_acc: 0.5508\n",
            "Epoch 124/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5010 - acc: 0.7969 - val_loss: 1.3899 - val_acc: 0.5454\n",
            "Epoch 125/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5030 - acc: 0.7930 - val_loss: 1.3331 - val_acc: 0.5593\n",
            "Epoch 126/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5096 - acc: 0.7938 - val_loss: 1.2989 - val_acc: 0.5656\n",
            "Epoch 127/200\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.5025 - acc: 0.7982 - val_loss: 1.3565 - val_acc: 0.5493\n",
            "Epoch 128/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5083 - acc: 0.7913 - val_loss: 1.3451 - val_acc: 0.5632\n",
            "Epoch 129/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5062 - acc: 0.7990 - val_loss: 1.3018 - val_acc: 0.5725\n",
            "Epoch 130/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5046 - acc: 0.7911 - val_loss: 1.4183 - val_acc: 0.5586\n",
            "Epoch 131/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5203 - acc: 0.7843 - val_loss: 1.3148 - val_acc: 0.5586\n",
            "Epoch 132/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5178 - acc: 0.7891 - val_loss: 1.3065 - val_acc: 0.5578\n",
            "Epoch 133/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5075 - acc: 0.7947 - val_loss: 1.4136 - val_acc: 0.5531\n",
            "Epoch 134/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4990 - acc: 0.7940 - val_loss: 1.3392 - val_acc: 0.5586\n",
            "Epoch 135/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4984 - acc: 0.8015 - val_loss: 1.2848 - val_acc: 0.5718\n",
            "Epoch 136/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5054 - acc: 0.7916 - val_loss: 1.3108 - val_acc: 0.5679\n",
            "Epoch 137/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5022 - acc: 0.7955 - val_loss: 1.3194 - val_acc: 0.5586\n",
            "Epoch 138/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4973 - acc: 0.7969 - val_loss: 1.3254 - val_acc: 0.5578\n",
            "Epoch 139/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5182 - acc: 0.7870 - val_loss: 1.4372 - val_acc: 0.5469\n",
            "Epoch 140/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5173 - acc: 0.7858 - val_loss: 1.3158 - val_acc: 0.5679\n",
            "Epoch 141/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5150 - acc: 0.7874 - val_loss: 1.3220 - val_acc: 0.5710\n",
            "Epoch 142/200\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.5208 - acc: 0.7812 - val_loss: 1.4223 - val_acc: 0.5446\n",
            "Epoch 143/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4974 - acc: 0.8007 - val_loss: 1.4445 - val_acc: 0.5299\n",
            "Epoch 144/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5131 - acc: 0.7942 - val_loss: 1.3829 - val_acc: 0.5454\n",
            "Epoch 145/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5037 - acc: 0.7924 - val_loss: 1.3377 - val_acc: 0.5524\n",
            "Epoch 146/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4999 - acc: 0.7976 - val_loss: 1.3577 - val_acc: 0.5562\n",
            "Epoch 147/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5035 - acc: 0.7957 - val_loss: 1.2926 - val_acc: 0.5640\n",
            "Epoch 148/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5010 - acc: 0.7938 - val_loss: 1.4102 - val_acc: 0.5431\n",
            "Epoch 149/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5116 - acc: 0.7945 - val_loss: 1.3826 - val_acc: 0.5493\n",
            "Epoch 150/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5049 - acc: 0.7928 - val_loss: 1.3461 - val_acc: 0.5617\n",
            "Epoch 151/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5042 - acc: 0.7953 - val_loss: 1.3892 - val_acc: 0.5516\n",
            "Epoch 152/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4946 - acc: 0.7982 - val_loss: 1.3521 - val_acc: 0.5617\n",
            "Epoch 153/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5015 - acc: 0.7928 - val_loss: 1.2726 - val_acc: 0.5718\n",
            "Epoch 154/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4951 - acc: 0.7994 - val_loss: 1.3979 - val_acc: 0.5516\n",
            "Epoch 155/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5066 - acc: 0.7866 - val_loss: 1.3242 - val_acc: 0.5671\n",
            "Epoch 156/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5036 - acc: 0.7930 - val_loss: 1.3462 - val_acc: 0.5586\n",
            "Epoch 157/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5035 - acc: 0.7945 - val_loss: 1.3732 - val_acc: 0.5407\n",
            "Epoch 158/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4993 - acc: 0.7955 - val_loss: 1.3404 - val_acc: 0.5663\n",
            "Epoch 159/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5047 - acc: 0.7953 - val_loss: 1.3269 - val_acc: 0.5570\n",
            "Epoch 160/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4981 - acc: 0.7932 - val_loss: 1.3702 - val_acc: 0.5508\n",
            "Epoch 161/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5022 - acc: 0.7967 - val_loss: 1.2948 - val_acc: 0.5694\n",
            "Epoch 162/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5028 - acc: 0.7932 - val_loss: 1.3408 - val_acc: 0.5586\n",
            "Epoch 163/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4945 - acc: 0.8038 - val_loss: 1.3328 - val_acc: 0.5764\n",
            "Epoch 164/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5031 - acc: 0.7914 - val_loss: 1.3483 - val_acc: 0.5562\n",
            "Epoch 165/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5076 - acc: 0.7893 - val_loss: 1.3403 - val_acc: 0.5694\n",
            "Epoch 166/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4944 - acc: 0.8017 - val_loss: 1.3445 - val_acc: 0.5586\n",
            "Epoch 167/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5170 - acc: 0.7942 - val_loss: 1.3415 - val_acc: 0.5648\n",
            "Epoch 168/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4990 - acc: 0.7922 - val_loss: 1.3559 - val_acc: 0.5671\n",
            "Epoch 169/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5027 - acc: 0.7924 - val_loss: 1.3270 - val_acc: 0.5640\n",
            "Epoch 170/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5079 - acc: 0.7889 - val_loss: 1.3482 - val_acc: 0.5539\n",
            "Epoch 171/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4933 - acc: 0.8056 - val_loss: 1.3074 - val_acc: 0.5617\n",
            "Epoch 172/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5003 - acc: 0.7936 - val_loss: 1.2893 - val_acc: 0.5718\n",
            "Epoch 173/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5013 - acc: 0.7916 - val_loss: 1.4133 - val_acc: 0.5446\n",
            "Epoch 174/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5051 - acc: 0.7947 - val_loss: 1.3593 - val_acc: 0.5648\n",
            "Epoch 175/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5056 - acc: 0.7899 - val_loss: 1.3244 - val_acc: 0.5617\n",
            "Epoch 176/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5172 - acc: 0.7876 - val_loss: 1.3488 - val_acc: 0.5555\n",
            "Epoch 177/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5150 - acc: 0.7913 - val_loss: 1.3138 - val_acc: 0.5764\n",
            "Epoch 178/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5035 - acc: 0.7920 - val_loss: 1.2934 - val_acc: 0.5593\n",
            "Epoch 179/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5020 - acc: 0.7940 - val_loss: 1.3344 - val_acc: 0.5562\n",
            "Epoch 180/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4982 - acc: 0.7938 - val_loss: 1.3141 - val_acc: 0.5562\n",
            "Epoch 181/200\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.4994 - acc: 0.7936 - val_loss: 1.3124 - val_acc: 0.5679\n",
            "Epoch 182/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4976 - acc: 0.7920 - val_loss: 1.3651 - val_acc: 0.5756\n",
            "Epoch 183/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5004 - acc: 0.8017 - val_loss: 1.3918 - val_acc: 0.5500\n",
            "Epoch 184/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.5110 - acc: 0.7920 - val_loss: 1.3466 - val_acc: 0.5593\n",
            "Epoch 185/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4940 - acc: 0.7976 - val_loss: 1.4105 - val_acc: 0.5547\n",
            "Epoch 186/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5021 - acc: 0.7965 - val_loss: 1.3893 - val_acc: 0.5524\n",
            "Epoch 187/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4966 - acc: 0.7994 - val_loss: 1.4195 - val_acc: 0.5462\n",
            "Epoch 188/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4989 - acc: 0.7928 - val_loss: 1.3902 - val_acc: 0.5524\n",
            "Epoch 189/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4935 - acc: 0.8003 - val_loss: 1.3532 - val_acc: 0.5617\n",
            "Epoch 190/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4994 - acc: 0.7969 - val_loss: 1.3268 - val_acc: 0.5586\n",
            "Epoch 191/200\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4974 - acc: 0.7940 - val_loss: 1.3568 - val_acc: 0.5454\n",
            "Epoch 192/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4942 - acc: 0.7994 - val_loss: 1.2723 - val_acc: 0.5764\n",
            "Epoch 193/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4976 - acc: 0.7976 - val_loss: 1.4208 - val_acc: 0.5485\n",
            "Epoch 194/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5032 - acc: 0.7926 - val_loss: 1.3452 - val_acc: 0.5578\n",
            "Epoch 195/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5004 - acc: 0.8003 - val_loss: 1.4336 - val_acc: 0.5531\n",
            "Epoch 196/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5133 - acc: 0.7849 - val_loss: 1.3437 - val_acc: 0.5539\n",
            "Epoch 197/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.5014 - acc: 0.7973 - val_loss: 1.3527 - val_acc: 0.5454\n",
            "Epoch 198/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4995 - acc: 0.7976 - val_loss: 1.3009 - val_acc: 0.5702\n",
            "Epoch 199/200\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4920 - acc: 0.7984 - val_loss: 1.3300 - val_acc: 0.5609\n",
            "Epoch 200/200\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.5046 - acc: 0.7944 - val_loss: 1.3973 - val_acc: 0.5516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v0xe3EOAwXw",
        "outputId": "95ab434d-90cc-405d-b664-d91253f6d566"
      },
      "source": [
        "model001.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adamax(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])  \r\n",
        "hist00 = model001.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393a7a2158> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393a7a2158> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393a7a2158> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 0.4764 - acc: 0.8103WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393887b268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393887b268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393887b268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 21s 103ms/step - loss: 0.4763 - acc: 0.8103 - val_loss: 1.3376 - val_acc: 0.5702\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4586 - acc: 0.8209 - val_loss: 1.3264 - val_acc: 0.5593\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.4662 - acc: 0.8165 - val_loss: 1.3482 - val_acc: 0.5516\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.4613 - acc: 0.8177 - val_loss: 1.3236 - val_acc: 0.5733\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.4506 - acc: 0.8229 - val_loss: 1.3477 - val_acc: 0.5547\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4533 - acc: 0.8269 - val_loss: 1.3275 - val_acc: 0.5555\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4414 - acc: 0.8289 - val_loss: 1.3178 - val_acc: 0.5725\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4671 - acc: 0.8212 - val_loss: 1.2962 - val_acc: 0.5718\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4540 - acc: 0.8257 - val_loss: 1.3432 - val_acc: 0.5625\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4351 - acc: 0.8311 - val_loss: 1.3368 - val_acc: 0.5640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHyrdQ4ULrGu",
        "outputId": "fac6ff45-6303-46af-c96c-8520ed8c0842"
      },
      "source": [
        "hist00 = model001.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.4578 - acc: 0.8172 - val_loss: 1.3310 - val_acc: 0.5648\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4567 - acc: 0.8214 - val_loss: 1.3149 - val_acc: 0.5656\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4598 - acc: 0.8141 - val_loss: 1.3105 - val_acc: 0.5578\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4631 - acc: 0.8178 - val_loss: 1.3109 - val_acc: 0.5687\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.4595 - acc: 0.8212 - val_loss: 1.3083 - val_acc: 0.5671\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4602 - acc: 0.8178 - val_loss: 1.3269 - val_acc: 0.5663\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4601 - acc: 0.8197 - val_loss: 1.2964 - val_acc: 0.5578\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 87ms/step - loss: 0.4632 - acc: 0.8201 - val_loss: 1.2917 - val_acc: 0.5663\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 14s 86ms/step - loss: 0.4604 - acc: 0.8170 - val_loss: 1.3141 - val_acc: 0.5741\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 0.4581 - acc: 0.8222 - val_loss: 1.3124 - val_acc: 0.5764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz7l_sbnMW_N",
        "outputId": "7d26b477-4071-4aca-ae17-2f998176ec90"
      },
      "source": [
        "model_6 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\"),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*10, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "])\r\n",
        "model_6.build([None,224,224,3])\r\n",
        "\r\n",
        "model_6.summary()\r\n",
        "model_6.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1001)              5327773   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 60)                60120     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 366       \n",
            "=================================================================\n",
            "Total params: 5,388,259\n",
            "Trainable params: 60,486\n",
            "Non-trainable params: 5,327,773\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39383d37b8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39383d37b8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f39383d37b8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 1.6828 - acc: 0.4237WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393307c8c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393307c8c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393307c8c8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 30s 104ms/step - loss: 1.6825 - acc: 0.4239 - val_loss: 1.5813 - val_acc: 0.4500\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.5318 - acc: 0.4893 - val_loss: 1.4904 - val_acc: 0.5043\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.4319 - acc: 0.5527 - val_loss: 1.4055 - val_acc: 0.5089\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.3277 - acc: 0.5789 - val_loss: 1.3476 - val_acc: 0.5206\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.2456 - acc: 0.5984 - val_loss: 1.3233 - val_acc: 0.5074\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 1.1908 - acc: 0.6011 - val_loss: 1.2425 - val_acc: 0.5376\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.1275 - acc: 0.6088 - val_loss: 1.2167 - val_acc: 0.5322\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 1.0913 - acc: 0.6054 - val_loss: 1.2054 - val_acc: 0.5330\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.0448 - acc: 0.6164 - val_loss: 1.1978 - val_acc: 0.5376\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.0142 - acc: 0.6205 - val_loss: 1.1427 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk-3u6D6Nb3X",
        "outputId": "4b2b4828-0df4-4774-f018-87a05efdd9a3"
      },
      "source": [
        "model_6.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adamax(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393307cc80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393307cc80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f393307cc80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 0.9751 - acc: 0.6290WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f392fe96488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f392fe96488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f392fe96488> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 21s 102ms/step - loss: 0.9751 - acc: 0.6290 - val_loss: 1.1383 - val_acc: 0.5500\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9598 - acc: 0.6306 - val_loss: 1.1241 - val_acc: 0.5524\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9462 - acc: 0.6365 - val_loss: 1.1170 - val_acc: 0.5469\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9459 - acc: 0.6310 - val_loss: 1.1294 - val_acc: 0.5376\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9276 - acc: 0.6427 - val_loss: 1.1073 - val_acc: 0.5485\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9167 - acc: 0.6303 - val_loss: 1.0770 - val_acc: 0.5640\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9009 - acc: 0.6482 - val_loss: 1.0950 - val_acc: 0.5539\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8954 - acc: 0.6447 - val_loss: 1.0826 - val_acc: 0.5547\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8869 - acc: 0.6492 - val_loss: 1.0743 - val_acc: 0.5586\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 15s 92ms/step - loss: 0.8776 - acc: 0.6399 - val_loss: 1.0722 - val_acc: 0.5555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7cSfhO2OIrl",
        "outputId": "47ca94af-1140-4d8f-8476-e462969c7c44"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "162/162 [==============================] - 15s 92ms/step - loss: 0.8748 - acc: 0.6465 - val_loss: 1.0783 - val_acc: 0.5593\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 15s 92ms/step - loss: 0.8700 - acc: 0.6467 - val_loss: 1.0517 - val_acc: 0.5524\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8652 - acc: 0.6475 - val_loss: 1.0749 - val_acc: 0.5547\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8580 - acc: 0.6475 - val_loss: 1.0810 - val_acc: 0.5531\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8529 - acc: 0.6516 - val_loss: 1.0940 - val_acc: 0.5462\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8484 - acc: 0.6508 - val_loss: 1.0548 - val_acc: 0.5578\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8448 - acc: 0.6493 - val_loss: 1.0548 - val_acc: 0.5593\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8389 - acc: 0.6543 - val_loss: 1.0722 - val_acc: 0.5578\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.8371 - acc: 0.6537 - val_loss: 1.0411 - val_acc: 0.5547\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8310 - acc: 0.6522 - val_loss: 1.0463 - val_acc: 0.5500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK6nGg4oOu_B",
        "outputId": "7e5279a6-395a-4461-b48e-51b0f4eb8e4a"
      },
      "source": [
        "steps_per_epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-OCAKWoPU85",
        "outputId": "4f506681-9b5a-4664-f105-1f96d862e60a"
      },
      "source": [
        "val_steps_per_epoch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB1V6EV7PWp0",
        "outputId": "640bab71-201a-4eb7-9ac5-1e32e6d136bc"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch*3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "162/162 [==============================] - ETA: 0s - loss: 0.8293 - acc: 0.6494WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 123.0 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 123.0 batches). You may need to use the repeat() function when building your dataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 15s 92ms/step - loss: 0.8293 - acc: 0.6494 - val_loss: 1.0354 - val_acc: 0.5524\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8225 - acc: 0.6560\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8201 - acc: 0.6558\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8179 - acc: 0.6580\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8177 - acc: 0.6558\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 12s 74ms/step - loss: 0.8119 - acc: 0.6628\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 12s 72ms/step - loss: 0.8089 - acc: 0.6657\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8074 - acc: 0.6616\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8050 - acc: 0.6667\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 12s 73ms/step - loss: 0.8009 - acc: 0.6655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bokE9PK7PZ7t",
        "outputId": "5d9b0cd5-96a5-4080-e26f-b6b3a468e25a"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch/2,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "81/81 [==============================] - 9s 112ms/step - loss: 0.8101 - acc: 0.6597 - val_loss: 1.0483 - val_acc: 0.5570\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.8058 - acc: 0.6449 - val_loss: 1.0201 - val_acc: 0.5702\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 9s 106ms/step - loss: 0.7921 - acc: 0.6663 - val_loss: 1.0448 - val_acc: 0.5562\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7853 - acc: 0.6752 - val_loss: 1.0415 - val_acc: 0.5632\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7955 - acc: 0.6752 - val_loss: 1.0291 - val_acc: 0.5640\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.8047 - acc: 0.6555 - val_loss: 1.0323 - val_acc: 0.5555\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7943 - acc: 0.6771 - val_loss: 1.0581 - val_acc: 0.5570\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7712 - acc: 0.6791 - val_loss: 1.0094 - val_acc: 0.5803\n",
            "Epoch 9/10\n",
            "81/81 [==============================] - 9s 107ms/step - loss: 0.8005 - acc: 0.6617 - val_loss: 1.0462 - val_acc: 0.5508\n",
            "Epoch 10/10\n",
            "81/81 [==============================] - 9s 107ms/step - loss: 0.7956 - acc: 0.6655 - val_loss: 1.0549 - val_acc: 0.5555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXPOWrIOP7bw",
        "outputId": "87d92a34-485e-4409-d626-87a145b7ed5a"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch/2,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "81/81 [==============================] - 9s 109ms/step - loss: 0.7821 - acc: 0.6717 - val_loss: 1.0386 - val_acc: 0.5617\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 9s 109ms/step - loss: 0.7883 - acc: 0.6628 - val_loss: 1.0831 - val_acc: 0.5407\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7840 - acc: 0.6717 - val_loss: 1.0361 - val_acc: 0.5656\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 9s 107ms/step - loss: 0.7790 - acc: 0.6837 - val_loss: 1.0156 - val_acc: 0.5617\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 9s 110ms/step - loss: 0.7832 - acc: 0.6817 - val_loss: 1.0304 - val_acc: 0.5586\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 9s 106ms/step - loss: 0.7717 - acc: 0.6748 - val_loss: 1.0240 - val_acc: 0.5671\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 9s 105ms/step - loss: 0.7843 - acc: 0.6671 - val_loss: 1.0367 - val_acc: 0.5593\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 9s 107ms/step - loss: 0.7776 - acc: 0.6690 - val_loss: 1.0223 - val_acc: 0.5687\n",
            "Epoch 9/10\n",
            "81/81 [==============================] - 9s 108ms/step - loss: 0.7742 - acc: 0.6806 - val_loss: 1.0326 - val_acc: 0.5648\n",
            "Epoch 10/10\n",
            "81/81 [==============================] - 9s 110ms/step - loss: 0.7738 - acc: 0.6848 - val_loss: 1.0365 - val_acc: 0.5609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MpF0hFrQR4E",
        "outputId": "6ada67a4-ba71-46a5-b1a7-edfdeea270ad"
      },
      "source": [
        "model_6 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\"),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*40, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*30, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*20, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax'),\r\n",
        "\r\n",
        "])\r\n",
        "model_6.build([None,224,224,3])\r\n",
        "\r\n",
        "model_6.summary()\r\n",
        "model_6.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 1001)              5327773   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 240)               240480    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 180)               43380     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 120)               21720     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 6)                 726       \n",
            "=================================================================\n",
            "Total params: 5,634,079\n",
            "Trainable params: 306,306\n",
            "Non-trainable params: 5,327,773\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f392d261840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f392d261840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f392d261840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 1.7901 - acc: 0.2051WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393a77e510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393a77e510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f393a77e510> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 30s 103ms/step - loss: 1.7901 - acc: 0.2051 - val_loss: 1.7880 - val_acc: 0.2002\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.7876 - acc: 0.2021 - val_loss: 1.7869 - val_acc: 0.2002\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.7859 - acc: 0.2009 - val_loss: 1.7837 - val_acc: 0.2002\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 1.7802 - acc: 0.2033 - val_loss: 1.7692 - val_acc: 0.2002\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.7552 - acc: 0.2047 - val_loss: 1.7086 - val_acc: 0.3344\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.6687 - acc: 0.3817 - val_loss: 1.5794 - val_acc: 0.4740\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.5234 - acc: 0.4816 - val_loss: 1.4514 - val_acc: 0.4608\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.3786 - acc: 0.4877 - val_loss: 1.3402 - val_acc: 0.4763\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.2959 - acc: 0.4873 - val_loss: 1.2793 - val_acc: 0.4833\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.2346 - acc: 0.4936 - val_loss: 1.2698 - val_acc: 0.4779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2t__7qiRFkl",
        "outputId": "84cfe874-3f9e-4564-f920-97ce48c6ac2e"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 1.2033 - acc: 0.4896 - val_loss: 1.2358 - val_acc: 0.4825\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 1.1831 - acc: 0.4933 - val_loss: 1.4177 - val_acc: 0.4135\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1772 - acc: 0.4945 - val_loss: 1.2441 - val_acc: 0.4763\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 1.1588 - acc: 0.4941 - val_loss: 1.2208 - val_acc: 0.4779\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 1.1518 - acc: 0.4960 - val_loss: 1.2222 - val_acc: 0.4779\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1531 - acc: 0.4958 - val_loss: 1.2779 - val_acc: 0.4422\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1485 - acc: 0.4982 - val_loss: 1.2121 - val_acc: 0.4787\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1412 - acc: 0.4958 - val_loss: 1.2749 - val_acc: 0.4631\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.1305 - acc: 0.4991 - val_loss: 1.2085 - val_acc: 0.4810\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.1379 - acc: 0.5013 - val_loss: 1.2257 - val_acc: 0.4709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlaiwofiR1Fd",
        "outputId": "e271fa07-53cb-4054-b1ee-b96d30cfef9b"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=20,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 1.1219 - acc: 0.5016 - val_loss: 1.2094 - val_acc: 0.4794\n",
            "Epoch 2/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.1354 - acc: 0.4964 - val_loss: 1.2710 - val_acc: 0.4569\n",
            "Epoch 3/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1174 - acc: 0.5003 - val_loss: 1.2922 - val_acc: 0.4585\n",
            "Epoch 4/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 1.1212 - acc: 0.5022 - val_loss: 1.1986 - val_acc: 0.4802\n",
            "Epoch 5/20\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 1.1198 - acc: 0.4995 - val_loss: 1.2442 - val_acc: 0.4663\n",
            "Epoch 6/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.1153 - acc: 0.5090 - val_loss: 1.2581 - val_acc: 0.4554\n",
            "Epoch 7/20\n",
            "162/162 [==============================] - 14s 88ms/step - loss: 1.0878 - acc: 0.5328 - val_loss: 1.3008 - val_acc: 0.4763\n",
            "Epoch 8/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.0627 - acc: 0.5496 - val_loss: 1.2354 - val_acc: 0.4973\n",
            "Epoch 9/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.0479 - acc: 0.5498 - val_loss: 1.1510 - val_acc: 0.5151\n",
            "Epoch 10/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.0215 - acc: 0.5572 - val_loss: 1.1798 - val_acc: 0.4825\n",
            "Epoch 11/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 1.0124 - acc: 0.5630 - val_loss: 1.0993 - val_acc: 0.5384\n",
            "Epoch 12/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9971 - acc: 0.5821 - val_loss: 1.2247 - val_acc: 0.5206\n",
            "Epoch 13/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9925 - acc: 0.5796 - val_loss: 1.2273 - val_acc: 0.5144\n",
            "Epoch 14/20\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.9836 - acc: 0.5868 - val_loss: 1.1037 - val_acc: 0.5469\n",
            "Epoch 15/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9763 - acc: 0.5858 - val_loss: 1.1216 - val_acc: 0.5275\n",
            "Epoch 16/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9697 - acc: 0.5933 - val_loss: 1.1113 - val_acc: 0.5438\n",
            "Epoch 17/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9701 - acc: 0.5841 - val_loss: 1.1372 - val_acc: 0.5306\n",
            "Epoch 18/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9558 - acc: 0.5941 - val_loss: 1.1933 - val_acc: 0.5384\n",
            "Epoch 19/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9651 - acc: 0.5868 - val_loss: 1.2240 - val_acc: 0.5151\n",
            "Epoch 20/20\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.9394 - acc: 0.6024 - val_loss: 1.1364 - val_acc: 0.5438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cviTQORSVvL",
        "outputId": "ccecd818-c9ee-4d1c-f60f-b564d4735ca0"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=20,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9359 - acc: 0.6001 - val_loss: 1.1512 - val_acc: 0.5477\n",
            "Epoch 2/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9249 - acc: 0.6021 - val_loss: 1.0873 - val_acc: 0.5493\n",
            "Epoch 3/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9339 - acc: 0.5904 - val_loss: 1.1754 - val_acc: 0.5275\n",
            "Epoch 4/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9181 - acc: 0.6026 - val_loss: 1.2074 - val_acc: 0.5275\n",
            "Epoch 5/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9072 - acc: 0.6111 - val_loss: 1.1669 - val_acc: 0.5384\n",
            "Epoch 6/20\n",
            "162/162 [==============================] - 15s 91ms/step - loss: 0.9014 - acc: 0.6189 - val_loss: 1.0772 - val_acc: 0.5531\n",
            "Epoch 7/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9219 - acc: 0.5937 - val_loss: 1.1225 - val_acc: 0.5469\n",
            "Epoch 8/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.9024 - acc: 0.6137 - val_loss: 1.1066 - val_acc: 0.5485\n",
            "Epoch 9/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9108 - acc: 0.6094 - val_loss: 1.1764 - val_acc: 0.5275\n",
            "Epoch 10/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8934 - acc: 0.6129 - val_loss: 1.0927 - val_acc: 0.5516\n",
            "Epoch 11/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8978 - acc: 0.6094 - val_loss: 1.0685 - val_acc: 0.5617\n",
            "Epoch 12/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8887 - acc: 0.6185 - val_loss: 1.1202 - val_acc: 0.5586\n",
            "Epoch 13/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8896 - acc: 0.6111 - val_loss: 1.1365 - val_acc: 0.5578\n",
            "Epoch 14/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8960 - acc: 0.6146 - val_loss: 1.0721 - val_acc: 0.5539\n",
            "Epoch 15/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8840 - acc: 0.6185 - val_loss: 1.0992 - val_acc: 0.5710\n",
            "Epoch 16/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.9012 - acc: 0.6059 - val_loss: 1.0623 - val_acc: 0.5656\n",
            "Epoch 17/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8729 - acc: 0.6260 - val_loss: 1.1497 - val_acc: 0.5500\n",
            "Epoch 18/20\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8677 - acc: 0.6245 - val_loss: 1.1709 - val_acc: 0.5423\n",
            "Epoch 19/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8718 - acc: 0.6235 - val_loss: 1.0963 - val_acc: 0.5547\n",
            "Epoch 20/20\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8652 - acc: 0.6220 - val_loss: 1.0718 - val_acc: 0.5601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqvHQG7BTgsD",
        "outputId": "01ffb461-71ce-4b93-e0e7-b3c440e6bcd8"
      },
      "source": [
        "hist_6 = model_6.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "162/162 [==============================] - 15s 92ms/step - loss: 0.8656 - acc: 0.6247 - val_loss: 1.0727 - val_acc: 0.5593\n",
            "Epoch 2/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8695 - acc: 0.6191 - val_loss: 1.0620 - val_acc: 0.5609\n",
            "Epoch 3/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8587 - acc: 0.6303 - val_loss: 1.0658 - val_acc: 0.5632\n",
            "Epoch 4/30\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.8638 - acc: 0.6268 - val_loss: 1.1292 - val_acc: 0.5663\n",
            "Epoch 5/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8550 - acc: 0.6268 - val_loss: 1.0611 - val_acc: 0.5578\n",
            "Epoch 6/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8462 - acc: 0.6355 - val_loss: 1.1685 - val_acc: 0.5454\n",
            "Epoch 7/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8434 - acc: 0.6363 - val_loss: 1.2145 - val_acc: 0.5337\n",
            "Epoch 8/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8584 - acc: 0.6318 - val_loss: 1.0638 - val_acc: 0.5656\n",
            "Epoch 9/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8424 - acc: 0.6301 - val_loss: 1.1167 - val_acc: 0.5562\n",
            "Epoch 10/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8383 - acc: 0.6380 - val_loss: 1.1259 - val_acc: 0.5586\n",
            "Epoch 11/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8306 - acc: 0.6469 - val_loss: 1.1368 - val_acc: 0.5555\n",
            "Epoch 12/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8441 - acc: 0.6378 - val_loss: 1.0647 - val_acc: 0.5586\n",
            "Epoch 13/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8307 - acc: 0.6436 - val_loss: 1.1356 - val_acc: 0.5539\n",
            "Epoch 14/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8189 - acc: 0.6535 - val_loss: 1.1568 - val_acc: 0.5446\n",
            "Epoch 15/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8465 - acc: 0.6324 - val_loss: 1.1663 - val_acc: 0.5431\n",
            "Epoch 16/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8254 - acc: 0.6496 - val_loss: 1.1719 - val_acc: 0.5547\n",
            "Epoch 17/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8185 - acc: 0.6487 - val_loss: 1.1167 - val_acc: 0.5469\n",
            "Epoch 18/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8208 - acc: 0.6549 - val_loss: 1.0618 - val_acc: 0.5694\n",
            "Epoch 19/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8158 - acc: 0.6494 - val_loss: 1.0872 - val_acc: 0.5671\n",
            "Epoch 20/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8090 - acc: 0.6529 - val_loss: 1.0945 - val_acc: 0.5679\n",
            "Epoch 21/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8060 - acc: 0.6510 - val_loss: 1.0582 - val_acc: 0.5756\n",
            "Epoch 22/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8108 - acc: 0.6578 - val_loss: 1.1938 - val_acc: 0.5493\n",
            "Epoch 23/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8061 - acc: 0.6618 - val_loss: 1.0958 - val_acc: 0.5593\n",
            "Epoch 24/30\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.8123 - acc: 0.6520 - val_loss: 1.1417 - val_acc: 0.5477\n",
            "Epoch 25/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.8081 - acc: 0.6582 - val_loss: 1.1056 - val_acc: 0.5586\n",
            "Epoch 26/30\n",
            "162/162 [==============================] - 14s 89ms/step - loss: 0.7976 - acc: 0.6591 - val_loss: 1.1715 - val_acc: 0.5446\n",
            "Epoch 27/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.8079 - acc: 0.6580 - val_loss: 1.1123 - val_acc: 0.5687\n",
            "Epoch 28/30\n",
            "162/162 [==============================] - 15s 89ms/step - loss: 0.7879 - acc: 0.6669 - val_loss: 1.0976 - val_acc: 0.5702\n",
            "Epoch 29/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.7896 - acc: 0.6649 - val_loss: 1.1764 - val_acc: 0.5562\n",
            "Epoch 30/30\n",
            "162/162 [==============================] - 15s 90ms/step - loss: 0.7953 - acc: 0.6680 - val_loss: 1.1084 - val_acc: 0.5663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOurEMnPUqFL",
        "outputId": "78659405-a764-4700-de4a-2c601f4e299c"
      },
      "source": [
        "#load_weights('model_weights_choosed_1.h5')\r\n",
        "#import h5py\r\n",
        "#weight = h5py.File('model_weights_choosed_1.h5','r')   #打开h5文件\r\n",
        "model_7=tf.keras.applications.VGG16(\r\n",
        "    include_top=True, weights=None, input_tensor=None,\r\n",
        "    input_shape=None, pooling=None, classes=6,\r\n",
        "    classifier_activation='softmax'\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "model_7.build([None,224,224,3])\r\n",
        "\r\n",
        "model_7.summary()\r\n",
        "model_7.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_7 = model_7.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 6)                 24582     \n",
            "=================================================================\n",
            "Total params: 134,285,126\n",
            "Trainable params: 134,285,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3927a59598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3927a59598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f3927a59598> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - ETA: 0s - loss: 1.8090 - acc: 0.1999WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39279d3a60> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39279d3a60> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f39279d3a60> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "162/162 [==============================] - 77s 449ms/step - loss: 1.8089 - acc: 0.1999 - val_loss: 1.7880 - val_acc: 0.2002\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 68s 420ms/step - loss: 1.7892 - acc: 0.1991 - val_loss: 1.7881 - val_acc: 0.2002\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 68s 420ms/step - loss: 1.7896 - acc: 0.2019 - val_loss: 1.7875 - val_acc: 0.2002\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 68s 421ms/step - loss: 1.7909 - acc: 0.1909 - val_loss: 1.7876 - val_acc: 0.2002\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 68s 420ms/step - loss: 1.7908 - acc: 0.1915 - val_loss: 1.7875 - val_acc: 0.2002\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 68s 419ms/step - loss: 1.7893 - acc: 0.1901 - val_loss: 1.7874 - val_acc: 0.2002\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 68s 419ms/step - loss: 1.7876 - acc: 0.2008 - val_loss: 1.7875 - val_acc: 0.2002\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 68s 419ms/step - loss: 1.7859 - acc: 0.2128 - val_loss: 1.7879 - val_acc: 0.2002\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 68s 419ms/step - loss: 1.7894 - acc: 0.1919 - val_loss: 1.7875 - val_acc: 0.2002\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 68s 419ms/step - loss: 1.7907 - acc: 0.1858 - val_loss: 1.7875 - val_acc: 0.2002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5yA4b1YYuxG",
        "outputId": "361fcfa6-6c20-4c6b-a3b8-53d21fa55cd2"
      },
      "source": [
        "datagen_kwargs_train = dict(rescale=1./255)\r\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs_train)\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"training\", \r\n",
        "    shuffle=True,\r\n",
        "    target_size=IMAGE_SHAPE)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6458 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2SMV4tJarzW",
        "outputId": "9930e9bb-0c62-4d70-e694-3be925945008"
      },
      "source": [
        "model_88 = tf.keras.Sequential([\r\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/nasnet_mobile/classification/4\"),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*40, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes*20, activation='softmax'),\r\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\r\n",
        "\r\n",
        "])\r\n",
        "model_88.build([None,224,224,3])\r\n",
        "\r\n",
        "model_88.summary()\r\n",
        "model_88.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adamax(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1001)              5327773   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 240)               240480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 120)               28920     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 726       \n",
            "=================================================================\n",
            "Total params: 5,597,899\n",
            "Trainable params: 270,126\n",
            "Non-trainable params: 5,327,773\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4df0d5840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4df0d5840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4df0d5840> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - ETA: 0s - loss: 1.7901 - acc: 0.1925WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4e968e268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4e968e268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4e968e268> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - 1489s 7s/step - loss: 1.7901 - acc: 0.1925 - val_loss: 1.7850 - val_acc: 0.2002\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 18s 90ms/step - loss: 1.7828 - acc: 0.2007 - val_loss: 1.7780 - val_acc: 0.2002\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.7757 - acc: 0.1975 - val_loss: 1.7679 - val_acc: 0.2002\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.7643 - acc: 0.1954 - val_loss: 1.7525 - val_acc: 0.2002\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.7474 - acc: 0.1914 - val_loss: 1.7302 - val_acc: 0.2002\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 1.7219 - acc: 0.2145 - val_loss: 1.7017 - val_acc: 0.3413\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 1.6912 - acc: 0.3780 - val_loss: 1.6659 - val_acc: 0.4492\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.6498 - acc: 0.4955 - val_loss: 1.6232 - val_acc: 0.5058\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.6063 - acc: 0.5376 - val_loss: 1.5773 - val_acc: 0.5043\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 1.5596 - acc: 0.5371 - val_loss: 1.5265 - val_acc: 0.5237\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.5017 - acc: 0.5494 - val_loss: 1.4730 - val_acc: 0.5175\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 18s 90ms/step - loss: 1.4473 - acc: 0.5622 - val_loss: 1.4221 - val_acc: 0.5376\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.4014 - acc: 0.5579 - val_loss: 1.3745 - val_acc: 0.5206\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 1.3445 - acc: 0.5702 - val_loss: 1.3299 - val_acc: 0.5353\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.3020 - acc: 0.5735 - val_loss: 1.2859 - val_acc: 0.5539\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.2586 - acc: 0.5688 - val_loss: 1.2518 - val_acc: 0.5493\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.2174 - acc: 0.5881 - val_loss: 1.2175 - val_acc: 0.5562\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 1.1853 - acc: 0.5958 - val_loss: 1.1877 - val_acc: 0.5663\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.1498 - acc: 0.6005 - val_loss: 1.1725 - val_acc: 0.5593\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 1.1416 - acc: 0.5879 - val_loss: 1.1444 - val_acc: 0.5671\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.1038 - acc: 0.6042 - val_loss: 1.1148 - val_acc: 0.5795\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 1.0804 - acc: 0.6102 - val_loss: 1.0976 - val_acc: 0.5873\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 1.0591 - acc: 0.6091 - val_loss: 1.0801 - val_acc: 0.5865\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 1.0335 - acc: 0.6173 - val_loss: 1.0608 - val_acc: 0.5888\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 1.0186 - acc: 0.6232 - val_loss: 1.0525 - val_acc: 0.5896\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 18s 91ms/step - loss: 1.0099 - acc: 0.6045 - val_loss: 1.0357 - val_acc: 0.5865\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.9870 - acc: 0.6112 - val_loss: 1.0161 - val_acc: 0.5974\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.9729 - acc: 0.6194 - val_loss: 1.0302 - val_acc: 0.5702\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.9678 - acc: 0.6121 - val_loss: 1.0024 - val_acc: 0.5834\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.9701 - acc: 0.6063 - val_loss: 0.9848 - val_acc: 0.5989\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.9335 - acc: 0.6216 - val_loss: 0.9901 - val_acc: 0.5873\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.9285 - acc: 0.6240 - val_loss: 0.9683 - val_acc: 0.6090\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.9257 - acc: 0.6259 - val_loss: 0.9654 - val_acc: 0.6012\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 18s 90ms/step - loss: 0.9166 - acc: 0.6129 - val_loss: 0.9590 - val_acc: 0.5857\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.9014 - acc: 0.6250 - val_loss: 0.9442 - val_acc: 0.6036\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.9021 - acc: 0.6267 - val_loss: 0.9359 - val_acc: 0.6043\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8853 - acc: 0.6334 - val_loss: 0.9337 - val_acc: 0.6059\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.8965 - acc: 0.6235 - val_loss: 0.9287 - val_acc: 0.6082\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8743 - acc: 0.6276 - val_loss: 0.9191 - val_acc: 0.6152\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.8807 - acc: 0.6332 - val_loss: 0.9145 - val_acc: 0.6106\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8643 - acc: 0.6355 - val_loss: 0.9185 - val_acc: 0.6121\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8626 - acc: 0.6298 - val_loss: 0.9243 - val_acc: 0.5997\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8483 - acc: 0.6397 - val_loss: 0.9055 - val_acc: 0.6137\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.8488 - acc: 0.6484 - val_loss: 0.9059 - val_acc: 0.6152\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8458 - acc: 0.6503 - val_loss: 0.9020 - val_acc: 0.6005\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.8466 - acc: 0.6378 - val_loss: 0.8900 - val_acc: 0.6113\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8402 - acc: 0.6495 - val_loss: 0.8844 - val_acc: 0.6237\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8379 - acc: 0.6487 - val_loss: 0.8964 - val_acc: 0.6214\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.8373 - acc: 0.6382 - val_loss: 0.8830 - val_acc: 0.6276\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8190 - acc: 0.6603 - val_loss: 0.8775 - val_acc: 0.6168\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8355 - acc: 0.6376 - val_loss: 0.8676 - val_acc: 0.6369\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8369 - acc: 0.6462 - val_loss: 0.8754 - val_acc: 0.6284\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.8135 - acc: 0.6570 - val_loss: 0.8818 - val_acc: 0.6237\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8085 - acc: 0.6529 - val_loss: 0.8694 - val_acc: 0.6253\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.8227 - acc: 0.6487 - val_loss: 0.8635 - val_acc: 0.6299\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8078 - acc: 0.6540 - val_loss: 0.8492 - val_acc: 0.6377\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.8149 - acc: 0.6435 - val_loss: 0.8468 - val_acc: 0.6439\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.8013 - acc: 0.6660 - val_loss: 0.8684 - val_acc: 0.6330\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7949 - acc: 0.6697 - val_loss: 0.8648 - val_acc: 0.6323\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.7961 - acc: 0.6684 - val_loss: 0.8457 - val_acc: 0.6385\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.8014 - acc: 0.6563 - val_loss: 0.8371 - val_acc: 0.6393\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7999 - acc: 0.6454 - val_loss: 0.8512 - val_acc: 0.6245\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.8052 - acc: 0.6592 - val_loss: 0.8322 - val_acc: 0.6548\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7809 - acc: 0.6664 - val_loss: 0.8402 - val_acc: 0.6338\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7962 - acc: 0.6637 - val_loss: 0.8359 - val_acc: 0.6470\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7933 - acc: 0.6568 - val_loss: 0.8472 - val_acc: 0.6261\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7810 - acc: 0.6621 - val_loss: 0.8387 - val_acc: 0.6408\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7864 - acc: 0.6662 - val_loss: 0.8231 - val_acc: 0.6493\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7834 - acc: 0.6721 - val_loss: 0.8366 - val_acc: 0.6431\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7625 - acc: 0.6832 - val_loss: 0.8229 - val_acc: 0.6470\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7777 - acc: 0.6677 - val_loss: 0.8342 - val_acc: 0.6455\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7807 - acc: 0.6645 - val_loss: 0.8151 - val_acc: 0.6509\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7880 - acc: 0.6566 - val_loss: 0.8173 - val_acc: 0.6517\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7955 - acc: 0.6592 - val_loss: 0.8208 - val_acc: 0.6486\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7722 - acc: 0.6675 - val_loss: 0.8053 - val_acc: 0.6594\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7649 - acc: 0.6695 - val_loss: 0.8112 - val_acc: 0.6517\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7585 - acc: 0.6724 - val_loss: 0.8133 - val_acc: 0.6555\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7656 - acc: 0.6791 - val_loss: 0.8107 - val_acc: 0.6540\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.7485 - acc: 0.6712 - val_loss: 0.8000 - val_acc: 0.6563\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.7491 - acc: 0.6797 - val_loss: 0.7982 - val_acc: 0.6579\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7494 - acc: 0.6753 - val_loss: 0.8060 - val_acc: 0.6501\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.7688 - acc: 0.6671 - val_loss: 0.7954 - val_acc: 0.6641\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7395 - acc: 0.6754 - val_loss: 0.8069 - val_acc: 0.6517\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7373 - acc: 0.6897 - val_loss: 0.8035 - val_acc: 0.6501\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7510 - acc: 0.6733 - val_loss: 0.8282 - val_acc: 0.6416\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.7430 - acc: 0.6919 - val_loss: 0.8281 - val_acc: 0.6237\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7568 - acc: 0.6747 - val_loss: 0.8227 - val_acc: 0.6362\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7453 - acc: 0.6900 - val_loss: 0.8034 - val_acc: 0.6594\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7392 - acc: 0.6905 - val_loss: 0.8132 - val_acc: 0.6478\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7406 - acc: 0.6807 - val_loss: 0.7779 - val_acc: 0.6625\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7215 - acc: 0.6977 - val_loss: 0.7977 - val_acc: 0.6602\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7326 - acc: 0.6929 - val_loss: 0.8071 - val_acc: 0.6532\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7488 - acc: 0.6763 - val_loss: 0.8285 - val_acc: 0.6307\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7322 - acc: 0.6856 - val_loss: 0.7929 - val_acc: 0.6610\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7318 - acc: 0.6939 - val_loss: 0.7719 - val_acc: 0.6749\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7197 - acc: 0.6904 - val_loss: 0.8080 - val_acc: 0.6377\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7227 - acc: 0.6972 - val_loss: 0.7730 - val_acc: 0.6680\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7274 - acc: 0.6927 - val_loss: 0.7937 - val_acc: 0.6548\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7428 - acc: 0.6900 - val_loss: 0.7631 - val_acc: 0.6742\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.7343 - acc: 0.6788 - val_loss: 0.7684 - val_acc: 0.6680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouQ-hiTAm0Sj",
        "outputId": "1a65718c-51e9-40a5-c428-ce1a3590c32b"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights.h5')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7244 - acc: 0.6915 - val_loss: 0.7920 - val_acc: 0.6563\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.7273 - acc: 0.6884 - val_loss: 0.7709 - val_acc: 0.6649\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7249 - acc: 0.6894 - val_loss: 0.7580 - val_acc: 0.6687\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7237 - acc: 0.6909 - val_loss: 0.7710 - val_acc: 0.6687\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7216 - acc: 0.6932 - val_loss: 0.7683 - val_acc: 0.6633\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7193 - acc: 0.6905 - val_loss: 0.7670 - val_acc: 0.6633\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7203 - acc: 0.6898 - val_loss: 0.7693 - val_acc: 0.6656\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7181 - acc: 0.6909 - val_loss: 0.8748 - val_acc: 0.6183\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7225 - acc: 0.6974 - val_loss: 0.7687 - val_acc: 0.6695\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7245 - acc: 0.6934 - val_loss: 0.7514 - val_acc: 0.6835\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7158 - acc: 0.6937 - val_loss: 0.7594 - val_acc: 0.6672\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7127 - acc: 0.6951 - val_loss: 0.7491 - val_acc: 0.6796\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7125 - acc: 0.6973 - val_loss: 0.7552 - val_acc: 0.6773\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7111 - acc: 0.6996 - val_loss: 0.7655 - val_acc: 0.6734\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7215 - acc: 0.6906 - val_loss: 0.7603 - val_acc: 0.6625\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7177 - acc: 0.6937 - val_loss: 0.7905 - val_acc: 0.6439\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7105 - acc: 0.7001 - val_loss: 0.7486 - val_acc: 0.6819\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7060 - acc: 0.7027 - val_loss: 0.7613 - val_acc: 0.6726\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7100 - acc: 0.6977 - val_loss: 0.7533 - val_acc: 0.6796\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.7061 - acc: 0.6991 - val_loss: 0.7384 - val_acc: 0.6897\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7067 - acc: 0.7011 - val_loss: 0.7364 - val_acc: 0.6858\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7035 - acc: 0.7027 - val_loss: 0.7766 - val_acc: 0.6664\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7038 - acc: 0.7018 - val_loss: 0.7632 - val_acc: 0.6726\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7083 - acc: 0.7015 - val_loss: 0.7768 - val_acc: 0.6633\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6997 - acc: 0.7036 - val_loss: 0.7659 - val_acc: 0.6602\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.7052 - acc: 0.7024 - val_loss: 0.7574 - val_acc: 0.6687\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.7023 - acc: 0.7042 - val_loss: 0.7566 - val_acc: 0.6773\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6975 - acc: 0.7061 - val_loss: 0.7603 - val_acc: 0.6742\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6990 - acc: 0.7072 - val_loss: 0.7418 - val_acc: 0.6757\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.7012 - acc: 0.7030 - val_loss: 0.7661 - val_acc: 0.6633\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6987 - acc: 0.6987 - val_loss: 0.7384 - val_acc: 0.6734\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6986 - acc: 0.7047 - val_loss: 0.7467 - val_acc: 0.6811\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6939 - acc: 0.7072 - val_loss: 0.7425 - val_acc: 0.6711\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6976 - acc: 0.7058 - val_loss: 0.7343 - val_acc: 0.6874\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6916 - acc: 0.7103 - val_loss: 0.7257 - val_acc: 0.6943\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6957 - acc: 0.7049 - val_loss: 0.7224 - val_acc: 0.6982\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6924 - acc: 0.7056 - val_loss: 0.7493 - val_acc: 0.6664\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6937 - acc: 0.7042 - val_loss: 0.7316 - val_acc: 0.6835\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6910 - acc: 0.7100 - val_loss: 0.7277 - val_acc: 0.6897\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6877 - acc: 0.7135 - val_loss: 0.7536 - val_acc: 0.6711\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6924 - acc: 0.7049 - val_loss: 0.7622 - val_acc: 0.6726\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6881 - acc: 0.7095 - val_loss: 0.7658 - val_acc: 0.6641\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6860 - acc: 0.7121 - val_loss: 0.7162 - val_acc: 0.7005\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6899 - acc: 0.7067 - val_loss: 0.7631 - val_acc: 0.6734\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6869 - acc: 0.7126 - val_loss: 0.7291 - val_acc: 0.6819\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6866 - acc: 0.7106 - val_loss: 0.7175 - val_acc: 0.6951\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6887 - acc: 0.7061 - val_loss: 0.7134 - val_acc: 0.6959\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6837 - acc: 0.7131 - val_loss: 0.7198 - val_acc: 0.6920\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6861 - acc: 0.7124 - val_loss: 0.7315 - val_acc: 0.6811\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6784 - acc: 0.7159 - val_loss: 0.7405 - val_acc: 0.6835\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6815 - acc: 0.7186 - val_loss: 0.7184 - val_acc: 0.6959\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6859 - acc: 0.7106 - val_loss: 0.7161 - val_acc: 0.6951\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6803 - acc: 0.7101 - val_loss: 0.7170 - val_acc: 0.7036\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6799 - acc: 0.7148 - val_loss: 0.7365 - val_acc: 0.6827\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6819 - acc: 0.7092 - val_loss: 0.7528 - val_acc: 0.6835\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6829 - acc: 0.7143 - val_loss: 0.7183 - val_acc: 0.6959\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6742 - acc: 0.7193 - val_loss: 0.7280 - val_acc: 0.6912\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6780 - acc: 0.7177 - val_loss: 0.7205 - val_acc: 0.6928\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6761 - acc: 0.7191 - val_loss: 0.7063 - val_acc: 0.7044\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6773 - acc: 0.7168 - val_loss: 0.7533 - val_acc: 0.6780\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6732 - acc: 0.7222 - val_loss: 0.7081 - val_acc: 0.7075\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6691 - acc: 0.7171 - val_loss: 0.7033 - val_acc: 0.6998\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6740 - acc: 0.7172 - val_loss: 0.7211 - val_acc: 0.6943\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6755 - acc: 0.7202 - val_loss: 0.7004 - val_acc: 0.7075\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6700 - acc: 0.7255 - val_loss: 0.7355 - val_acc: 0.6819\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6726 - acc: 0.7250 - val_loss: 0.7208 - val_acc: 0.6959\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6653 - acc: 0.7245 - val_loss: 0.7405 - val_acc: 0.6819\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6702 - acc: 0.7183 - val_loss: 0.7302 - val_acc: 0.6811\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6671 - acc: 0.7221 - val_loss: 0.7043 - val_acc: 0.7021\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6665 - acc: 0.7221 - val_loss: 0.7071 - val_acc: 0.7021\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6709 - acc: 0.7221 - val_loss: 0.7177 - val_acc: 0.6920\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6658 - acc: 0.7231 - val_loss: 0.6953 - val_acc: 0.7083\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6640 - acc: 0.7216 - val_loss: 0.6998 - val_acc: 0.7106\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6597 - acc: 0.7281 - val_loss: 0.7019 - val_acc: 0.7013\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6681 - acc: 0.7211 - val_loss: 0.7157 - val_acc: 0.7099\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6641 - acc: 0.7264 - val_loss: 0.7017 - val_acc: 0.7029\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6657 - acc: 0.7217 - val_loss: 0.7159 - val_acc: 0.6897\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6638 - acc: 0.7250 - val_loss: 0.7160 - val_acc: 0.6982\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6603 - acc: 0.7275 - val_loss: 0.7273 - val_acc: 0.6905\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6636 - acc: 0.7270 - val_loss: 0.6964 - val_acc: 0.7060\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6618 - acc: 0.7250 - val_loss: 0.7142 - val_acc: 0.6951\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6613 - acc: 0.7261 - val_loss: 0.6934 - val_acc: 0.7106\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6642 - acc: 0.7222 - val_loss: 0.7052 - val_acc: 0.6881\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6603 - acc: 0.7241 - val_loss: 0.7100 - val_acc: 0.6912\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6613 - acc: 0.7273 - val_loss: 0.7124 - val_acc: 0.6951\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6569 - acc: 0.7293 - val_loss: 0.6897 - val_acc: 0.7083\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6626 - acc: 0.7238 - val_loss: 0.6915 - val_acc: 0.7137\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6537 - acc: 0.7301 - val_loss: 0.7135 - val_acc: 0.6974\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6554 - acc: 0.7282 - val_loss: 0.7004 - val_acc: 0.7145\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6527 - acc: 0.7303 - val_loss: 0.6816 - val_acc: 0.7277\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6519 - acc: 0.7310 - val_loss: 0.6991 - val_acc: 0.7036\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6601 - acc: 0.7264 - val_loss: 0.7063 - val_acc: 0.6943\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6490 - acc: 0.7354 - val_loss: 0.6840 - val_acc: 0.7122\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6574 - acc: 0.7255 - val_loss: 0.6840 - val_acc: 0.7145\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6515 - acc: 0.7299 - val_loss: 0.7087 - val_acc: 0.7052\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6502 - acc: 0.7354 - val_loss: 0.6786 - val_acc: 0.7184\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6536 - acc: 0.7317 - val_loss: 0.6870 - val_acc: 0.7168\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6501 - acc: 0.7312 - val_loss: 0.6944 - val_acc: 0.7067\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6507 - acc: 0.7317 - val_loss: 0.6845 - val_acc: 0.7153\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6506 - acc: 0.7321 - val_loss: 0.6990 - val_acc: 0.7021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE_JUT-0pUku",
        "outputId": "de101251-0cb7-4aad-88ad-77fc8338008f"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml1', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights1.h5')\r\n",
        "\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import model_from_yaml\r\n",
        "# 加载模型结构\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models/\r\n",
        "model33 = model_from_yaml(open('model_architecture.yaml1').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "# 加载模型参数\r\n",
        "model33.load_weights('model_weights1.h5')\r\n",
        "model33.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist33 = model33.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=3,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe479048f28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe479048f28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe479048f28> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - ETA: 0s - loss: 0.7267 - acc: 0.7012WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe473ebdb70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe473ebdb70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe473ebdb70> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - 32s 98ms/step - loss: 0.7267 - acc: 0.7012 - val_loss: 0.7936 - val_acc: 0.6726\n",
            "Epoch 2/3\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.7193 - acc: 0.6938 - val_loss: 0.7735 - val_acc: 0.6726\n",
            "Epoch 3/3\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6777 - acc: 0.7140 - val_loss: 0.8067 - val_acc: 0.6447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9AQbBrmpU8P",
        "outputId": "3ac5bf51-7e57-4e04-b1f8-c268b7fe4445"
      },
      "source": [
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6443 - acc: 0.7361 - val_loss: 0.6843 - val_acc: 0.7122\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6492 - acc: 0.7296 - val_loss: 0.6818 - val_acc: 0.7099\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6450 - acc: 0.7352 - val_loss: 0.6954 - val_acc: 0.7106\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6508 - acc: 0.7303 - val_loss: 0.6922 - val_acc: 0.7106\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6482 - acc: 0.7352 - val_loss: 0.6920 - val_acc: 0.7114\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6480 - acc: 0.7290 - val_loss: 0.7298 - val_acc: 0.6819\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6431 - acc: 0.7371 - val_loss: 0.7093 - val_acc: 0.6920\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6489 - acc: 0.7321 - val_loss: 0.6997 - val_acc: 0.7067\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6441 - acc: 0.7341 - val_loss: 0.6918 - val_acc: 0.7060\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6486 - acc: 0.7278 - val_loss: 0.6804 - val_acc: 0.7114\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6422 - acc: 0.7347 - val_loss: 0.6853 - val_acc: 0.7145\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6427 - acc: 0.7409 - val_loss: 0.6863 - val_acc: 0.7083\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6386 - acc: 0.7395 - val_loss: 0.6960 - val_acc: 0.6959\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6439 - acc: 0.7307 - val_loss: 0.7247 - val_acc: 0.6928\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6396 - acc: 0.7391 - val_loss: 0.6873 - val_acc: 0.7060\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6371 - acc: 0.7357 - val_loss: 0.6796 - val_acc: 0.7254\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6392 - acc: 0.7406 - val_loss: 0.7562 - val_acc: 0.6734\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6427 - acc: 0.7363 - val_loss: 0.6817 - val_acc: 0.7122\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6372 - acc: 0.7329 - val_loss: 0.6949 - val_acc: 0.7161\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6379 - acc: 0.7363 - val_loss: 0.6963 - val_acc: 0.7067\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6370 - acc: 0.7368 - val_loss: 0.7047 - val_acc: 0.7029\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6410 - acc: 0.7346 - val_loss: 0.7169 - val_acc: 0.7075\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6378 - acc: 0.7340 - val_loss: 0.6962 - val_acc: 0.7036\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 19s 93ms/step - loss: 0.6365 - acc: 0.7409 - val_loss: 0.6749 - val_acc: 0.7137\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6344 - acc: 0.7374 - val_loss: 0.6712 - val_acc: 0.7176\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6343 - acc: 0.7382 - val_loss: 0.6785 - val_acc: 0.7215\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6269 - acc: 0.7453 - val_loss: 0.6724 - val_acc: 0.7207\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 18s 86ms/step - loss: 0.6356 - acc: 0.7323 - val_loss: 0.7591 - val_acc: 0.6687\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6355 - acc: 0.7382 - val_loss: 0.6863 - val_acc: 0.7075\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6362 - acc: 0.7389 - val_loss: 0.6750 - val_acc: 0.7130\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6334 - acc: 0.7395 - val_loss: 0.6811 - val_acc: 0.7168\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6328 - acc: 0.7422 - val_loss: 0.6649 - val_acc: 0.7370\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6319 - acc: 0.7428 - val_loss: 0.6970 - val_acc: 0.6951\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6287 - acc: 0.7371 - val_loss: 0.6633 - val_acc: 0.7223\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6258 - acc: 0.7493 - val_loss: 0.6659 - val_acc: 0.7199\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6326 - acc: 0.7388 - val_loss: 0.6860 - val_acc: 0.7060\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6314 - acc: 0.7383 - val_loss: 0.6665 - val_acc: 0.7192\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6301 - acc: 0.7436 - val_loss: 0.7003 - val_acc: 0.6912\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6246 - acc: 0.7430 - val_loss: 0.6776 - val_acc: 0.7192\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6247 - acc: 0.7451 - val_loss: 0.6734 - val_acc: 0.7145\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6316 - acc: 0.7428 - val_loss: 0.6964 - val_acc: 0.7052\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6241 - acc: 0.7416 - val_loss: 0.6811 - val_acc: 0.7036\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6263 - acc: 0.7422 - val_loss: 0.6611 - val_acc: 0.7184\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6199 - acc: 0.7464 - val_loss: 0.6715 - val_acc: 0.7207\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6247 - acc: 0.7451 - val_loss: 0.6642 - val_acc: 0.7254\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6250 - acc: 0.7451 - val_loss: 0.6925 - val_acc: 0.7091\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6203 - acc: 0.7461 - val_loss: 0.6637 - val_acc: 0.7292\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6199 - acc: 0.7440 - val_loss: 0.6701 - val_acc: 0.7137\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6261 - acc: 0.7434 - val_loss: 0.6765 - val_acc: 0.7277\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6202 - acc: 0.7450 - val_loss: 0.6642 - val_acc: 0.7199\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6200 - acc: 0.7439 - val_loss: 0.6578 - val_acc: 0.7199\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6218 - acc: 0.7456 - val_loss: 0.6825 - val_acc: 0.7137\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6209 - acc: 0.7445 - val_loss: 0.6492 - val_acc: 0.7355\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6168 - acc: 0.7465 - val_loss: 0.6674 - val_acc: 0.7137\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6188 - acc: 0.7459 - val_loss: 0.6561 - val_acc: 0.7261\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6202 - acc: 0.7443 - val_loss: 0.6535 - val_acc: 0.7261\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6162 - acc: 0.7498 - val_loss: 0.6458 - val_acc: 0.7261\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6117 - acc: 0.7543 - val_loss: 0.6717 - val_acc: 0.7230\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6157 - acc: 0.7476 - val_loss: 0.6610 - val_acc: 0.7292\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6170 - acc: 0.7465 - val_loss: 0.6630 - val_acc: 0.7292\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6127 - acc: 0.7532 - val_loss: 0.6605 - val_acc: 0.7316\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6133 - acc: 0.7510 - val_loss: 0.6606 - val_acc: 0.7230\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 18s 86ms/step - loss: 0.6187 - acc: 0.7474 - val_loss: 0.6433 - val_acc: 0.7386\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6109 - acc: 0.7535 - val_loss: 0.6694 - val_acc: 0.7184\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6208 - acc: 0.7456 - val_loss: 0.6697 - val_acc: 0.7230\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6108 - acc: 0.7513 - val_loss: 0.6848 - val_acc: 0.7052\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6104 - acc: 0.7512 - val_loss: 0.6493 - val_acc: 0.7308\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6079 - acc: 0.7535 - val_loss: 0.6793 - val_acc: 0.7106\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.6111 - acc: 0.7505 - val_loss: 0.6516 - val_acc: 0.7401\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.6110 - acc: 0.7499 - val_loss: 0.6541 - val_acc: 0.7215\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6136 - acc: 0.7505 - val_loss: 0.6363 - val_acc: 0.7324\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6091 - acc: 0.7527 - val_loss: 0.6508 - val_acc: 0.7347\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6074 - acc: 0.7502 - val_loss: 0.6554 - val_acc: 0.7300\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6106 - acc: 0.7482 - val_loss: 0.6454 - val_acc: 0.7362\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 18s 90ms/step - loss: 0.6098 - acc: 0.7509 - val_loss: 0.6659 - val_acc: 0.7145\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6073 - acc: 0.7505 - val_loss: 0.6672 - val_acc: 0.7176\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6076 - acc: 0.7495 - val_loss: 0.6535 - val_acc: 0.7300\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6066 - acc: 0.7557 - val_loss: 0.6613 - val_acc: 0.7254\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.6076 - acc: 0.7516 - val_loss: 0.6530 - val_acc: 0.7300\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6043 - acc: 0.7561 - val_loss: 0.6496 - val_acc: 0.7269\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6025 - acc: 0.7601 - val_loss: 0.6404 - val_acc: 0.7362\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6054 - acc: 0.7558 - val_loss: 0.6487 - val_acc: 0.7355\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6054 - acc: 0.7533 - val_loss: 0.6786 - val_acc: 0.7044\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5968 - acc: 0.7628 - val_loss: 0.6397 - val_acc: 0.7355\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6018 - acc: 0.7553 - val_loss: 0.6428 - val_acc: 0.7362\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6094 - acc: 0.7515 - val_loss: 0.6887 - val_acc: 0.6998\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6044 - acc: 0.7546 - val_loss: 0.6333 - val_acc: 0.7463\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.6015 - acc: 0.7555 - val_loss: 0.6551 - val_acc: 0.7269\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6008 - acc: 0.7569 - val_loss: 0.6458 - val_acc: 0.7292\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.6010 - acc: 0.7552 - val_loss: 0.6444 - val_acc: 0.7370\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5992 - acc: 0.7581 - val_loss: 0.6581 - val_acc: 0.7277\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6006 - acc: 0.7586 - val_loss: 0.6672 - val_acc: 0.7145\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.6028 - acc: 0.7552 - val_loss: 0.6613 - val_acc: 0.7316\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5968 - acc: 0.7584 - val_loss: 0.6317 - val_acc: 0.7347\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5940 - acc: 0.7587 - val_loss: 0.6274 - val_acc: 0.7401\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.6046 - acc: 0.7543 - val_loss: 0.6377 - val_acc: 0.7455\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5968 - acc: 0.7611 - val_loss: 0.6449 - val_acc: 0.7254\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5975 - acc: 0.7569 - val_loss: 0.6244 - val_acc: 0.7339\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 18s 89ms/step - loss: 0.6009 - acc: 0.7558 - val_loss: 0.6424 - val_acc: 0.7324\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5991 - acc: 0.7567 - val_loss: 0.6285 - val_acc: 0.7455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYbCrMfqpVPb",
        "outputId": "da76429c-0f64-46e8-9e27-41d78abb2c2a"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml2', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights2.h5')\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5940 - acc: 0.7629 - val_loss: 0.6304 - val_acc: 0.7502\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5889 - acc: 0.7659 - val_loss: 0.6231 - val_acc: 0.7463\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5917 - acc: 0.7645 - val_loss: 0.6489 - val_acc: 0.7238\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5957 - acc: 0.7581 - val_loss: 0.6289 - val_acc: 0.7401\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5933 - acc: 0.7608 - val_loss: 0.6216 - val_acc: 0.7432\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5906 - acc: 0.7634 - val_loss: 0.6804 - val_acc: 0.7106\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5941 - acc: 0.7618 - val_loss: 0.6461 - val_acc: 0.7331\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5871 - acc: 0.7626 - val_loss: 0.6546 - val_acc: 0.7285\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5919 - acc: 0.7632 - val_loss: 0.6290 - val_acc: 0.7316\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5904 - acc: 0.7626 - val_loss: 0.6310 - val_acc: 0.7393\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5899 - acc: 0.7648 - val_loss: 0.6285 - val_acc: 0.7409\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5901 - acc: 0.7649 - val_loss: 0.6376 - val_acc: 0.7347\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 18s 86ms/step - loss: 0.5863 - acc: 0.7649 - val_loss: 0.6422 - val_acc: 0.7300\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5855 - acc: 0.7682 - val_loss: 0.6281 - val_acc: 0.7502\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5894 - acc: 0.7597 - val_loss: 0.6636 - val_acc: 0.7199\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5857 - acc: 0.7651 - val_loss: 0.6244 - val_acc: 0.7378\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5876 - acc: 0.7673 - val_loss: 0.6438 - val_acc: 0.7300\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5848 - acc: 0.7665 - val_loss: 0.6487 - val_acc: 0.7238\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5842 - acc: 0.7649 - val_loss: 0.6192 - val_acc: 0.7479\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5846 - acc: 0.7682 - val_loss: 0.6287 - val_acc: 0.7355\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5791 - acc: 0.7674 - val_loss: 0.6332 - val_acc: 0.7417\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5917 - acc: 0.7612 - val_loss: 0.6420 - val_acc: 0.7347\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5879 - acc: 0.7640 - val_loss: 0.6359 - val_acc: 0.7362\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5822 - acc: 0.7654 - val_loss: 0.6366 - val_acc: 0.7246\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5790 - acc: 0.7668 - val_loss: 0.6124 - val_acc: 0.7486\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5766 - acc: 0.7707 - val_loss: 0.6444 - val_acc: 0.7370\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5865 - acc: 0.7651 - val_loss: 0.6285 - val_acc: 0.7448\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5759 - acc: 0.7725 - val_loss: 0.6155 - val_acc: 0.7471\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5817 - acc: 0.7657 - val_loss: 0.6215 - val_acc: 0.7455\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5824 - acc: 0.7659 - val_loss: 0.6280 - val_acc: 0.7471\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5749 - acc: 0.7739 - val_loss: 0.6217 - val_acc: 0.7440\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5802 - acc: 0.7659 - val_loss: 0.6383 - val_acc: 0.7261\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5807 - acc: 0.7673 - val_loss: 0.6201 - val_acc: 0.7471\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5712 - acc: 0.7756 - val_loss: 0.6141 - val_acc: 0.7494\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5756 - acc: 0.7739 - val_loss: 0.6125 - val_acc: 0.7440\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5761 - acc: 0.7688 - val_loss: 0.6094 - val_acc: 0.7502\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5847 - acc: 0.7634 - val_loss: 0.6912 - val_acc: 0.7005\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5755 - acc: 0.7714 - val_loss: 0.6333 - val_acc: 0.7424\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5752 - acc: 0.7728 - val_loss: 0.6088 - val_acc: 0.7494\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5715 - acc: 0.7725 - val_loss: 0.6101 - val_acc: 0.7533\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5748 - acc: 0.7674 - val_loss: 0.6479 - val_acc: 0.7378\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5754 - acc: 0.7694 - val_loss: 0.6070 - val_acc: 0.7440\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5774 - acc: 0.7718 - val_loss: 0.6099 - val_acc: 0.7424\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5689 - acc: 0.7742 - val_loss: 0.6106 - val_acc: 0.7502\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5675 - acc: 0.7710 - val_loss: 0.6216 - val_acc: 0.7479\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5686 - acc: 0.7736 - val_loss: 0.6407 - val_acc: 0.7254\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5672 - acc: 0.7766 - val_loss: 0.6040 - val_acc: 0.7548\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5704 - acc: 0.7738 - val_loss: 0.6121 - val_acc: 0.7370\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5753 - acc: 0.7696 - val_loss: 0.6062 - val_acc: 0.7432\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5707 - acc: 0.7725 - val_loss: 0.6012 - val_acc: 0.7525\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5724 - acc: 0.7702 - val_loss: 0.6451 - val_acc: 0.7261\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5632 - acc: 0.7784 - val_loss: 0.6186 - val_acc: 0.7432\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5634 - acc: 0.7753 - val_loss: 0.6182 - val_acc: 0.7448\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 18s 86ms/step - loss: 0.5658 - acc: 0.7803 - val_loss: 0.6013 - val_acc: 0.7486\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5664 - acc: 0.7739 - val_loss: 0.6213 - val_acc: 0.7393\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5692 - acc: 0.7733 - val_loss: 0.5979 - val_acc: 0.7479\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5663 - acc: 0.7741 - val_loss: 0.6004 - val_acc: 0.7486\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5681 - acc: 0.7725 - val_loss: 0.6043 - val_acc: 0.7533\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5608 - acc: 0.7800 - val_loss: 0.5959 - val_acc: 0.7541\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5669 - acc: 0.7781 - val_loss: 0.6090 - val_acc: 0.7502\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5631 - acc: 0.7741 - val_loss: 0.6198 - val_acc: 0.7401\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5644 - acc: 0.7778 - val_loss: 0.6379 - val_acc: 0.7261\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5683 - acc: 0.7707 - val_loss: 0.5958 - val_acc: 0.7580\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5606 - acc: 0.7789 - val_loss: 0.5922 - val_acc: 0.7541\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5599 - acc: 0.7810 - val_loss: 0.5948 - val_acc: 0.7580\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5650 - acc: 0.7764 - val_loss: 0.5987 - val_acc: 0.7595\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5692 - acc: 0.7710 - val_loss: 0.6287 - val_acc: 0.7463\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5668 - acc: 0.7783 - val_loss: 0.6222 - val_acc: 0.7409\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5552 - acc: 0.7809 - val_loss: 0.5985 - val_acc: 0.7510\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5668 - acc: 0.7762 - val_loss: 0.5988 - val_acc: 0.7618\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5618 - acc: 0.7790 - val_loss: 0.5893 - val_acc: 0.7595\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5541 - acc: 0.7814 - val_loss: 0.6177 - val_acc: 0.7362\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5589 - acc: 0.7787 - val_loss: 0.6464 - val_acc: 0.7269\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5554 - acc: 0.7831 - val_loss: 0.6354 - val_acc: 0.7331\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5589 - acc: 0.7767 - val_loss: 0.6011 - val_acc: 0.7649\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5551 - acc: 0.7820 - val_loss: 0.5967 - val_acc: 0.7448\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5591 - acc: 0.7736 - val_loss: 0.5936 - val_acc: 0.7634\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5568 - acc: 0.7773 - val_loss: 0.5861 - val_acc: 0.7595\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5524 - acc: 0.7810 - val_loss: 0.5955 - val_acc: 0.7634\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5555 - acc: 0.7781 - val_loss: 0.5952 - val_acc: 0.7479\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5561 - acc: 0.7772 - val_loss: 0.6050 - val_acc: 0.7517\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5547 - acc: 0.7817 - val_loss: 0.5967 - val_acc: 0.7572\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5491 - acc: 0.7820 - val_loss: 0.6239 - val_acc: 0.7355\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5516 - acc: 0.7767 - val_loss: 0.6338 - val_acc: 0.7409\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5497 - acc: 0.7848 - val_loss: 0.6092 - val_acc: 0.7564\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5494 - acc: 0.7857 - val_loss: 0.6090 - val_acc: 0.7448\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5505 - acc: 0.7852 - val_loss: 0.5864 - val_acc: 0.7533\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5490 - acc: 0.7835 - val_loss: 0.5890 - val_acc: 0.7649\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5486 - acc: 0.7800 - val_loss: 0.6137 - val_acc: 0.7486\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5485 - acc: 0.7872 - val_loss: 0.5976 - val_acc: 0.7649\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5498 - acc: 0.7824 - val_loss: 0.6181 - val_acc: 0.7564\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5496 - acc: 0.7843 - val_loss: 0.5935 - val_acc: 0.7603\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.5491 - acc: 0.7817 - val_loss: 0.5838 - val_acc: 0.7642\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5458 - acc: 0.7868 - val_loss: 0.6374 - val_acc: 0.7502\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5506 - acc: 0.7848 - val_loss: 0.5849 - val_acc: 0.7665\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5412 - acc: 0.7897 - val_loss: 0.5816 - val_acc: 0.7618\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5467 - acc: 0.7851 - val_loss: 0.5920 - val_acc: 0.7580\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5450 - acc: 0.7841 - val_loss: 0.6171 - val_acc: 0.7595\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5490 - acc: 0.7831 - val_loss: 0.6340 - val_acc: 0.7347\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 18s 88ms/step - loss: 0.5465 - acc: 0.7840 - val_loss: 0.5786 - val_acc: 0.7649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yfHAt__1P1k",
        "outputId": "32be5c24-72b3-414a-8d1b-fb0ab98105a5"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml3', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights3.h5')\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5477 - acc: 0.7857 - val_loss: 0.6064 - val_acc: 0.7448\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5435 - acc: 0.7885 - val_loss: 0.6014 - val_acc: 0.7486\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5442 - acc: 0.7821 - val_loss: 0.6428 - val_acc: 0.7362\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5430 - acc: 0.7857 - val_loss: 0.5833 - val_acc: 0.7735\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5434 - acc: 0.7877 - val_loss: 0.5816 - val_acc: 0.7626\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5381 - acc: 0.7900 - val_loss: 0.5964 - val_acc: 0.7494\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5454 - acc: 0.7845 - val_loss: 0.5884 - val_acc: 0.7603\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5389 - acc: 0.7916 - val_loss: 0.5853 - val_acc: 0.7727\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5378 - acc: 0.7902 - val_loss: 0.6127 - val_acc: 0.7409\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5428 - acc: 0.7846 - val_loss: 0.6526 - val_acc: 0.7215\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5479 - acc: 0.7809 - val_loss: 0.6120 - val_acc: 0.7486\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5356 - acc: 0.7886 - val_loss: 0.6495 - val_acc: 0.7261\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5385 - acc: 0.7889 - val_loss: 0.5817 - val_acc: 0.7580\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5381 - acc: 0.7858 - val_loss: 0.5889 - val_acc: 0.7766\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5411 - acc: 0.7849 - val_loss: 0.6253 - val_acc: 0.7517\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5356 - acc: 0.7924 - val_loss: 0.6211 - val_acc: 0.7347\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5393 - acc: 0.7860 - val_loss: 0.5948 - val_acc: 0.7642\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5345 - acc: 0.7908 - val_loss: 0.5810 - val_acc: 0.7781\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5322 - acc: 0.7934 - val_loss: 0.5745 - val_acc: 0.7719\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5334 - acc: 0.7900 - val_loss: 0.5851 - val_acc: 0.7618\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5327 - acc: 0.7959 - val_loss: 0.5785 - val_acc: 0.7673\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5356 - acc: 0.7897 - val_loss: 0.5875 - val_acc: 0.7587\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5402 - acc: 0.7894 - val_loss: 0.5855 - val_acc: 0.7642\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5328 - acc: 0.7920 - val_loss: 0.5863 - val_acc: 0.7657\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5329 - acc: 0.7947 - val_loss: 0.5798 - val_acc: 0.7719\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5305 - acc: 0.7922 - val_loss: 0.5966 - val_acc: 0.7603\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5411 - acc: 0.7838 - val_loss: 0.5861 - val_acc: 0.7673\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5305 - acc: 0.7928 - val_loss: 0.5896 - val_acc: 0.7688\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5343 - acc: 0.7916 - val_loss: 0.6311 - val_acc: 0.7463\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5347 - acc: 0.7866 - val_loss: 0.6423 - val_acc: 0.7463\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5303 - acc: 0.7930 - val_loss: 0.5700 - val_acc: 0.7773\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5336 - acc: 0.7925 - val_loss: 0.5633 - val_acc: 0.7719\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5337 - acc: 0.7914 - val_loss: 0.6069 - val_acc: 0.7634\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5311 - acc: 0.7931 - val_loss: 0.5934 - val_acc: 0.7525\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5360 - acc: 0.7893 - val_loss: 0.6110 - val_acc: 0.7502\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5275 - acc: 0.7942 - val_loss: 0.5613 - val_acc: 0.7789\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5324 - acc: 0.7944 - val_loss: 0.5995 - val_acc: 0.7603\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5352 - acc: 0.7897 - val_loss: 0.5809 - val_acc: 0.7634\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5246 - acc: 0.7998 - val_loss: 0.5721 - val_acc: 0.7797\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5330 - acc: 0.7937 - val_loss: 0.5675 - val_acc: 0.7766\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5249 - acc: 0.7910 - val_loss: 0.5892 - val_acc: 0.7502\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5276 - acc: 0.7945 - val_loss: 0.5889 - val_acc: 0.7642\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5250 - acc: 0.7975 - val_loss: 0.5895 - val_acc: 0.7486\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5255 - acc: 0.7959 - val_loss: 0.5704 - val_acc: 0.7750\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5238 - acc: 0.7967 - val_loss: 0.5675 - val_acc: 0.7673\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5320 - acc: 0.7968 - val_loss: 0.5909 - val_acc: 0.7471\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5283 - acc: 0.7924 - val_loss: 0.6124 - val_acc: 0.7548\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5239 - acc: 0.7962 - val_loss: 0.5727 - val_acc: 0.7642\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5192 - acc: 0.8007 - val_loss: 0.5818 - val_acc: 0.7603\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5236 - acc: 0.7950 - val_loss: 0.5909 - val_acc: 0.7556\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5217 - acc: 0.7970 - val_loss: 0.5748 - val_acc: 0.7657\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5261 - acc: 0.7978 - val_loss: 0.5678 - val_acc: 0.7797\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5200 - acc: 0.7982 - val_loss: 0.5823 - val_acc: 0.7510\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5222 - acc: 0.8001 - val_loss: 0.5740 - val_acc: 0.7548\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5311 - acc: 0.7913 - val_loss: 0.5700 - val_acc: 0.7680\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5188 - acc: 0.7996 - val_loss: 0.6086 - val_acc: 0.7440\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5217 - acc: 0.7950 - val_loss: 0.5645 - val_acc: 0.7766\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5190 - acc: 0.7989 - val_loss: 0.5689 - val_acc: 0.7673\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5194 - acc: 0.7964 - val_loss: 0.5796 - val_acc: 0.7696\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5221 - acc: 0.7951 - val_loss: 0.6381 - val_acc: 0.7300\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5271 - acc: 0.7936 - val_loss: 0.5717 - val_acc: 0.7673\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5208 - acc: 0.7941 - val_loss: 0.6356 - val_acc: 0.7502\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5243 - acc: 0.7927 - val_loss: 0.5688 - val_acc: 0.7735\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5202 - acc: 0.7951 - val_loss: 0.5601 - val_acc: 0.7750\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5185 - acc: 0.7953 - val_loss: 0.5671 - val_acc: 0.7828\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5218 - acc: 0.7985 - val_loss: 0.5785 - val_acc: 0.7704\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5199 - acc: 0.7982 - val_loss: 0.5853 - val_acc: 0.7564\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5174 - acc: 0.7954 - val_loss: 0.5959 - val_acc: 0.7673\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5225 - acc: 0.7959 - val_loss: 0.5589 - val_acc: 0.7812\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5166 - acc: 0.8012 - val_loss: 0.5788 - val_acc: 0.7711\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5198 - acc: 0.7992 - val_loss: 0.5571 - val_acc: 0.7773\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5134 - acc: 0.8024 - val_loss: 0.5615 - val_acc: 0.7727\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5158 - acc: 0.8007 - val_loss: 0.5699 - val_acc: 0.7750\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5126 - acc: 0.8004 - val_loss: 0.5620 - val_acc: 0.7680\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5188 - acc: 0.7978 - val_loss: 0.5634 - val_acc: 0.7727\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5118 - acc: 0.8004 - val_loss: 0.5843 - val_acc: 0.7548\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5097 - acc: 0.8041 - val_loss: 0.5527 - val_acc: 0.7836\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5211 - acc: 0.7953 - val_loss: 0.5641 - val_acc: 0.7797\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5129 - acc: 0.7970 - val_loss: 0.5810 - val_acc: 0.7634\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5185 - acc: 0.7993 - val_loss: 0.5806 - val_acc: 0.7673\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5145 - acc: 0.7972 - val_loss: 0.5779 - val_acc: 0.7797\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5109 - acc: 0.8020 - val_loss: 0.5619 - val_acc: 0.7742\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5137 - acc: 0.8040 - val_loss: 0.6122 - val_acc: 0.7525\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5145 - acc: 0.8016 - val_loss: 0.5752 - val_acc: 0.7688\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5091 - acc: 0.8023 - val_loss: 0.5729 - val_acc: 0.7704\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5154 - acc: 0.8002 - val_loss: 0.5859 - val_acc: 0.7634\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5121 - acc: 0.8016 - val_loss: 0.6043 - val_acc: 0.7339\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5157 - acc: 0.7978 - val_loss: 0.5496 - val_acc: 0.7804\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5085 - acc: 0.8064 - val_loss: 0.6129 - val_acc: 0.7424\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5106 - acc: 0.8030 - val_loss: 0.5589 - val_acc: 0.7828\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5065 - acc: 0.8041 - val_loss: 0.5971 - val_acc: 0.7618\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5163 - acc: 0.7965 - val_loss: 0.5664 - val_acc: 0.7719\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5050 - acc: 0.8060 - val_loss: 0.5547 - val_acc: 0.7828\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5095 - acc: 0.7992 - val_loss: 0.5690 - val_acc: 0.7804\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5053 - acc: 0.8035 - val_loss: 0.6358 - val_acc: 0.7261\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5036 - acc: 0.8063 - val_loss: 0.5548 - val_acc: 0.7727\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5081 - acc: 0.8041 - val_loss: 0.5724 - val_acc: 0.7673\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5120 - acc: 0.8015 - val_loss: 0.5507 - val_acc: 0.7936\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5053 - acc: 0.8046 - val_loss: 0.5877 - val_acc: 0.7711\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5049 - acc: 0.8057 - val_loss: 0.5560 - val_acc: 0.7750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PEEpkRz1QF6",
        "outputId": "eadd13f2-a01a-45f2-ee60-2e5b06bd7e50"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml4', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights4.h5')\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5037 - acc: 0.8064 - val_loss: 0.5980 - val_acc: 0.7541\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5090 - acc: 0.8012 - val_loss: 0.5541 - val_acc: 0.7719\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5094 - acc: 0.8004 - val_loss: 0.5692 - val_acc: 0.7618\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5039 - acc: 0.8050 - val_loss: 0.5603 - val_acc: 0.7828\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 16s 82ms/step - loss: 0.5080 - acc: 0.8018 - val_loss: 0.5754 - val_acc: 0.7696\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 16s 80ms/step - loss: 0.5070 - acc: 0.8026 - val_loss: 0.5706 - val_acc: 0.7665\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5050 - acc: 0.8015 - val_loss: 0.5604 - val_acc: 0.7843\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5037 - acc: 0.8047 - val_loss: 0.5414 - val_acc: 0.7843\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5003 - acc: 0.8081 - val_loss: 0.5501 - val_acc: 0.7727\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5016 - acc: 0.8068 - val_loss: 0.5545 - val_acc: 0.7758\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5024 - acc: 0.8071 - val_loss: 0.5484 - val_acc: 0.7882\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5008 - acc: 0.8085 - val_loss: 0.5670 - val_acc: 0.7704\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5016 - acc: 0.8066 - val_loss: 0.5409 - val_acc: 0.7929\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5012 - acc: 0.8068 - val_loss: 0.5678 - val_acc: 0.7626\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4987 - acc: 0.8074 - val_loss: 0.5660 - val_acc: 0.7789\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5090 - acc: 0.8037 - val_loss: 0.5654 - val_acc: 0.7719\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5077 - acc: 0.8020 - val_loss: 0.5494 - val_acc: 0.7867\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 16s 80ms/step - loss: 0.4980 - acc: 0.8075 - val_loss: 0.5775 - val_acc: 0.7680\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4975 - acc: 0.8095 - val_loss: 0.5549 - val_acc: 0.7797\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5012 - acc: 0.8077 - val_loss: 0.5784 - val_acc: 0.7665\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5005 - acc: 0.8066 - val_loss: 0.5633 - val_acc: 0.7789\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5029 - acc: 0.7992 - val_loss: 0.5359 - val_acc: 0.7929\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5022 - acc: 0.8068 - val_loss: 0.5549 - val_acc: 0.7789\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4974 - acc: 0.8069 - val_loss: 0.5633 - val_acc: 0.7735\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4945 - acc: 0.8128 - val_loss: 0.5484 - val_acc: 0.7688\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4932 - acc: 0.8145 - val_loss: 0.5378 - val_acc: 0.7936\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4996 - acc: 0.8080 - val_loss: 0.5350 - val_acc: 0.7867\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4980 - acc: 0.8106 - val_loss: 0.5485 - val_acc: 0.7797\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4898 - acc: 0.8112 - val_loss: 0.5592 - val_acc: 0.7704\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4965 - acc: 0.8068 - val_loss: 0.5529 - val_acc: 0.7781\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4934 - acc: 0.8098 - val_loss: 0.5296 - val_acc: 0.7936\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4940 - acc: 0.8091 - val_loss: 0.5478 - val_acc: 0.7936\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4967 - acc: 0.8080 - val_loss: 0.5944 - val_acc: 0.7510\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4968 - acc: 0.8103 - val_loss: 0.5320 - val_acc: 0.7913\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4891 - acc: 0.8131 - val_loss: 0.5498 - val_acc: 0.7766\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4965 - acc: 0.8077 - val_loss: 0.5426 - val_acc: 0.7843\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4956 - acc: 0.8072 - val_loss: 0.5462 - val_acc: 0.7929\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4938 - acc: 0.8078 - val_loss: 0.5393 - val_acc: 0.7874\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4875 - acc: 0.8122 - val_loss: 0.5403 - val_acc: 0.7851\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4898 - acc: 0.8133 - val_loss: 0.5836 - val_acc: 0.7626\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4905 - acc: 0.8134 - val_loss: 0.5383 - val_acc: 0.7936\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4984 - acc: 0.8077 - val_loss: 0.5362 - val_acc: 0.7905\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4885 - acc: 0.8123 - val_loss: 0.5284 - val_acc: 0.7929\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4902 - acc: 0.8111 - val_loss: 0.6049 - val_acc: 0.7463\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4943 - acc: 0.8088 - val_loss: 0.5327 - val_acc: 0.7859\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4917 - acc: 0.8108 - val_loss: 0.5456 - val_acc: 0.7960\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4901 - acc: 0.8129 - val_loss: 0.5579 - val_acc: 0.7773\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4900 - acc: 0.8134 - val_loss: 0.5708 - val_acc: 0.7696\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4920 - acc: 0.8123 - val_loss: 0.5326 - val_acc: 0.7967\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4914 - acc: 0.8134 - val_loss: 0.5481 - val_acc: 0.7929\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4869 - acc: 0.8142 - val_loss: 0.5365 - val_acc: 0.7921\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4872 - acc: 0.8139 - val_loss: 0.5297 - val_acc: 0.7890\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4890 - acc: 0.8139 - val_loss: 0.5734 - val_acc: 0.7673\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4873 - acc: 0.8126 - val_loss: 0.5481 - val_acc: 0.7874\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4911 - acc: 0.8091 - val_loss: 0.5298 - val_acc: 0.7913\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4878 - acc: 0.8131 - val_loss: 0.5255 - val_acc: 0.7929\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4852 - acc: 0.8162 - val_loss: 0.5317 - val_acc: 0.7991\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4828 - acc: 0.8157 - val_loss: 0.5352 - val_acc: 0.7921\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4840 - acc: 0.8139 - val_loss: 0.5321 - val_acc: 0.7975\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4817 - acc: 0.8191 - val_loss: 0.5215 - val_acc: 0.7991\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4849 - acc: 0.8134 - val_loss: 0.5443 - val_acc: 0.7882\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4858 - acc: 0.8157 - val_loss: 0.5365 - val_acc: 0.7905\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4832 - acc: 0.8156 - val_loss: 0.5374 - val_acc: 0.7851\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4819 - acc: 0.8188 - val_loss: 0.5652 - val_acc: 0.7649\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4871 - acc: 0.8129 - val_loss: 0.5408 - val_acc: 0.7960\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4816 - acc: 0.8177 - val_loss: 0.5695 - val_acc: 0.7727\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4851 - acc: 0.8137 - val_loss: 0.5497 - val_acc: 0.7882\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4886 - acc: 0.8083 - val_loss: 0.5838 - val_acc: 0.7719\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4753 - acc: 0.8213 - val_loss: 0.5341 - val_acc: 0.7921\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4825 - acc: 0.8133 - val_loss: 0.5368 - val_acc: 0.7851\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4869 - acc: 0.8133 - val_loss: 0.5601 - val_acc: 0.7742\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4835 - acc: 0.8148 - val_loss: 0.5246 - val_acc: 0.7983\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4761 - acc: 0.8159 - val_loss: 0.5354 - val_acc: 0.7952\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4818 - acc: 0.8128 - val_loss: 0.5375 - val_acc: 0.7874\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4818 - acc: 0.8177 - val_loss: 0.5357 - val_acc: 0.7882\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4849 - acc: 0.8160 - val_loss: 0.5242 - val_acc: 0.7991\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4782 - acc: 0.8199 - val_loss: 0.5174 - val_acc: 0.8006\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4788 - acc: 0.8165 - val_loss: 0.5997 - val_acc: 0.7572\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4858 - acc: 0.8120 - val_loss: 0.5320 - val_acc: 0.7967\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4822 - acc: 0.8129 - val_loss: 0.5184 - val_acc: 0.8006\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4778 - acc: 0.8207 - val_loss: 0.5527 - val_acc: 0.7680\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4759 - acc: 0.8215 - val_loss: 0.5223 - val_acc: 0.7890\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4889 - acc: 0.8136 - val_loss: 0.5371 - val_acc: 0.7952\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4753 - acc: 0.8210 - val_loss: 0.5156 - val_acc: 0.8029\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4798 - acc: 0.8174 - val_loss: 0.5712 - val_acc: 0.7673\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4769 - acc: 0.8198 - val_loss: 0.5176 - val_acc: 0.8161\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4755 - acc: 0.8170 - val_loss: 0.5250 - val_acc: 0.8068\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4730 - acc: 0.8221 - val_loss: 0.5291 - val_acc: 0.7898\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4765 - acc: 0.8199 - val_loss: 0.5646 - val_acc: 0.7673\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4762 - acc: 0.8198 - val_loss: 0.5190 - val_acc: 0.7960\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4749 - acc: 0.8208 - val_loss: 0.5323 - val_acc: 0.7913\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4751 - acc: 0.8188 - val_loss: 0.5141 - val_acc: 0.7975\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4752 - acc: 0.8168 - val_loss: 0.5356 - val_acc: 0.7867\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4792 - acc: 0.8177 - val_loss: 0.5437 - val_acc: 0.7921\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4788 - acc: 0.8170 - val_loss: 0.5241 - val_acc: 0.7921\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4717 - acc: 0.8208 - val_loss: 0.5238 - val_acc: 0.7929\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4736 - acc: 0.8194 - val_loss: 0.5809 - val_acc: 0.7657\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4736 - acc: 0.8165 - val_loss: 0.5342 - val_acc: 0.7828\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4742 - acc: 0.8185 - val_loss: 0.5382 - val_acc: 0.7804\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4735 - acc: 0.8177 - val_loss: 0.5182 - val_acc: 0.7998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaG3plOd1QVr",
        "outputId": "8c6d5118-7120-445d-9707-e69c87959ed8"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml5', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights5.h5')\r\n",
        "\r\n",
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=30,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4693 - acc: 0.8198 - val_loss: 0.5263 - val_acc: 0.7913\n",
            "Epoch 2/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4698 - acc: 0.8212 - val_loss: 0.5396 - val_acc: 0.7859\n",
            "Epoch 3/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4733 - acc: 0.8221 - val_loss: 0.5315 - val_acc: 0.7773\n",
            "Epoch 4/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4721 - acc: 0.8174 - val_loss: 0.5202 - val_acc: 0.8068\n",
            "Epoch 5/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4681 - acc: 0.8253 - val_loss: 0.5300 - val_acc: 0.7781\n",
            "Epoch 6/30\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4718 - acc: 0.8222 - val_loss: 0.5278 - val_acc: 0.7929\n",
            "Epoch 7/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4756 - acc: 0.8170 - val_loss: 0.5344 - val_acc: 0.7843\n",
            "Epoch 8/30\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4721 - acc: 0.8198 - val_loss: 0.5286 - val_acc: 0.7898\n",
            "Epoch 9/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4667 - acc: 0.8236 - val_loss: 0.5236 - val_acc: 0.7929\n",
            "Epoch 10/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4645 - acc: 0.8232 - val_loss: 0.5284 - val_acc: 0.7921\n",
            "Epoch 11/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4695 - acc: 0.8205 - val_loss: 0.5177 - val_acc: 0.8045\n",
            "Epoch 12/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4753 - acc: 0.8185 - val_loss: 0.5188 - val_acc: 0.8092\n",
            "Epoch 13/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4742 - acc: 0.8191 - val_loss: 0.5265 - val_acc: 0.8037\n",
            "Epoch 14/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4683 - acc: 0.8218 - val_loss: 0.5185 - val_acc: 0.7975\n",
            "Epoch 15/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4651 - acc: 0.8218 - val_loss: 0.5434 - val_acc: 0.7836\n",
            "Epoch 16/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4744 - acc: 0.8210 - val_loss: 0.5315 - val_acc: 0.7929\n",
            "Epoch 17/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4663 - acc: 0.8246 - val_loss: 0.5131 - val_acc: 0.8022\n",
            "Epoch 18/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4707 - acc: 0.8253 - val_loss: 0.5132 - val_acc: 0.8045\n",
            "Epoch 19/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4712 - acc: 0.8179 - val_loss: 0.5184 - val_acc: 0.8053\n",
            "Epoch 20/30\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4684 - acc: 0.8247 - val_loss: 0.5266 - val_acc: 0.7983\n",
            "Epoch 21/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4631 - acc: 0.8249 - val_loss: 0.5422 - val_acc: 0.7843\n",
            "Epoch 22/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4705 - acc: 0.8168 - val_loss: 0.5053 - val_acc: 0.8107\n",
            "Epoch 23/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4617 - acc: 0.8308 - val_loss: 0.5017 - val_acc: 0.8045\n",
            "Epoch 24/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4624 - acc: 0.8255 - val_loss: 0.5210 - val_acc: 0.7944\n",
            "Epoch 25/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4651 - acc: 0.8224 - val_loss: 0.5483 - val_acc: 0.7882\n",
            "Epoch 26/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4668 - acc: 0.8242 - val_loss: 0.5205 - val_acc: 0.7898\n",
            "Epoch 27/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4692 - acc: 0.8227 - val_loss: 0.5507 - val_acc: 0.7797\n",
            "Epoch 28/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4614 - acc: 0.8260 - val_loss: 0.5596 - val_acc: 0.7704\n",
            "Epoch 29/30\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4618 - acc: 0.8242 - val_loss: 0.5234 - val_acc: 0.7960\n",
            "Epoch 30/30\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4638 - acc: 0.8233 - val_loss: 0.5613 - val_acc: 0.7657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlZDConz1QlY",
        "outputId": "e0b98208-562e-4f31-9714-f06c5cc94c4c"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml6', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights6.h5')\r\n",
        "\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import model_from_yaml\r\n",
        "# 加载模型结构\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models/\r\n",
        "model33 = model_from_yaml(open('model_architecture.yaml6').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "# 加载模型参数\r\n",
        "model33.load_weights('model_weights6.h5')\r\n",
        "model33.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist33 = model33.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=3,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4d61836a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4d61836a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fe4d61836a8> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - ETA: 0s - loss: 0.5961 - acc: 0.7627WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4791fde18> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4791fde18> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fe4791fde18> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - 31s 94ms/step - loss: 0.5961 - acc: 0.7627 - val_loss: 0.7080 - val_acc: 0.7005\n",
            "Epoch 2/3\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6262 - acc: 0.7508 - val_loss: 0.6390 - val_acc: 0.7339\n",
            "Epoch 3/3\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6030 - acc: 0.7535 - val_loss: 0.6050 - val_acc: 0.7502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxZu66FcRO4w",
        "outputId": "9b94a2ee-e9da-4cd5-96d9-31d1720ec778"
      },
      "source": [
        "hist33 = model33.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=10,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5860 - acc: 0.7639 - val_loss: 0.6275 - val_acc: 0.7409\n",
            "Epoch 2/10\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5755 - acc: 0.7673 - val_loss: 0.5981 - val_acc: 0.7486\n",
            "Epoch 3/10\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5894 - acc: 0.7617 - val_loss: 0.7193 - val_acc: 0.7013\n",
            "Epoch 4/10\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5774 - acc: 0.7662 - val_loss: 0.6574 - val_acc: 0.7292\n",
            "Epoch 5/10\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5580 - acc: 0.7716 - val_loss: 0.5886 - val_acc: 0.7587\n",
            "Epoch 6/10\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5557 - acc: 0.7815 - val_loss: 0.6123 - val_acc: 0.7331\n",
            "Epoch 7/10\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5559 - acc: 0.7764 - val_loss: 0.5952 - val_acc: 0.7580\n",
            "Epoch 8/10\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5621 - acc: 0.7711 - val_loss: 0.5736 - val_acc: 0.7611\n",
            "Epoch 9/10\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5694 - acc: 0.7705 - val_loss: 0.5983 - val_acc: 0.7541\n",
            "Epoch 10/10\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.5576 - acc: 0.7764 - val_loss: 0.5803 - val_acc: 0.7618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM1OLQxyRPK0",
        "outputId": "36f345f4-2d19-409a-cd9e-829d82d601f5"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model_88.to_yaml()\r\n",
        "open('model_architecture.yaml6', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model_88.save_weights('model_weights6.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAWT9d57RPbm",
        "outputId": "8c48e25d-dcd3-4934-f9ef-03b170dde6e0"
      },
      "source": [
        "hist_88 = model_88.fit_generator(\r\n",
        "    train_generator, \r\n",
        "    epochs=3,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if len(self._training_endpoints) > 1:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.4657 - acc: 0.8213 - val_loss: 0.5801 - val_acc: 0.7634\n",
            "Epoch 2/3\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4657 - acc: 0.8266 - val_loss: 0.5269 - val_acc: 0.7944\n",
            "Epoch 3/3\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4570 - acc: 0.8317 - val_loss: 0.5189 - val_acc: 0.7913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU4hnJklSmCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b3105f-66cd-40c9-cc57-74a2ff9da23c"
      },
      "source": [
        "\r\n",
        "\r\n",
        "from keras.models import model_from_json\r\n",
        "from keras.models import model_from_yaml\r\n",
        "# 加载模型结构\r\n",
        "%cd /content/gdrive/MyDrive/colab/saved_models/\r\n",
        "model = model_from_yaml(open('model_architecture.yaml6').read(),custom_objects={'KerasLayer': hub.KerasLayer})\r\n",
        "# model = model_from_json(open('../docs/keras/model_architecture.json').read())\r\n",
        "\r\n",
        "\r\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\r\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\r\n",
        "# 加载模型参数\r\n",
        "model.load_weights('model_weights6.h5')\r\n",
        "model.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  metrics=['acc'])\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=3,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7efbd7d49c80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7efbd7d49c80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7efbd7d49c80> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - ETA: 0s - loss: 0.5633 - acc: 0.7731WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7efc303441e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7efc303441e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7efc303441e0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "202/202 [==============================] - 1789s 9s/step - loss: 0.5634 - acc: 0.7731 - val_loss: 0.7789 - val_acc: 0.7013\n",
            "Epoch 2/3\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6311 - acc: 0.7441 - val_loss: 0.6005 - val_acc: 0.7533\n",
            "Epoch 3/3\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.6419 - acc: 0.7398 - val_loss: 0.6268 - val_acc: 0.7378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k80Hr7_SmX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41229394-f343-49af-9f15-9c570fd8768d"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml7', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights7.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5661 - acc: 0.7730 - val_loss: 0.6995 - val_acc: 0.7122\n",
            "Epoch 2/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5887 - acc: 0.7563 - val_loss: 0.6798 - val_acc: 0.7083\n",
            "Epoch 3/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5375 - acc: 0.7828 - val_loss: 0.5831 - val_acc: 0.7657\n",
            "Epoch 4/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5741 - acc: 0.7635 - val_loss: 0.6598 - val_acc: 0.7300\n",
            "Epoch 5/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5703 - acc: 0.7688 - val_loss: 0.6463 - val_acc: 0.7525\n",
            "Epoch 6/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.6174 - acc: 0.7550 - val_loss: 0.8023 - val_acc: 0.6726\n",
            "Epoch 7/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5843 - acc: 0.7682 - val_loss: 0.6299 - val_acc: 0.7230\n",
            "Epoch 8/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5682 - acc: 0.7685 - val_loss: 0.6417 - val_acc: 0.7386\n",
            "Epoch 9/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5724 - acc: 0.7693 - val_loss: 0.5488 - val_acc: 0.7836\n",
            "Epoch 10/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5600 - acc: 0.7685 - val_loss: 0.8205 - val_acc: 0.6540\n",
            "Epoch 11/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5729 - acc: 0.7666 - val_loss: 0.8098 - val_acc: 0.6734\n",
            "Epoch 12/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5607 - acc: 0.7738 - val_loss: 0.5686 - val_acc: 0.7696\n",
            "Epoch 13/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5741 - acc: 0.7640 - val_loss: 0.6159 - val_acc: 0.7486\n",
            "Epoch 14/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5459 - acc: 0.7764 - val_loss: 0.6031 - val_acc: 0.7362\n",
            "Epoch 15/50\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.5696 - acc: 0.7654 - val_loss: 0.6042 - val_acc: 0.7494\n",
            "Epoch 16/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5610 - acc: 0.7728 - val_loss: 0.6339 - val_acc: 0.7238\n",
            "Epoch 17/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5709 - acc: 0.7708 - val_loss: 0.6656 - val_acc: 0.7285\n",
            "Epoch 18/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5723 - acc: 0.7656 - val_loss: 0.5668 - val_acc: 0.7587\n",
            "Epoch 19/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5495 - acc: 0.7761 - val_loss: 0.5769 - val_acc: 0.7665\n",
            "Epoch 20/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5439 - acc: 0.7784 - val_loss: 0.6319 - val_acc: 0.7308\n",
            "Epoch 21/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5395 - acc: 0.7814 - val_loss: 0.5788 - val_acc: 0.7611\n",
            "Epoch 22/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5487 - acc: 0.7761 - val_loss: 0.5857 - val_acc: 0.7548\n",
            "Epoch 23/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5496 - acc: 0.7783 - val_loss: 0.5568 - val_acc: 0.7649\n",
            "Epoch 24/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5506 - acc: 0.7731 - val_loss: 0.5921 - val_acc: 0.7479\n",
            "Epoch 25/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5551 - acc: 0.7759 - val_loss: 0.5747 - val_acc: 0.7618\n",
            "Epoch 26/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5475 - acc: 0.7747 - val_loss: 0.5741 - val_acc: 0.7634\n",
            "Epoch 27/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5591 - acc: 0.7707 - val_loss: 0.5712 - val_acc: 0.7742\n",
            "Epoch 28/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5393 - acc: 0.7776 - val_loss: 0.6284 - val_acc: 0.7440\n",
            "Epoch 29/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5862 - acc: 0.7639 - val_loss: 0.6070 - val_acc: 0.7401\n",
            "Epoch 30/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5741 - acc: 0.7651 - val_loss: 0.6022 - val_acc: 0.7541\n",
            "Epoch 31/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5578 - acc: 0.7719 - val_loss: 0.5874 - val_acc: 0.7595\n",
            "Epoch 32/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5491 - acc: 0.7767 - val_loss: 0.5934 - val_acc: 0.7595\n",
            "Epoch 33/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5465 - acc: 0.7824 - val_loss: 0.6291 - val_acc: 0.7479\n",
            "Epoch 34/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5452 - acc: 0.7806 - val_loss: 0.5686 - val_acc: 0.7727\n",
            "Epoch 35/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5407 - acc: 0.7772 - val_loss: 0.5827 - val_acc: 0.7649\n",
            "Epoch 36/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5433 - acc: 0.7773 - val_loss: 0.5512 - val_acc: 0.7735\n",
            "Epoch 37/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5499 - acc: 0.7792 - val_loss: 0.5694 - val_acc: 0.7502\n",
            "Epoch 38/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5440 - acc: 0.7758 - val_loss: 0.5855 - val_acc: 0.7603\n",
            "Epoch 39/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5223 - acc: 0.7902 - val_loss: 0.5831 - val_acc: 0.7626\n",
            "Epoch 40/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5259 - acc: 0.7897 - val_loss: 0.6367 - val_acc: 0.7347\n",
            "Epoch 41/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5752 - acc: 0.7668 - val_loss: 0.6139 - val_acc: 0.7401\n",
            "Epoch 42/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5298 - acc: 0.7877 - val_loss: 0.6111 - val_acc: 0.7494\n",
            "Epoch 43/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5378 - acc: 0.7855 - val_loss: 0.5860 - val_acc: 0.7541\n",
            "Epoch 44/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5538 - acc: 0.7786 - val_loss: 0.5846 - val_acc: 0.7649\n",
            "Epoch 45/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5312 - acc: 0.7882 - val_loss: 0.6071 - val_acc: 0.7525\n",
            "Epoch 46/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5402 - acc: 0.7818 - val_loss: 0.5923 - val_acc: 0.7548\n",
            "Epoch 47/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5564 - acc: 0.7773 - val_loss: 0.5802 - val_acc: 0.7704\n",
            "Epoch 48/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5408 - acc: 0.7831 - val_loss: 0.5904 - val_acc: 0.7525\n",
            "Epoch 49/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5493 - acc: 0.7803 - val_loss: 0.6088 - val_acc: 0.7448\n",
            "Epoch 50/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5208 - acc: 0.7910 - val_loss: 0.5805 - val_acc: 0.7455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUZZWNEiyC5U",
        "outputId": "8543b4dd-5b72-4d3b-bef2-5bd8bf25e622"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml8', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights8.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5387 - acc: 0.7845 - val_loss: 0.6018 - val_acc: 0.7455\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5454 - acc: 0.7801 - val_loss: 0.5821 - val_acc: 0.7696\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5399 - acc: 0.7840 - val_loss: 0.5330 - val_acc: 0.7843\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5224 - acc: 0.7872 - val_loss: 0.5879 - val_acc: 0.7502\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5195 - acc: 0.7920 - val_loss: 0.5837 - val_acc: 0.7533\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5253 - acc: 0.7965 - val_loss: 0.5634 - val_acc: 0.7704\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5360 - acc: 0.7848 - val_loss: 0.6034 - val_acc: 0.7471\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5460 - acc: 0.7759 - val_loss: 0.5459 - val_acc: 0.7804\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5218 - acc: 0.7961 - val_loss: 0.5614 - val_acc: 0.7727\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5214 - acc: 0.7916 - val_loss: 0.6224 - val_acc: 0.7494\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5403 - acc: 0.7781 - val_loss: 0.5803 - val_acc: 0.7665\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5294 - acc: 0.7944 - val_loss: 0.5703 - val_acc: 0.7750\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5057 - acc: 0.8012 - val_loss: 0.5245 - val_acc: 0.7905\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5143 - acc: 0.7981 - val_loss: 0.5243 - val_acc: 0.7952\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5403 - acc: 0.7810 - val_loss: 0.6226 - val_acc: 0.7324\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5126 - acc: 0.7962 - val_loss: 0.5493 - val_acc: 0.7867\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5204 - acc: 0.7902 - val_loss: 0.5532 - val_acc: 0.7789\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5360 - acc: 0.7831 - val_loss: 0.6137 - val_acc: 0.7517\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5458 - acc: 0.7795 - val_loss: 0.5440 - val_acc: 0.7766\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.5127 - acc: 0.7965 - val_loss: 0.5393 - val_acc: 0.7812\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5284 - acc: 0.7860 - val_loss: 0.5480 - val_acc: 0.7696\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5084 - acc: 0.7967 - val_loss: 0.6258 - val_acc: 0.7417\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5433 - acc: 0.7838 - val_loss: 0.6076 - val_acc: 0.7409\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5274 - acc: 0.7874 - val_loss: 0.6236 - val_acc: 0.7502\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5266 - acc: 0.7866 - val_loss: 0.6196 - val_acc: 0.7316\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5321 - acc: 0.7865 - val_loss: 0.5890 - val_acc: 0.7649\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5267 - acc: 0.7913 - val_loss: 0.5501 - val_acc: 0.7727\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5181 - acc: 0.7928 - val_loss: 0.6209 - val_acc: 0.7362\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5325 - acc: 0.7894 - val_loss: 0.5337 - val_acc: 0.7905\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5073 - acc: 0.7981 - val_loss: 0.5260 - val_acc: 0.7828\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5126 - acc: 0.7970 - val_loss: 0.5470 - val_acc: 0.7859\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5105 - acc: 0.7967 - val_loss: 0.5760 - val_acc: 0.7657\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5087 - acc: 0.7947 - val_loss: 0.5002 - val_acc: 0.8115\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5217 - acc: 0.7954 - val_loss: 0.5915 - val_acc: 0.7580\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5152 - acc: 0.7967 - val_loss: 0.5217 - val_acc: 0.7905\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5040 - acc: 0.8027 - val_loss: 0.6551 - val_acc: 0.7161\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5267 - acc: 0.7910 - val_loss: 0.5867 - val_acc: 0.7587\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5054 - acc: 0.8018 - val_loss: 0.6113 - val_acc: 0.7525\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5258 - acc: 0.7930 - val_loss: 0.5844 - val_acc: 0.7541\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5185 - acc: 0.7896 - val_loss: 0.5503 - val_acc: 0.7820\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5444 - acc: 0.7851 - val_loss: 0.6708 - val_acc: 0.7300\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5445 - acc: 0.7775 - val_loss: 0.5706 - val_acc: 0.7611\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5120 - acc: 0.7941 - val_loss: 0.5618 - val_acc: 0.7773\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5378 - acc: 0.7882 - val_loss: 0.6566 - val_acc: 0.7580\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5040 - acc: 0.8030 - val_loss: 0.5108 - val_acc: 0.8045\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5180 - acc: 0.7939 - val_loss: 0.6969 - val_acc: 0.7215\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5096 - acc: 0.7962 - val_loss: 0.5715 - val_acc: 0.7626\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5104 - acc: 0.7948 - val_loss: 0.5532 - val_acc: 0.7649\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5018 - acc: 0.8009 - val_loss: 0.5410 - val_acc: 0.7797\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4971 - acc: 0.8029 - val_loss: 0.5362 - val_acc: 0.7804\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4958 - acc: 0.8055 - val_loss: 0.5369 - val_acc: 0.7773\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5015 - acc: 0.8020 - val_loss: 0.5542 - val_acc: 0.7719\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5034 - acc: 0.8043 - val_loss: 0.5850 - val_acc: 0.7541\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4906 - acc: 0.8091 - val_loss: 0.5125 - val_acc: 0.7998\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5046 - acc: 0.8035 - val_loss: 0.5684 - val_acc: 0.7711\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4986 - acc: 0.8044 - val_loss: 0.5289 - val_acc: 0.7967\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4835 - acc: 0.8143 - val_loss: 0.5704 - val_acc: 0.7688\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5148 - acc: 0.7942 - val_loss: 0.5999 - val_acc: 0.7502\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4901 - acc: 0.8033 - val_loss: 0.5325 - val_acc: 0.7967\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4956 - acc: 0.8001 - val_loss: 0.5179 - val_acc: 0.7952\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4892 - acc: 0.8060 - val_loss: 0.5567 - val_acc: 0.7711\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4991 - acc: 0.8046 - val_loss: 0.6726 - val_acc: 0.7230\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5029 - acc: 0.8007 - val_loss: 0.5400 - val_acc: 0.7874\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5071 - acc: 0.7979 - val_loss: 0.5412 - val_acc: 0.7921\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4820 - acc: 0.8131 - val_loss: 0.4900 - val_acc: 0.8154\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5027 - acc: 0.8035 - val_loss: 0.5223 - val_acc: 0.7929\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5042 - acc: 0.8023 - val_loss: 0.4990 - val_acc: 0.8076\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4804 - acc: 0.8103 - val_loss: 0.5817 - val_acc: 0.7587\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5043 - acc: 0.7965 - val_loss: 0.5247 - val_acc: 0.7921\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4887 - acc: 0.8102 - val_loss: 0.5133 - val_acc: 0.7967\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4850 - acc: 0.8083 - val_loss: 0.5190 - val_acc: 0.7890\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4844 - acc: 0.8108 - val_loss: 0.5345 - val_acc: 0.7929\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5009 - acc: 0.8023 - val_loss: 0.5276 - val_acc: 0.7921\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5021 - acc: 0.7998 - val_loss: 0.5182 - val_acc: 0.7921\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4931 - acc: 0.8049 - val_loss: 0.7752 - val_acc: 0.6765\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4969 - acc: 0.8050 - val_loss: 0.5252 - val_acc: 0.7975\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.5027 - acc: 0.8020 - val_loss: 0.5770 - val_acc: 0.7696\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4953 - acc: 0.8043 - val_loss: 0.4938 - val_acc: 0.8185\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4810 - acc: 0.8120 - val_loss: 0.4983 - val_acc: 0.8107\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4653 - acc: 0.8222 - val_loss: 0.5163 - val_acc: 0.7913\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4897 - acc: 0.8074 - val_loss: 0.6406 - val_acc: 0.7386\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4905 - acc: 0.8075 - val_loss: 0.5092 - val_acc: 0.8068\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4934 - acc: 0.8063 - val_loss: 0.5024 - val_acc: 0.8037\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4844 - acc: 0.8071 - val_loss: 0.5095 - val_acc: 0.8061\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4714 - acc: 0.8179 - val_loss: 0.5218 - val_acc: 0.7952\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4906 - acc: 0.8117 - val_loss: 0.5562 - val_acc: 0.7766\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4736 - acc: 0.8167 - val_loss: 0.5960 - val_acc: 0.7572\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4849 - acc: 0.8077 - val_loss: 0.5125 - val_acc: 0.8099\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4762 - acc: 0.8160 - val_loss: 0.5066 - val_acc: 0.7929\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4739 - acc: 0.8146 - val_loss: 0.4910 - val_acc: 0.8029\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4799 - acc: 0.8114 - val_loss: 0.5418 - val_acc: 0.7828\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4942 - acc: 0.8060 - val_loss: 0.5242 - val_acc: 0.7851\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4911 - acc: 0.8072 - val_loss: 0.5797 - val_acc: 0.7688\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4875 - acc: 0.8066 - val_loss: 0.5490 - val_acc: 0.7789\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4883 - acc: 0.8092 - val_loss: 0.4853 - val_acc: 0.8192\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4647 - acc: 0.8176 - val_loss: 0.5113 - val_acc: 0.7975\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4878 - acc: 0.8080 - val_loss: 0.5077 - val_acc: 0.8022\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4740 - acc: 0.8162 - val_loss: 0.6414 - val_acc: 0.7339\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4930 - acc: 0.8091 - val_loss: 0.5012 - val_acc: 0.7967\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4857 - acc: 0.8098 - val_loss: 0.5638 - val_acc: 0.7680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV-kw5KoyDLn",
        "outputId": "025320b2-2f12-490d-a3e2-038794441e84"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml9', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights9.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4848 - acc: 0.8095 - val_loss: 0.5865 - val_acc: 0.7564\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4804 - acc: 0.8094 - val_loss: 0.4965 - val_acc: 0.8053\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4772 - acc: 0.8151 - val_loss: 0.5139 - val_acc: 0.7991\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4779 - acc: 0.8140 - val_loss: 0.6445 - val_acc: 0.7300\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4793 - acc: 0.8116 - val_loss: 0.5390 - val_acc: 0.7921\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4770 - acc: 0.8143 - val_loss: 0.5035 - val_acc: 0.8014\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4719 - acc: 0.8165 - val_loss: 0.5868 - val_acc: 0.7680\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5004 - acc: 0.8040 - val_loss: 0.4708 - val_acc: 0.8317\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4549 - acc: 0.8258 - val_loss: 0.4625 - val_acc: 0.8216\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4813 - acc: 0.8109 - val_loss: 0.6221 - val_acc: 0.7525\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4523 - acc: 0.8244 - val_loss: 0.5357 - val_acc: 0.7944\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4995 - acc: 0.8037 - val_loss: 0.5871 - val_acc: 0.7572\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4785 - acc: 0.8100 - val_loss: 0.5499 - val_acc: 0.7680\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5126 - acc: 0.7936 - val_loss: 0.5098 - val_acc: 0.8006\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5008 - acc: 0.7996 - val_loss: 0.4905 - val_acc: 0.7998\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4687 - acc: 0.8201 - val_loss: 0.5442 - val_acc: 0.7797\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4668 - acc: 0.8154 - val_loss: 0.5622 - val_acc: 0.7642\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4665 - acc: 0.8173 - val_loss: 0.4891 - val_acc: 0.8138\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4596 - acc: 0.8210 - val_loss: 0.4905 - val_acc: 0.8076\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4504 - acc: 0.8292 - val_loss: 0.5709 - val_acc: 0.7587\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4804 - acc: 0.8137 - val_loss: 0.5507 - val_acc: 0.7657\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.5052 - acc: 0.7998 - val_loss: 0.4867 - val_acc: 0.8146\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4422 - acc: 0.8309 - val_loss: 0.5601 - val_acc: 0.7812\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4858 - acc: 0.8058 - val_loss: 0.4885 - val_acc: 0.8045\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4728 - acc: 0.8164 - val_loss: 0.4895 - val_acc: 0.8177\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4763 - acc: 0.8114 - val_loss: 0.4968 - val_acc: 0.8053\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4960 - acc: 0.8054 - val_loss: 0.5016 - val_acc: 0.8076\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4576 - acc: 0.8201 - val_loss: 0.5173 - val_acc: 0.7859\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4542 - acc: 0.8227 - val_loss: 0.5665 - val_acc: 0.7704\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4622 - acc: 0.8196 - val_loss: 0.5064 - val_acc: 0.7851\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4662 - acc: 0.8218 - val_loss: 0.4739 - val_acc: 0.8254\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4411 - acc: 0.8363 - val_loss: 0.5082 - val_acc: 0.7882\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4763 - acc: 0.8139 - val_loss: 0.5280 - val_acc: 0.7812\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4838 - acc: 0.8088 - val_loss: 0.5406 - val_acc: 0.7851\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4648 - acc: 0.8162 - val_loss: 0.4850 - val_acc: 0.8161\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4630 - acc: 0.8238 - val_loss: 0.5409 - val_acc: 0.7882\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.5106 - acc: 0.8013 - val_loss: 0.5026 - val_acc: 0.7998\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4664 - acc: 0.8150 - val_loss: 0.4698 - val_acc: 0.8247\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4415 - acc: 0.8321 - val_loss: 0.5864 - val_acc: 0.7657\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4557 - acc: 0.8230 - val_loss: 0.5016 - val_acc: 0.7944\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4790 - acc: 0.8139 - val_loss: 0.6086 - val_acc: 0.7494\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4739 - acc: 0.8174 - val_loss: 0.4772 - val_acc: 0.8223\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4794 - acc: 0.8106 - val_loss: 0.4855 - val_acc: 0.8177\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4646 - acc: 0.8221 - val_loss: 0.4993 - val_acc: 0.7960\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4565 - acc: 0.8224 - val_loss: 0.4911 - val_acc: 0.8115\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4499 - acc: 0.8246 - val_loss: 0.5693 - val_acc: 0.7711\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4609 - acc: 0.8210 - val_loss: 0.4963 - val_acc: 0.8029\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4523 - acc: 0.8239 - val_loss: 0.4733 - val_acc: 0.8053\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4389 - acc: 0.8338 - val_loss: 0.4567 - val_acc: 0.8278\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4572 - acc: 0.8216 - val_loss: 0.5043 - val_acc: 0.8053\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4626 - acc: 0.8165 - val_loss: 0.4560 - val_acc: 0.8254\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4650 - acc: 0.8202 - val_loss: 0.5335 - val_acc: 0.7828\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4731 - acc: 0.8140 - val_loss: 0.4990 - val_acc: 0.8014\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4385 - acc: 0.8304 - val_loss: 0.4791 - val_acc: 0.8185\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4534 - acc: 0.8249 - val_loss: 0.4613 - val_acc: 0.8317\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4631 - acc: 0.8198 - val_loss: 0.5144 - val_acc: 0.7905\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4603 - acc: 0.8222 - val_loss: 0.5219 - val_acc: 0.8029\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4494 - acc: 0.8294 - val_loss: 0.5825 - val_acc: 0.7611\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4875 - acc: 0.8041 - val_loss: 0.5410 - val_acc: 0.7898\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4515 - acc: 0.8264 - val_loss: 0.5023 - val_acc: 0.8037\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4378 - acc: 0.8332 - val_loss: 0.4964 - val_acc: 0.8006\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4312 - acc: 0.8382 - val_loss: 0.4597 - val_acc: 0.8177\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4370 - acc: 0.8356 - val_loss: 0.4841 - val_acc: 0.8200\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4614 - acc: 0.8267 - val_loss: 0.5548 - val_acc: 0.7890\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4487 - acc: 0.8242 - val_loss: 0.5296 - val_acc: 0.7836\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4489 - acc: 0.8278 - val_loss: 0.4636 - val_acc: 0.8247\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4811 - acc: 0.8129 - val_loss: 0.5176 - val_acc: 0.7960\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4606 - acc: 0.8185 - val_loss: 0.5317 - val_acc: 0.7960\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4553 - acc: 0.8253 - val_loss: 0.4951 - val_acc: 0.8053\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4569 - acc: 0.8227 - val_loss: 0.5764 - val_acc: 0.7766\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4425 - acc: 0.8334 - val_loss: 0.4904 - val_acc: 0.8037\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4401 - acc: 0.8323 - val_loss: 0.4481 - val_acc: 0.8285\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4344 - acc: 0.8352 - val_loss: 0.5153 - val_acc: 0.7921\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4467 - acc: 0.8295 - val_loss: 0.5677 - val_acc: 0.7828\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4717 - acc: 0.8159 - val_loss: 0.4927 - val_acc: 0.7967\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4504 - acc: 0.8264 - val_loss: 0.4674 - val_acc: 0.8239\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4325 - acc: 0.8314 - val_loss: 0.4611 - val_acc: 0.8223\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4282 - acc: 0.8362 - val_loss: 0.4724 - val_acc: 0.8138\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4643 - acc: 0.8188 - val_loss: 0.5916 - val_acc: 0.7611\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4362 - acc: 0.8357 - val_loss: 0.4783 - val_acc: 0.8053\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4374 - acc: 0.8326 - val_loss: 0.5155 - val_acc: 0.7905\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4430 - acc: 0.8281 - val_loss: 0.4728 - val_acc: 0.8068\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4487 - acc: 0.8252 - val_loss: 0.5183 - val_acc: 0.7890\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4535 - acc: 0.8252 - val_loss: 0.4491 - val_acc: 0.8379\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4254 - acc: 0.8368 - val_loss: 0.5218 - val_acc: 0.7851\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4402 - acc: 0.8278 - val_loss: 0.4964 - val_acc: 0.7936\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4353 - acc: 0.8342 - val_loss: 0.4694 - val_acc: 0.8022\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4510 - acc: 0.8292 - val_loss: 0.4796 - val_acc: 0.8161\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4346 - acc: 0.8328 - val_loss: 0.4503 - val_acc: 0.8192\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4334 - acc: 0.8338 - val_loss: 0.5187 - val_acc: 0.7836\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4553 - acc: 0.8221 - val_loss: 0.4903 - val_acc: 0.8068\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4322 - acc: 0.8349 - val_loss: 0.4441 - val_acc: 0.8317\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4233 - acc: 0.8397 - val_loss: 0.4533 - val_acc: 0.8332\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4166 - acc: 0.8417 - val_loss: 0.5141 - val_acc: 0.7944\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4573 - acc: 0.8229 - val_loss: 0.4947 - val_acc: 0.7991\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4367 - acc: 0.8342 - val_loss: 0.5207 - val_acc: 0.7882\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4384 - acc: 0.8325 - val_loss: 0.5369 - val_acc: 0.7882\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4221 - acc: 0.8388 - val_loss: 0.4428 - val_acc: 0.8317\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4250 - acc: 0.8365 - val_loss: 0.5058 - val_acc: 0.7991\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4287 - acc: 0.8380 - val_loss: 0.5833 - val_acc: 0.7797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUkv5RM3yDaS",
        "outputId": "5b5b772b-61cb-47f9-eaa4-d4710d2b1514"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml10', 'w').write(yaml_string)\r\n",
        "# json_string = model.to_json()\r\n",
        "# open('../docs/keras/model_architecture.json', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights10.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4578 - acc: 0.8225 - val_loss: 0.5277 - val_acc: 0.7929\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4268 - acc: 0.8417 - val_loss: 0.4429 - val_acc: 0.8340\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4168 - acc: 0.8425 - val_loss: 0.4255 - val_acc: 0.8270\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4192 - acc: 0.8411 - val_loss: 0.4783 - val_acc: 0.8029\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4095 - acc: 0.8448 - val_loss: 0.4884 - val_acc: 0.8061\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4201 - acc: 0.8402 - val_loss: 0.5888 - val_acc: 0.7704\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4268 - acc: 0.8386 - val_loss: 0.4596 - val_acc: 0.8216\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4388 - acc: 0.8400 - val_loss: 0.4873 - val_acc: 0.8123\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4476 - acc: 0.8303 - val_loss: 0.4690 - val_acc: 0.8045\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4208 - acc: 0.8442 - val_loss: 0.4814 - val_acc: 0.8138\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4355 - acc: 0.8369 - val_loss: 0.4596 - val_acc: 0.8200\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4005 - acc: 0.8510 - val_loss: 0.4944 - val_acc: 0.7983\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4325 - acc: 0.8349 - val_loss: 0.4699 - val_acc: 0.8231\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.4345 - acc: 0.8351 - val_loss: 0.4430 - val_acc: 0.8309\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4170 - acc: 0.8424 - val_loss: 0.4420 - val_acc: 0.8293\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 16s 82ms/step - loss: 0.4217 - acc: 0.8407 - val_loss: 0.6000 - val_acc: 0.7634\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4260 - acc: 0.8376 - val_loss: 0.4520 - val_acc: 0.8254\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4326 - acc: 0.8329 - val_loss: 0.4357 - val_acc: 0.8371\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4107 - acc: 0.8490 - val_loss: 0.4534 - val_acc: 0.8301\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4133 - acc: 0.8472 - val_loss: 0.4361 - val_acc: 0.8379\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4023 - acc: 0.8486 - val_loss: 0.4197 - val_acc: 0.8472\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4432 - acc: 0.8325 - val_loss: 0.4759 - val_acc: 0.8130\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4330 - acc: 0.8326 - val_loss: 0.4718 - val_acc: 0.8247\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4218 - acc: 0.8421 - val_loss: 0.4193 - val_acc: 0.8503\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4143 - acc: 0.8462 - val_loss: 0.4599 - val_acc: 0.8293\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4115 - acc: 0.8452 - val_loss: 0.4840 - val_acc: 0.8061\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4018 - acc: 0.8512 - val_loss: 0.4572 - val_acc: 0.8231\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4299 - acc: 0.8405 - val_loss: 0.5183 - val_acc: 0.7983\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4162 - acc: 0.8416 - val_loss: 0.4387 - val_acc: 0.8355\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4225 - acc: 0.8424 - val_loss: 0.4936 - val_acc: 0.8130\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4071 - acc: 0.8495 - val_loss: 0.4643 - val_acc: 0.8107\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4472 - acc: 0.8286 - val_loss: 0.5185 - val_acc: 0.7913\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4127 - acc: 0.8433 - val_loss: 0.4735 - val_acc: 0.8169\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4306 - acc: 0.8360 - val_loss: 0.4430 - val_acc: 0.8177\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4474 - acc: 0.8294 - val_loss: 0.4354 - val_acc: 0.8441\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4033 - acc: 0.8467 - val_loss: 0.5071 - val_acc: 0.8045\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 18s 87ms/step - loss: 0.4046 - acc: 0.8489 - val_loss: 0.4704 - val_acc: 0.8177\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4082 - acc: 0.8472 - val_loss: 0.5563 - val_acc: 0.7750\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4113 - acc: 0.8501 - val_loss: 0.4591 - val_acc: 0.8270\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4100 - acc: 0.8425 - val_loss: 0.4422 - val_acc: 0.8379\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3926 - acc: 0.8544 - val_loss: 0.5037 - val_acc: 0.7913\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3982 - acc: 0.8520 - val_loss: 0.4080 - val_acc: 0.8549\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4423 - acc: 0.8360 - val_loss: 0.4944 - val_acc: 0.8099\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4302 - acc: 0.8340 - val_loss: 0.5593 - val_acc: 0.7750\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4045 - acc: 0.8507 - val_loss: 0.5663 - val_acc: 0.8029\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4208 - acc: 0.8428 - val_loss: 0.4627 - val_acc: 0.8324\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4089 - acc: 0.8464 - val_loss: 0.4106 - val_acc: 0.8534\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4036 - acc: 0.8483 - val_loss: 0.4237 - val_acc: 0.8340\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3903 - acc: 0.8537 - val_loss: 0.4390 - val_acc: 0.8355\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4253 - acc: 0.8425 - val_loss: 0.4785 - val_acc: 0.8084\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4533 - acc: 0.8269 - val_loss: 0.4726 - val_acc: 0.8239\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4034 - acc: 0.8475 - val_loss: 0.4488 - val_acc: 0.8200\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4012 - acc: 0.8524 - val_loss: 0.4475 - val_acc: 0.8262\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4123 - acc: 0.8486 - val_loss: 0.4225 - val_acc: 0.8410\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4328 - acc: 0.8354 - val_loss: 0.4623 - val_acc: 0.8185\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4369 - acc: 0.8338 - val_loss: 0.4535 - val_acc: 0.8200\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3906 - acc: 0.8571 - val_loss: 0.5643 - val_acc: 0.7742\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4440 - acc: 0.8303 - val_loss: 0.4889 - val_acc: 0.8107\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4090 - acc: 0.8473 - val_loss: 0.4200 - val_acc: 0.8448\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3810 - acc: 0.8608 - val_loss: 0.5633 - val_acc: 0.7828\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4029 - acc: 0.8501 - val_loss: 0.5078 - val_acc: 0.8099\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4229 - acc: 0.8377 - val_loss: 0.4418 - val_acc: 0.8317\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3897 - acc: 0.8572 - val_loss: 0.4600 - val_acc: 0.8262\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3963 - acc: 0.8543 - val_loss: 0.4106 - val_acc: 0.8386\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4000 - acc: 0.8534 - val_loss: 0.4060 - val_acc: 0.8479\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3966 - acc: 0.8504 - val_loss: 0.4803 - val_acc: 0.8138\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3806 - acc: 0.8620 - val_loss: 0.4602 - val_acc: 0.8270\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4464 - acc: 0.8309 - val_loss: 0.4198 - val_acc: 0.8448\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4279 - acc: 0.8431 - val_loss: 0.4405 - val_acc: 0.8262\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3901 - acc: 0.8520 - val_loss: 0.4263 - val_acc: 0.8417\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3955 - acc: 0.8518 - val_loss: 0.5111 - val_acc: 0.8061\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4089 - acc: 0.8439 - val_loss: 0.4364 - val_acc: 0.8363\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4184 - acc: 0.8452 - val_loss: 0.4885 - val_acc: 0.8084\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4049 - acc: 0.8472 - val_loss: 0.4314 - val_acc: 0.8410\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4020 - acc: 0.8518 - val_loss: 0.4065 - val_acc: 0.8526\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3824 - acc: 0.8585 - val_loss: 0.3874 - val_acc: 0.8565\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3988 - acc: 0.8538 - val_loss: 0.4313 - val_acc: 0.8402\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3841 - acc: 0.8616 - val_loss: 0.4708 - val_acc: 0.8161\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3865 - acc: 0.8561 - val_loss: 0.4630 - val_acc: 0.8324\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3993 - acc: 0.8554 - val_loss: 0.4097 - val_acc: 0.8510\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3881 - acc: 0.8575 - val_loss: 0.3981 - val_acc: 0.8611\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4022 - acc: 0.8510 - val_loss: 0.4359 - val_acc: 0.8386\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4055 - acc: 0.8462 - val_loss: 0.4903 - val_acc: 0.8037\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4057 - acc: 0.8470 - val_loss: 0.4239 - val_acc: 0.8386\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3863 - acc: 0.8579 - val_loss: 0.4566 - val_acc: 0.8270\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4154 - acc: 0.8428 - val_loss: 0.4033 - val_acc: 0.8495\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3927 - acc: 0.8561 - val_loss: 0.4879 - val_acc: 0.8123\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3907 - acc: 0.8506 - val_loss: 0.4137 - val_acc: 0.8526\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3875 - acc: 0.8565 - val_loss: 0.4403 - val_acc: 0.8448\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3729 - acc: 0.8656 - val_loss: 0.4093 - val_acc: 0.8503\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3874 - acc: 0.8600 - val_loss: 0.4069 - val_acc: 0.8542\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3921 - acc: 0.8565 - val_loss: 0.4392 - val_acc: 0.8301\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3864 - acc: 0.8579 - val_loss: 0.4680 - val_acc: 0.8192\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4089 - acc: 0.8465 - val_loss: 0.4460 - val_acc: 0.8317\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3970 - acc: 0.8518 - val_loss: 0.4189 - val_acc: 0.8456\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3879 - acc: 0.8574 - val_loss: 0.4555 - val_acc: 0.8216\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3980 - acc: 0.8523 - val_loss: 0.4803 - val_acc: 0.8169\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4034 - acc: 0.8538 - val_loss: 0.3996 - val_acc: 0.8526\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3636 - acc: 0.8668 - val_loss: 0.4085 - val_acc: 0.8518\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3893 - acc: 0.8569 - val_loss: 0.4023 - val_acc: 0.8518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNYVumyuyDpq",
        "outputId": "5926ecf3-f637-4a90-8e94-4ac0f88c56c6"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml11', 'w').write(yaml_string)\r\n",
        "json_string = model.to_json()\r\n",
        "open('model_architecture.json11', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights11.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=100,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3977 - acc: 0.8498 - val_loss: 0.3857 - val_acc: 0.8611\n",
            "Epoch 2/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3776 - acc: 0.8602 - val_loss: 0.4355 - val_acc: 0.8348\n",
            "Epoch 3/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3827 - acc: 0.8599 - val_loss: 0.4311 - val_acc: 0.8324\n",
            "Epoch 4/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4186 - acc: 0.8414 - val_loss: 0.4307 - val_acc: 0.8394\n",
            "Epoch 5/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4016 - acc: 0.8521 - val_loss: 0.4664 - val_acc: 0.8146\n",
            "Epoch 6/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3783 - acc: 0.8597 - val_loss: 0.5692 - val_acc: 0.7944\n",
            "Epoch 7/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3975 - acc: 0.8566 - val_loss: 0.4001 - val_acc: 0.8627\n",
            "Epoch 8/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3808 - acc: 0.8585 - val_loss: 0.4149 - val_acc: 0.8472\n",
            "Epoch 9/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4012 - acc: 0.8496 - val_loss: 0.3911 - val_acc: 0.8611\n",
            "Epoch 10/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4233 - acc: 0.8402 - val_loss: 0.4596 - val_acc: 0.8355\n",
            "Epoch 11/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3757 - acc: 0.8636 - val_loss: 0.3977 - val_acc: 0.8596\n",
            "Epoch 12/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3779 - acc: 0.8603 - val_loss: 0.4461 - val_acc: 0.8301\n",
            "Epoch 13/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3854 - acc: 0.8588 - val_loss: 0.3900 - val_acc: 0.8573\n",
            "Epoch 14/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3797 - acc: 0.8627 - val_loss: 0.4208 - val_acc: 0.8433\n",
            "Epoch 15/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3748 - acc: 0.8637 - val_loss: 0.4561 - val_acc: 0.8270\n",
            "Epoch 16/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3966 - acc: 0.8503 - val_loss: 0.3941 - val_acc: 0.8673\n",
            "Epoch 17/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3959 - acc: 0.8555 - val_loss: 0.5018 - val_acc: 0.8053\n",
            "Epoch 18/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3920 - acc: 0.8568 - val_loss: 0.3848 - val_acc: 0.8666\n",
            "Epoch 19/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3594 - acc: 0.8712 - val_loss: 0.3970 - val_acc: 0.8604\n",
            "Epoch 20/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3914 - acc: 0.8561 - val_loss: 0.3837 - val_acc: 0.8619\n",
            "Epoch 21/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3783 - acc: 0.8617 - val_loss: 0.5162 - val_acc: 0.7998\n",
            "Epoch 22/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3707 - acc: 0.8656 - val_loss: 0.4048 - val_acc: 0.8472\n",
            "Epoch 23/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3759 - acc: 0.8639 - val_loss: 0.4330 - val_acc: 0.8371\n",
            "Epoch 24/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3858 - acc: 0.8609 - val_loss: 0.4277 - val_acc: 0.8363\n",
            "Epoch 25/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3816 - acc: 0.8591 - val_loss: 0.3675 - val_acc: 0.8798\n",
            "Epoch 26/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3618 - acc: 0.8710 - val_loss: 0.4031 - val_acc: 0.8448\n",
            "Epoch 27/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3679 - acc: 0.8650 - val_loss: 0.4093 - val_acc: 0.8518\n",
            "Epoch 28/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3707 - acc: 0.8634 - val_loss: 0.4191 - val_acc: 0.8278\n",
            "Epoch 29/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3823 - acc: 0.8596 - val_loss: 0.3723 - val_acc: 0.8743\n",
            "Epoch 30/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3699 - acc: 0.8654 - val_loss: 0.4181 - val_acc: 0.8417\n",
            "Epoch 31/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3739 - acc: 0.8631 - val_loss: 0.3777 - val_acc: 0.8658\n",
            "Epoch 32/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3700 - acc: 0.8684 - val_loss: 0.4325 - val_acc: 0.8464\n",
            "Epoch 33/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3699 - acc: 0.8651 - val_loss: 0.4219 - val_acc: 0.8417\n",
            "Epoch 34/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3561 - acc: 0.8738 - val_loss: 0.3839 - val_acc: 0.8658\n",
            "Epoch 35/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3799 - acc: 0.8623 - val_loss: 0.3947 - val_acc: 0.8557\n",
            "Epoch 36/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3767 - acc: 0.8575 - val_loss: 0.3903 - val_acc: 0.8542\n",
            "Epoch 37/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3692 - acc: 0.8653 - val_loss: 0.4586 - val_acc: 0.8247\n",
            "Epoch 38/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3924 - acc: 0.8555 - val_loss: 0.5720 - val_acc: 0.7967\n",
            "Epoch 39/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4143 - acc: 0.8447 - val_loss: 0.4119 - val_acc: 0.8479\n",
            "Epoch 40/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3718 - acc: 0.8668 - val_loss: 0.3690 - val_acc: 0.8728\n",
            "Epoch 41/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3612 - acc: 0.8713 - val_loss: 0.4117 - val_acc: 0.8619\n",
            "Epoch 42/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3707 - acc: 0.8654 - val_loss: 0.3842 - val_acc: 0.8627\n",
            "Epoch 43/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3805 - acc: 0.8602 - val_loss: 0.4111 - val_acc: 0.8456\n",
            "Epoch 44/100\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3918 - acc: 0.8552 - val_loss: 0.5113 - val_acc: 0.8014\n",
            "Epoch 45/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3983 - acc: 0.8524 - val_loss: 0.3904 - val_acc: 0.8697\n",
            "Epoch 46/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3620 - acc: 0.8688 - val_loss: 0.3855 - val_acc: 0.8580\n",
            "Epoch 47/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3960 - acc: 0.8512 - val_loss: 0.3846 - val_acc: 0.8635\n",
            "Epoch 48/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3694 - acc: 0.8650 - val_loss: 0.4654 - val_acc: 0.8177\n",
            "Epoch 49/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3511 - acc: 0.8749 - val_loss: 0.3646 - val_acc: 0.8704\n",
            "Epoch 50/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3693 - acc: 0.8633 - val_loss: 0.4661 - val_acc: 0.8254\n",
            "Epoch 51/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3919 - acc: 0.8535 - val_loss: 0.3900 - val_acc: 0.8681\n",
            "Epoch 52/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3543 - acc: 0.8733 - val_loss: 0.3994 - val_acc: 0.8526\n",
            "Epoch 53/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3816 - acc: 0.8596 - val_loss: 0.4134 - val_acc: 0.8487\n",
            "Epoch 54/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3977 - acc: 0.8571 - val_loss: 0.5177 - val_acc: 0.8169\n",
            "Epoch 55/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3821 - acc: 0.8596 - val_loss: 0.4065 - val_acc: 0.8619\n",
            "Epoch 56/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3932 - acc: 0.8534 - val_loss: 0.4194 - val_acc: 0.8487\n",
            "Epoch 57/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3609 - acc: 0.8668 - val_loss: 0.4316 - val_acc: 0.8340\n",
            "Epoch 58/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3460 - acc: 0.8775 - val_loss: 0.3852 - val_acc: 0.8635\n",
            "Epoch 59/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3550 - acc: 0.8704 - val_loss: 0.3794 - val_acc: 0.8619\n",
            "Epoch 60/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3677 - acc: 0.8671 - val_loss: 0.4004 - val_acc: 0.8503\n",
            "Epoch 61/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3759 - acc: 0.8640 - val_loss: 0.4529 - val_acc: 0.8355\n",
            "Epoch 62/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3954 - acc: 0.8569 - val_loss: 0.4449 - val_acc: 0.8363\n",
            "Epoch 63/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3813 - acc: 0.8631 - val_loss: 0.4868 - val_acc: 0.8138\n",
            "Epoch 64/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3793 - acc: 0.8614 - val_loss: 0.3826 - val_acc: 0.8580\n",
            "Epoch 65/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3802 - acc: 0.8616 - val_loss: 0.4362 - val_acc: 0.8301\n",
            "Epoch 66/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3675 - acc: 0.8653 - val_loss: 0.4707 - val_acc: 0.8231\n",
            "Epoch 67/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3453 - acc: 0.8771 - val_loss: 0.3730 - val_acc: 0.8635\n",
            "Epoch 68/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3637 - acc: 0.8712 - val_loss: 0.5890 - val_acc: 0.7913\n",
            "Epoch 69/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3684 - acc: 0.8682 - val_loss: 0.3520 - val_acc: 0.8821\n",
            "Epoch 70/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3907 - acc: 0.8580 - val_loss: 0.4315 - val_acc: 0.8456\n",
            "Epoch 71/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.4736 - acc: 0.8261 - val_loss: 0.4455 - val_acc: 0.8317\n",
            "Epoch 72/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4348 - acc: 0.8430 - val_loss: 0.4296 - val_acc: 0.8386\n",
            "Epoch 73/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3526 - acc: 0.8750 - val_loss: 0.6157 - val_acc: 0.7711\n",
            "Epoch 74/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3808 - acc: 0.8591 - val_loss: 0.4288 - val_acc: 0.8371\n",
            "Epoch 75/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3633 - acc: 0.8675 - val_loss: 0.4641 - val_acc: 0.8161\n",
            "Epoch 76/100\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3532 - acc: 0.8750 - val_loss: 0.4309 - val_acc: 0.8379\n",
            "Epoch 77/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3512 - acc: 0.8753 - val_loss: 0.4140 - val_acc: 0.8425\n",
            "Epoch 78/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3779 - acc: 0.8622 - val_loss: 0.3763 - val_acc: 0.8720\n",
            "Epoch 79/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3514 - acc: 0.8750 - val_loss: 0.4161 - val_acc: 0.8402\n",
            "Epoch 80/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3363 - acc: 0.8836 - val_loss: 0.4411 - val_acc: 0.8363\n",
            "Epoch 81/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3633 - acc: 0.8676 - val_loss: 0.4237 - val_acc: 0.8441\n",
            "Epoch 82/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3680 - acc: 0.8653 - val_loss: 0.4243 - val_acc: 0.8456\n",
            "Epoch 83/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3672 - acc: 0.8702 - val_loss: 0.4432 - val_acc: 0.8348\n",
            "Epoch 84/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4056 - acc: 0.8518 - val_loss: 0.4412 - val_acc: 0.8301\n",
            "Epoch 85/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3944 - acc: 0.8554 - val_loss: 0.3948 - val_acc: 0.8526\n",
            "Epoch 86/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3789 - acc: 0.8623 - val_loss: 0.5416 - val_acc: 0.8076\n",
            "Epoch 87/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3722 - acc: 0.8636 - val_loss: 0.3798 - val_acc: 0.8681\n",
            "Epoch 88/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3551 - acc: 0.8716 - val_loss: 0.3965 - val_acc: 0.8534\n",
            "Epoch 89/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3565 - acc: 0.8712 - val_loss: 0.3798 - val_acc: 0.8596\n",
            "Epoch 90/100\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.4008 - acc: 0.8529 - val_loss: 0.3871 - val_acc: 0.8611\n",
            "Epoch 91/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3616 - acc: 0.8690 - val_loss: 0.4232 - val_acc: 0.8371\n",
            "Epoch 92/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3789 - acc: 0.8636 - val_loss: 0.3675 - val_acc: 0.8635\n",
            "Epoch 93/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3594 - acc: 0.8716 - val_loss: 0.3973 - val_acc: 0.8611\n",
            "Epoch 94/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3423 - acc: 0.8783 - val_loss: 0.3485 - val_acc: 0.8813\n",
            "Epoch 95/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3486 - acc: 0.8741 - val_loss: 0.3738 - val_acc: 0.8704\n",
            "Epoch 96/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4020 - acc: 0.8531 - val_loss: 0.3883 - val_acc: 0.8565\n",
            "Epoch 97/100\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3589 - acc: 0.8718 - val_loss: 0.3988 - val_acc: 0.8635\n",
            "Epoch 98/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3487 - acc: 0.8749 - val_loss: 0.3622 - val_acc: 0.8798\n",
            "Epoch 99/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3613 - acc: 0.8715 - val_loss: 0.3632 - val_acc: 0.8681\n",
            "Epoch 100/100\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4224 - acc: 0.8448 - val_loss: 0.4606 - val_acc: 0.8169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsPqVNbMyEFb",
        "outputId": "2c1b8c8b-7ce7-4a3d-e7bc-79f95b69083f"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml12', 'w').write(yaml_string)\r\n",
        "json_string = model.to_json()\r\n",
        "open('model_architecture.json12', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights12.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3509 - acc: 0.8758 - val_loss: 0.3454 - val_acc: 0.8821\n",
            "Epoch 2/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3401 - acc: 0.8822 - val_loss: 0.4046 - val_acc: 0.8573\n",
            "Epoch 3/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3413 - acc: 0.8797 - val_loss: 0.3641 - val_acc: 0.8697\n",
            "Epoch 4/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3489 - acc: 0.8744 - val_loss: 0.4938 - val_acc: 0.8192\n",
            "Epoch 5/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3794 - acc: 0.8622 - val_loss: 0.4090 - val_acc: 0.8542\n",
            "Epoch 6/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3616 - acc: 0.8675 - val_loss: 0.3934 - val_acc: 0.8534\n",
            "Epoch 7/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3994 - acc: 0.8523 - val_loss: 0.3995 - val_acc: 0.8549\n",
            "Epoch 8/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3579 - acc: 0.8716 - val_loss: 0.4070 - val_acc: 0.8433\n",
            "Epoch 9/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3406 - acc: 0.8806 - val_loss: 0.3985 - val_acc: 0.8565\n",
            "Epoch 10/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3488 - acc: 0.8758 - val_loss: 0.3944 - val_acc: 0.8596\n",
            "Epoch 11/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3804 - acc: 0.8605 - val_loss: 0.4500 - val_acc: 0.8239\n",
            "Epoch 12/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3622 - acc: 0.8704 - val_loss: 0.4321 - val_acc: 0.8332\n",
            "Epoch 13/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3471 - acc: 0.8769 - val_loss: 0.3755 - val_acc: 0.8580\n",
            "Epoch 14/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3353 - acc: 0.8819 - val_loss: 0.4036 - val_acc: 0.8464\n",
            "Epoch 15/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3673 - acc: 0.8667 - val_loss: 0.3827 - val_acc: 0.8627\n",
            "Epoch 16/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3430 - acc: 0.8760 - val_loss: 0.4356 - val_acc: 0.8332\n",
            "Epoch 17/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4500 - acc: 0.8386 - val_loss: 0.4508 - val_acc: 0.8301\n",
            "Epoch 18/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3624 - acc: 0.8679 - val_loss: 0.4973 - val_acc: 0.8270\n",
            "Epoch 19/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3470 - acc: 0.8764 - val_loss: 0.3673 - val_acc: 0.8658\n",
            "Epoch 20/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3476 - acc: 0.8730 - val_loss: 0.3655 - val_acc: 0.8712\n",
            "Epoch 21/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3658 - acc: 0.8704 - val_loss: 0.4640 - val_acc: 0.8185\n",
            "Epoch 22/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3634 - acc: 0.8696 - val_loss: 0.4276 - val_acc: 0.8487\n",
            "Epoch 23/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3518 - acc: 0.8757 - val_loss: 0.4126 - val_acc: 0.8433\n",
            "Epoch 24/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3531 - acc: 0.8736 - val_loss: 0.4024 - val_acc: 0.8542\n",
            "Epoch 25/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3353 - acc: 0.8814 - val_loss: 0.3829 - val_acc: 0.8642\n",
            "Epoch 26/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3595 - acc: 0.8746 - val_loss: 0.3500 - val_acc: 0.8782\n",
            "Epoch 27/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3943 - acc: 0.8616 - val_loss: 0.6500 - val_acc: 0.7696\n",
            "Epoch 28/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4377 - acc: 0.8464 - val_loss: 0.3802 - val_acc: 0.8635\n",
            "Epoch 29/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3464 - acc: 0.8781 - val_loss: 0.4483 - val_acc: 0.8340\n",
            "Epoch 30/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3469 - acc: 0.8743 - val_loss: 0.3612 - val_acc: 0.8751\n",
            "Epoch 31/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3427 - acc: 0.8761 - val_loss: 0.3953 - val_acc: 0.8611\n",
            "Epoch 32/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3370 - acc: 0.8794 - val_loss: 0.3633 - val_acc: 0.8689\n",
            "Epoch 33/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3248 - acc: 0.8896 - val_loss: 0.3487 - val_acc: 0.8914\n",
            "Epoch 34/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3623 - acc: 0.8729 - val_loss: 0.3755 - val_acc: 0.8681\n",
            "Epoch 35/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3718 - acc: 0.8664 - val_loss: 0.4907 - val_acc: 0.8092\n",
            "Epoch 36/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3658 - acc: 0.8662 - val_loss: 0.3904 - val_acc: 0.8704\n",
            "Epoch 37/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3243 - acc: 0.8884 - val_loss: 0.3340 - val_acc: 0.8891\n",
            "Epoch 38/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3283 - acc: 0.8843 - val_loss: 0.3896 - val_acc: 0.8580\n",
            "Epoch 39/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3660 - acc: 0.8729 - val_loss: 0.5217 - val_acc: 0.8045\n",
            "Epoch 40/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3499 - acc: 0.8746 - val_loss: 0.3771 - val_acc: 0.8697\n",
            "Epoch 41/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3515 - acc: 0.8741 - val_loss: 0.3639 - val_acc: 0.8774\n",
            "Epoch 42/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3329 - acc: 0.8842 - val_loss: 0.3734 - val_acc: 0.8697\n",
            "Epoch 43/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3602 - acc: 0.8705 - val_loss: 0.4074 - val_acc: 0.8549\n",
            "Epoch 44/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3555 - acc: 0.8732 - val_loss: 0.3630 - val_acc: 0.8689\n",
            "Epoch 45/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3715 - acc: 0.8699 - val_loss: 0.3963 - val_acc: 0.8534\n",
            "Epoch 46/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3344 - acc: 0.8811 - val_loss: 0.3711 - val_acc: 0.8666\n",
            "Epoch 47/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3512 - acc: 0.8733 - val_loss: 0.4432 - val_acc: 0.8472\n",
            "Epoch 48/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3442 - acc: 0.8791 - val_loss: 0.3341 - val_acc: 0.8906\n",
            "Epoch 49/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3324 - acc: 0.8808 - val_loss: 0.4181 - val_acc: 0.8565\n",
            "Epoch 50/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3642 - acc: 0.8676 - val_loss: 0.3720 - val_acc: 0.8704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoSI7KkRyEVt",
        "outputId": "3341ab02-505b-4150-cff8-1e197f2a7e4a"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml13', 'w').write(yaml_string)\r\n",
        "json_string = model.to_json()\r\n",
        "open('model_architecture.json13', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights13.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3711 - acc: 0.8654 - val_loss: 0.3715 - val_acc: 0.8728\n",
            "Epoch 2/50\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3541 - acc: 0.8715 - val_loss: 0.3551 - val_acc: 0.8774\n",
            "Epoch 3/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3506 - acc: 0.8766 - val_loss: 0.3544 - val_acc: 0.8774\n",
            "Epoch 4/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3455 - acc: 0.8780 - val_loss: 0.3933 - val_acc: 0.8565\n",
            "Epoch 5/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3491 - acc: 0.8755 - val_loss: 0.4171 - val_acc: 0.8441\n",
            "Epoch 6/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3621 - acc: 0.8692 - val_loss: 0.4004 - val_acc: 0.8472\n",
            "Epoch 7/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3516 - acc: 0.8724 - val_loss: 0.3747 - val_acc: 0.8728\n",
            "Epoch 8/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3430 - acc: 0.8798 - val_loss: 0.3731 - val_acc: 0.8673\n",
            "Epoch 9/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3536 - acc: 0.8730 - val_loss: 0.4463 - val_acc: 0.8464\n",
            "Epoch 10/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3605 - acc: 0.8733 - val_loss: 0.4105 - val_acc: 0.8580\n",
            "Epoch 11/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3566 - acc: 0.8701 - val_loss: 0.3832 - val_acc: 0.8588\n",
            "Epoch 12/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3422 - acc: 0.8791 - val_loss: 0.3546 - val_acc: 0.8813\n",
            "Epoch 13/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3445 - acc: 0.8740 - val_loss: 0.3518 - val_acc: 0.8759\n",
            "Epoch 14/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3351 - acc: 0.8857 - val_loss: 0.3647 - val_acc: 0.8666\n",
            "Epoch 15/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3336 - acc: 0.8832 - val_loss: 0.3371 - val_acc: 0.8867\n",
            "Epoch 16/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3455 - acc: 0.8783 - val_loss: 0.3362 - val_acc: 0.8883\n",
            "Epoch 17/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3560 - acc: 0.8732 - val_loss: 0.3626 - val_acc: 0.8774\n",
            "Epoch 18/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3332 - acc: 0.8836 - val_loss: 0.3571 - val_acc: 0.8728\n",
            "Epoch 19/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3251 - acc: 0.8846 - val_loss: 0.3448 - val_acc: 0.8774\n",
            "Epoch 20/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3872 - acc: 0.8620 - val_loss: 0.3592 - val_acc: 0.8790\n",
            "Epoch 21/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3896 - acc: 0.8611 - val_loss: 0.4114 - val_acc: 0.8472\n",
            "Epoch 22/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3427 - acc: 0.8783 - val_loss: 0.3529 - val_acc: 0.8759\n",
            "Epoch 23/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3294 - acc: 0.8842 - val_loss: 0.3755 - val_acc: 0.8673\n",
            "Epoch 24/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3413 - acc: 0.8769 - val_loss: 0.3411 - val_acc: 0.8860\n",
            "Epoch 25/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3179 - acc: 0.8901 - val_loss: 0.3620 - val_acc: 0.8798\n",
            "Epoch 26/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3059 - acc: 0.8939 - val_loss: 0.3460 - val_acc: 0.8829\n",
            "Epoch 27/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3536 - acc: 0.8726 - val_loss: 0.5119 - val_acc: 0.8146\n",
            "Epoch 28/50\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3597 - acc: 0.8715 - val_loss: 0.4757 - val_acc: 0.8216\n",
            "Epoch 29/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3521 - acc: 0.8744 - val_loss: 0.4081 - val_acc: 0.8495\n",
            "Epoch 30/50\n",
            "202/202 [==============================] - 16s 82ms/step - loss: 0.3573 - acc: 0.8699 - val_loss: 0.3898 - val_acc: 0.8681\n",
            "Epoch 31/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3483 - acc: 0.8761 - val_loss: 0.3703 - val_acc: 0.8751\n",
            "Epoch 32/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3233 - acc: 0.8839 - val_loss: 0.3467 - val_acc: 0.8798\n",
            "Epoch 33/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3179 - acc: 0.8910 - val_loss: 0.4020 - val_acc: 0.8456\n",
            "Epoch 34/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3359 - acc: 0.8823 - val_loss: 0.5025 - val_acc: 0.8161\n",
            "Epoch 35/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3863 - acc: 0.8619 - val_loss: 0.3924 - val_acc: 0.8611\n",
            "Epoch 36/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3889 - acc: 0.8585 - val_loss: 0.3779 - val_acc: 0.8712\n",
            "Epoch 37/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3362 - acc: 0.8832 - val_loss: 0.3565 - val_acc: 0.8759\n",
            "Epoch 38/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3612 - acc: 0.8713 - val_loss: 0.4125 - val_acc: 0.8479\n",
            "Epoch 39/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3308 - acc: 0.8837 - val_loss: 0.4575 - val_acc: 0.8410\n",
            "Epoch 40/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3242 - acc: 0.8851 - val_loss: 0.3544 - val_acc: 0.8829\n",
            "Epoch 41/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3219 - acc: 0.8879 - val_loss: 0.3460 - val_acc: 0.8829\n",
            "Epoch 42/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3531 - acc: 0.8777 - val_loss: 0.3493 - val_acc: 0.8813\n",
            "Epoch 43/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3262 - acc: 0.8848 - val_loss: 0.3416 - val_acc: 0.8829\n",
            "Epoch 44/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3274 - acc: 0.8822 - val_loss: 0.4163 - val_acc: 0.8379\n",
            "Epoch 45/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3640 - acc: 0.8784 - val_loss: 0.5624 - val_acc: 0.8130\n",
            "Epoch 46/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4435 - acc: 0.8455 - val_loss: 0.3395 - val_acc: 0.8844\n",
            "Epoch 47/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3249 - acc: 0.8860 - val_loss: 0.3472 - val_acc: 0.8821\n",
            "Epoch 48/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3481 - acc: 0.8740 - val_loss: 0.3692 - val_acc: 0.8619\n",
            "Epoch 49/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3507 - acc: 0.8752 - val_loss: 0.3816 - val_acc: 0.8580\n",
            "Epoch 50/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3192 - acc: 0.8880 - val_loss: 0.3624 - val_acc: 0.8666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WTXacmMUnMQ",
        "outputId": "8022470c-3f30-4148-8d99-c36c903dc4e2"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml14', 'w').write(yaml_string)\r\n",
        "json_string = model.to_json()\r\n",
        "open('model_architecture.json14', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights14.h5')\r\n",
        "\r\n",
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=50,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n",
            "Epoch 1/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3207 - acc: 0.8893 - val_loss: 0.3394 - val_acc: 0.8766\n",
            "Epoch 2/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3434 - acc: 0.8772 - val_loss: 0.3704 - val_acc: 0.8658\n",
            "Epoch 3/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3215 - acc: 0.8899 - val_loss: 0.3636 - val_acc: 0.8658\n",
            "Epoch 4/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3621 - acc: 0.8721 - val_loss: 0.3685 - val_acc: 0.8689\n",
            "Epoch 5/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3661 - acc: 0.8712 - val_loss: 0.4355 - val_acc: 0.8324\n",
            "Epoch 6/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3623 - acc: 0.8692 - val_loss: 0.3495 - val_acc: 0.8805\n",
            "Epoch 7/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3363 - acc: 0.8836 - val_loss: 0.3642 - val_acc: 0.8728\n",
            "Epoch 8/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3379 - acc: 0.8800 - val_loss: 0.4087 - val_acc: 0.8510\n",
            "Epoch 9/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3686 - acc: 0.8693 - val_loss: 0.3820 - val_acc: 0.8549\n",
            "Epoch 10/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3463 - acc: 0.8761 - val_loss: 0.4562 - val_acc: 0.8441\n",
            "Epoch 11/50\n",
            "202/202 [==============================] - 16s 82ms/step - loss: 0.3521 - acc: 0.8758 - val_loss: 0.3473 - val_acc: 0.8844\n",
            "Epoch 12/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3619 - acc: 0.8692 - val_loss: 0.3685 - val_acc: 0.8681\n",
            "Epoch 13/50\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3288 - acc: 0.8846 - val_loss: 0.3416 - val_acc: 0.8891\n",
            "Epoch 14/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3089 - acc: 0.8933 - val_loss: 0.3460 - val_acc: 0.8836\n",
            "Epoch 15/50\n",
            "202/202 [==============================] - 16s 81ms/step - loss: 0.3285 - acc: 0.8840 - val_loss: 0.3524 - val_acc: 0.8813\n",
            "Epoch 16/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3334 - acc: 0.8825 - val_loss: 0.3973 - val_acc: 0.8697\n",
            "Epoch 17/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3259 - acc: 0.8871 - val_loss: 0.4196 - val_acc: 0.8472\n",
            "Epoch 18/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3054 - acc: 0.8956 - val_loss: 0.4030 - val_acc: 0.8573\n",
            "Epoch 19/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3516 - acc: 0.8771 - val_loss: 0.4836 - val_acc: 0.8208\n",
            "Epoch 20/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3480 - acc: 0.8749 - val_loss: 0.4423 - val_acc: 0.8402\n",
            "Epoch 21/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3498 - acc: 0.8730 - val_loss: 0.3273 - val_acc: 0.8867\n",
            "Epoch 22/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3180 - acc: 0.8930 - val_loss: 0.3296 - val_acc: 0.8960\n",
            "Epoch 23/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3218 - acc: 0.8868 - val_loss: 0.3688 - val_acc: 0.8728\n",
            "Epoch 24/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3366 - acc: 0.8801 - val_loss: 0.3498 - val_acc: 0.8689\n",
            "Epoch 25/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3627 - acc: 0.8693 - val_loss: 0.3643 - val_acc: 0.8782\n",
            "Epoch 26/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3439 - acc: 0.8761 - val_loss: 0.3458 - val_acc: 0.8844\n",
            "Epoch 27/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3165 - acc: 0.8922 - val_loss: 0.3210 - val_acc: 0.8976\n",
            "Epoch 28/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3418 - acc: 0.8767 - val_loss: 0.4229 - val_acc: 0.8487\n",
            "Epoch 29/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3488 - acc: 0.8744 - val_loss: 0.3350 - val_acc: 0.8852\n",
            "Epoch 30/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3442 - acc: 0.8795 - val_loss: 0.3175 - val_acc: 0.9007\n",
            "Epoch 31/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3139 - acc: 0.8918 - val_loss: 0.3798 - val_acc: 0.8588\n",
            "Epoch 32/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3571 - acc: 0.8710 - val_loss: 0.3763 - val_acc: 0.8735\n",
            "Epoch 33/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3304 - acc: 0.8837 - val_loss: 0.3527 - val_acc: 0.8774\n",
            "Epoch 34/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3494 - acc: 0.8749 - val_loss: 0.3736 - val_acc: 0.8650\n",
            "Epoch 35/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3166 - acc: 0.8888 - val_loss: 0.3254 - val_acc: 0.8976\n",
            "Epoch 36/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3453 - acc: 0.8784 - val_loss: 0.4042 - val_acc: 0.8611\n",
            "Epoch 37/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3530 - acc: 0.8767 - val_loss: 0.3657 - val_acc: 0.8704\n",
            "Epoch 38/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3369 - acc: 0.8817 - val_loss: 0.3269 - val_acc: 0.8914\n",
            "Epoch 39/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3253 - acc: 0.8819 - val_loss: 0.3234 - val_acc: 0.8991\n",
            "Epoch 40/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3443 - acc: 0.8784 - val_loss: 0.4870 - val_acc: 0.8239\n",
            "Epoch 41/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4466 - acc: 0.8391 - val_loss: 0.4778 - val_acc: 0.8309\n",
            "Epoch 42/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3285 - acc: 0.8867 - val_loss: 0.3851 - val_acc: 0.8580\n",
            "Epoch 43/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3125 - acc: 0.8916 - val_loss: 0.3275 - val_acc: 0.8929\n",
            "Epoch 44/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3084 - acc: 0.8935 - val_loss: 0.3266 - val_acc: 0.8906\n",
            "Epoch 45/50\n",
            "202/202 [==============================] - 17s 85ms/step - loss: 0.3275 - acc: 0.8839 - val_loss: 0.4071 - val_acc: 0.8464\n",
            "Epoch 46/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3626 - acc: 0.8730 - val_loss: 0.4011 - val_acc: 0.8689\n",
            "Epoch 47/50\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3384 - acc: 0.8829 - val_loss: 0.3603 - val_acc: 0.8689\n",
            "Epoch 48/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3067 - acc: 0.8938 - val_loss: 0.3506 - val_acc: 0.8813\n",
            "Epoch 49/50\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3247 - acc: 0.8856 - val_loss: 0.3411 - val_acc: 0.8875\n",
            "Epoch 50/50\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3035 - acc: 0.8966 - val_loss: 0.3413 - val_acc: 0.8774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IORbOHjRUng4",
        "outputId": "bc14e002-7632-4e79-b2e2-7e108c24bfc7"
      },
      "source": [
        "hist = model.fit(\r\n",
        "    train_generator, \r\n",
        "    epochs=20,\r\n",
        "    verbose=1,\r\n",
        "    steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=val_steps_per_epoch)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3259 - acc: 0.8868 - val_loss: 0.3676 - val_acc: 0.8611\n",
            "Epoch 2/20\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3237 - acc: 0.8829 - val_loss: 0.3350 - val_acc: 0.8798\n",
            "Epoch 3/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3706 - acc: 0.8662 - val_loss: 0.3162 - val_acc: 0.8984\n",
            "Epoch 4/20\n",
            "202/202 [==============================] - 17s 82ms/step - loss: 0.3171 - acc: 0.8896 - val_loss: 0.3435 - val_acc: 0.8798\n",
            "Epoch 5/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3237 - acc: 0.8856 - val_loss: 0.5084 - val_acc: 0.8355\n",
            "Epoch 6/20\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.3420 - acc: 0.8772 - val_loss: 0.3503 - val_acc: 0.8860\n",
            "Epoch 7/20\n",
            "202/202 [==============================] - 17s 86ms/step - loss: 0.3110 - acc: 0.8919 - val_loss: 0.4205 - val_acc: 0.8425\n",
            "Epoch 8/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3362 - acc: 0.8811 - val_loss: 0.3986 - val_acc: 0.8542\n",
            "Epoch 9/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3802 - acc: 0.8648 - val_loss: 0.4357 - val_acc: 0.8371\n",
            "Epoch 10/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3689 - acc: 0.8654 - val_loss: 0.3996 - val_acc: 0.8728\n",
            "Epoch 11/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4263 - acc: 0.8577 - val_loss: 0.4858 - val_acc: 0.8309\n",
            "Epoch 12/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3965 - acc: 0.8657 - val_loss: 0.3873 - val_acc: 0.8666\n",
            "Epoch 13/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3167 - acc: 0.8911 - val_loss: 0.3170 - val_acc: 0.8960\n",
            "Epoch 14/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3220 - acc: 0.8871 - val_loss: 0.3873 - val_acc: 0.8650\n",
            "Epoch 15/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3735 - acc: 0.8707 - val_loss: 0.3965 - val_acc: 0.8580\n",
            "Epoch 16/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.4101 - acc: 0.8569 - val_loss: 0.5080 - val_acc: 0.8386\n",
            "Epoch 17/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.4253 - acc: 0.8552 - val_loss: 0.4427 - val_acc: 0.8557\n",
            "Epoch 18/20\n",
            "202/202 [==============================] - 17s 84ms/step - loss: 0.3556 - acc: 0.8741 - val_loss: 0.3506 - val_acc: 0.8743\n",
            "Epoch 19/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3126 - acc: 0.8941 - val_loss: 0.3411 - val_acc: 0.8852\n",
            "Epoch 20/20\n",
            "202/202 [==============================] - 17s 83ms/step - loss: 0.3142 - acc: 0.8928 - val_loss: 0.3596 - val_acc: 0.8712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NswrSCbYUnsO",
        "outputId": "c035d8ad-0a64-441a-b496-5e1564cc29e9"
      },
      "source": [
        "%cd /content/gdrive/MyDrive/colab/saved_models\r\n",
        "# 保存模型结构到yaml文件或者json文件\r\n",
        "yaml_string = model.to_yaml()\r\n",
        "open('model_architecture.yaml15', 'w').write(yaml_string)\r\n",
        "json_string = model.to_json()\r\n",
        "open('model_architecture.json15', 'w').write(json_string)\r\n",
        "\r\n",
        "# 保存模型参数到h5文件\r\n",
        "model.save_weights('model_weights15.h5')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/saved_models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMrDubnCyEq5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjQ6-KWKDB_x"
      },
      "source": [
        "import math\r\n",
        "number_of_examples = len(valid_generator.filenames)\r\n",
        "number_of_generator_calls = math.ceil(number_of_examples / (1.0 * 32)) \r\n",
        "# 1.0 above is to skip integer division\r\n",
        "\r\n",
        "test_labels = []\r\n",
        "\r\n",
        "for i in range(0,int(number_of_generator_calls)):\r\n",
        "    #train_labels.extend(np.array(train_generator[i][1]))\r\n",
        "    _,l=next(iter(valid_generator))\r\n",
        "    label = np.argmax(l, axis=-1)\r\n",
        "    list_l=label.tolist()\r\n",
        "    test_labels.extend(label)\r\n",
        "\r\n",
        "a=valid_generator.filenames\r\n",
        "shape = len(a)\r\n",
        "name = [a[i][a[i].rindex('/')+1:len(a[i])] for i in range(shape)]\r\n",
        "#strr[strr.rindex( '\\\\' ) + 1 : len(strr)] "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AKR2rFnJQMw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ScQtalZLHH7",
        "outputId": "80e22c18-b948-4b8a-b1ab-651a4699a363"
      },
      "source": [
        "IMAGE_SHAPE = (224, 224)\r\n",
        "TRAINING_DATA_DIR = str(data_root)\r\n",
        "print(TRAINING_DATA_DIR);\r\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\r\n",
        "#datagen_kwargs_train = dict(rescale=1./255)\r\n",
        "valid_datagen00 = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\r\n",
        "valid_generator00 = valid_datagen.flow_from_directory(\r\n",
        "    TRAINING_DATA_DIR, \r\n",
        "    subset=\"validation\", \r\n",
        "    shuffle=False,\r\n",
        "    target_size=IMAGE_SHAPE\r\n",
        ")\r\n",
        "\r\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/colab/class/dataall/datanew\n",
            "Found 1289 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuR5EYWaNKWb",
        "outputId": "f5ff852e-3238-43eb-91b6-02688c0741cf"
      },
      "source": [
        "predicted_batch = model.predict(valid_generator00)\r\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\r\n",
        "predicted_id"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7efbe47c0620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7efbe47c0620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7efbe47c0620> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 5, 5, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE_9TGK_N15y",
        "outputId": "b8267f68-40c4-4cf3-b2a1-292a8e255c1c"
      },
      "source": [
        "valid_generator00.labels"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 5, 5, 5], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlVeT6nDOk8j"
      },
      "source": [
        "a1=pd.Series(name)\r\n",
        "a2=pd.Series(valid_generator00.labels)\r\n",
        "a3=pd.Series(predicted_id)\r\n",
        "aa=pd.concat([a1,a2,a3],axis=1)\r\n",
        "aa.columns=['name','label','predict_label']\r\n",
        "aa.to_csv('pred_end.csv')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWt2nGy9f2tg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}